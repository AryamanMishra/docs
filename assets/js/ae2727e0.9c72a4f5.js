"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5828],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>m});var i=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)t=r[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)t=r[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=i.createContext({}),p=function(e){var n=i.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},u=function(e){var n=p(e.components);return i.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},d=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=p(t),m=a,k=d["".concat(s,".").concat(m)]||d[m]||c[m]||r;return t?i.createElement(k,o(o({ref:n},u),{},{components:t})):i.createElement(k,o({ref:n},u))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,o=new Array(r);o[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var p=2;p<r;p++)o[p]=t[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,t)}d.displayName="MDXCreateElement"},3058:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var i=t(7462),a=(t(7294),t(3905));const r={sidebar_position:4},o="Apache Flink",l={unversionedId:"integrations/flink",id:"integrations/flink",title:"Apache Flink",description:"This integration is considered experimental: only specific workflows and use cases are supported.",source:"@site/docs/integrations/flink.md",sourceDirName:"integrations",slug:"/integrations/flink",permalink:"/docs/integrations/flink",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/flink.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Testing custom extractors",permalink:"/docs/integrations/airflow/extractors/extractor-testing"},next:{title:"dbt",permalink:"/docs/integrations/dbt"}},s={},p=[{value:"Getting lineage from Flink",id:"getting-lineage-from-flink",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Usage",id:"usage",level:2}],u={toc:p};function c(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,i.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"apache-flink"},"Apache Flink"),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"This integration is considered experimental: only specific workflows and use cases are supported.")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Apache Flink")," is one of the most popular stream processing frameworks. Apache Flink jobs run on clusters,\nwhich are composed of two types of nodes: ",(0,a.kt)("inlineCode",{parentName:"p"},"TaskManagers")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"JobManagers"),". While clusters typically consists of\nmultiple ",(0,a.kt)("inlineCode",{parentName:"p"},"TaskManagers"),", only reason to run multiple JobManagers is high availability. The jobs are ",(0,a.kt)("em",{parentName:"p"},"submitted"),"\nto ",(0,a.kt)("inlineCode",{parentName:"p"},"JobManager")," by ",(0,a.kt)("inlineCode",{parentName:"p"},"JobClient"),", that compiles user application into dataflow graph which is understandable by ",(0,a.kt)("inlineCode",{parentName:"p"},"JobManager"),".\n",(0,a.kt)("inlineCode",{parentName:"p"},"JobManager")," then coordinates job execution: it splits the parallel units of a job\nto ",(0,a.kt)("inlineCode",{parentName:"p"},"TaskManagers"),", manages heartbeats, triggers checkpoints, reacts to failures and much more."),(0,a.kt)("p",null,"Apache Flink has multiple deployment modes - Session Mode, Application Mode and Per-Job mode. The most popular\nare Session Mode and Application Mode. Session Mode consists of a ",(0,a.kt)("inlineCode",{parentName:"p"},"JobManager")," managing multiple jobs sharing single\nFlink cluster. In this mode, ",(0,a.kt)("inlineCode",{parentName:"p"},"JobClient")," is executed on a machine that submits the job to the cluster."),(0,a.kt)("p",null,"Application Mode is used where cluster is utilized for a single job. In this mode, ",(0,a.kt)("inlineCode",{parentName:"p"},"JobClient"),", where the main method runs,\nis executed on the ",(0,a.kt)("inlineCode",{parentName:"p"},"JobManager"),"."),(0,a.kt)("p",null,"Flink jobs read data from ",(0,a.kt)("inlineCode",{parentName:"p"},"Sources")," and write data to ",(0,a.kt)("inlineCode",{parentName:"p"},"Sinks"),". In contrast to systems like Apache Spark, Flink jobs can write\ndata to multiple places - they can have multiple ",(0,a.kt)("inlineCode",{parentName:"p"},"Sinks"),"."),(0,a.kt)("h2",{id:"getting-lineage-from-flink"},"Getting lineage from Flink"),(0,a.kt)("p",null,"OpenLineage utilizes Flink's ",(0,a.kt)("inlineCode",{parentName:"p"},"JobListener")," interface. This interface is used by Flink to notify user of job submission,\nsuccessful finish of job, or job failure. Implementations of this interface are executed on ",(0,a.kt)("inlineCode",{parentName:"p"},"JobClient"),". "),(0,a.kt)("p",null,"When OpenLineage listener receives information that job was submitted, it extracts ",(0,a.kt)("inlineCode",{parentName:"p"},"Transformations")," from job's\n",(0,a.kt)("inlineCode",{parentName:"p"},"ExecutionEnvironment"),". The ",(0,a.kt)("inlineCode",{parentName:"p"},"Transformations")," represent logical operations in the dataflow graph; they are composed\nof both Flink's build-in operators, but also user-provided ",(0,a.kt)("inlineCode",{parentName:"p"},"Sources"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"Sinks")," and functions. To get the lineage,\nOpenLineage integration processes dataflow graph. Currently, OpenLineage is interested only in information contained\nin ",(0,a.kt)("inlineCode",{parentName:"p"},"Sources")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"Sinks"),", as they are the places where Flink interacts with external systems. "),(0,a.kt)("p",null,"After job submission, OpenLineage integration starts actively listening to checkpoints - this gives insight into\nwhether the job runs properly."),(0,a.kt)("h2",{id:"limitations"},"Limitations"),(0,a.kt)("p",null,"Currently OpenLineage's Flink integration is limited to getting information from jobs running in Application Mode."),(0,a.kt)("p",null,"OpenLineage integration extracts lineage only from following ",(0,a.kt)("inlineCode",{parentName:"p"},"Sources")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"Sinks"),":"),(0,a.kt)("table",null,(0,a.kt)("tbody",null,(0,a.kt)("tr",null,(0,a.kt)("th",null,"Sources"),(0,a.kt)("th",null,"Sinks")),(0,a.kt)("tr",null,(0,a.kt)("td",null,"KafkaSource"),(0,a.kt)("td",null,"KafkaSink")),(0,a.kt)("tr",null,(0,a.kt)("td",null,"FlinkKafkaConsumer"),(0,a.kt)("td",null,"FlinkKafkaProducer")),(0,a.kt)("tr",null,(0,a.kt)("td",null,"IcebergFlinkSource"),(0,a.kt)("td",null)))),(0,a.kt)("p",null,"We expect this list to grow as we add support for more connectors."),(0,a.kt)("h2",{id:"usage"},"Usage"),(0,a.kt)("p",null,"In your job, you need to set up ",(0,a.kt)("inlineCode",{parentName:"p"},"OpenLineageFlinkJobListener"),"."),(0,a.kt)("p",null,"For example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"    JobListener listener = new OpenLineageFlinkJobListener(streamExecutionEnvironment);\n    streamExecutionEnvironment.registerJobListener(listener);\n")),(0,a.kt)("p",null,"Also, OpenLineage needs certain parameters to be set in ",(0,a.kt)("inlineCode",{parentName:"p"},"flink-conf.yaml"),":"),(0,a.kt)("table",null,(0,a.kt)("tbody",null,(0,a.kt)("tr",null,(0,a.kt)("th",null,"Configuration Key"),(0,a.kt)("th",null,"Description"),(0,a.kt)("th",null,"Expected Value"),(0,a.kt)("th",null,"Default")),(0,a.kt)("tr",null,(0,a.kt)("td",null,"execution.attached"),(0,a.kt)("td",null,"This setting needs to be true if OpenLineage is to detect job start and failure"),(0,a.kt)("td",null,"true"),(0,a.kt)("td",null,"false")))),(0,a.kt)("p",null,"OpenLineage jar needs to be present on ",(0,a.kt)("inlineCode",{parentName:"p"},"JobManager"),"."),(0,a.kt)("p",null,"When the ",(0,a.kt)("inlineCode",{parentName:"p"},"JobListener")," is configured, you need to point the OpenLineage integration where the events should end up.\nIf you're using ",(0,a.kt)("inlineCode",{parentName:"p"},"Marquez"),", simplest way to do that is to set up ",(0,a.kt)("inlineCode",{parentName:"p"},"OPENLINEAGE_URL")," environment\nvariable to ",(0,a.kt)("inlineCode",{parentName:"p"},"Marquez")," URL. More advanced settings are ",(0,a.kt)("a",{parentName:"p",href:"/docs/client/java"},"in the client documentation."),"."))}c.isMDXComponent=!0}}]);