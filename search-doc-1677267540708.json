[{"title":"Java","type":0,"sectionRef":"#","url":"/docs/client/java","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Java","url":"/docs/client/java#overview","content":"The OpenLineage Java client enables the creation of lineage metadata events with Java code. The core data structures currently offered by the client are the RunEvent, RunState, Run, Job, Dataset, and Transport classes, along with various Facets that can come under run, job, and dataset. There are various transport classes that the library provides that carry the lineage events into various target endpoints (e.g. HTTP). You can also use the Java client to create your own custom integrations. "},{"title":"Installation​","type":1,"pageTitle":"Java","url":"/docs/client/java#installation","content":"Java client is provided as library that can either be imported into your Java project using Maven or Gradle. Maven: &lt;dependency&gt; &lt;groupId&gt;io.openlineage&lt;/groupId&gt; &lt;artifactId&gt;openlineage-java&lt;/artifactId&gt; &lt;version&gt;0.12.0&lt;/version&gt; &lt;/dependency&gt;  or Gradle: implementation 'io.openlineage:openlineage-java:0.12.0'  For more information on the available versions of the openlineage-java, please refer to the maven repository. "},{"title":"Configuration​","type":1,"pageTitle":"Java","url":"/docs/client/java#configuration","content":"Use the following options to configure the client: An openlineage.yml in the user's current working directoryAn openlineage.yml under .openlineage/ in the user's home directory (ex: ~/.openlineage/openlineage.yml)Environment variables Note: By default, the client will give you sane defaults, but you can easily override them. "},{"title":"Environment Variables​","type":1,"pageTitle":"Java","url":"/docs/client/java#environment-variables","content":"The list of available environment varaibles can be found here. YAML transport: type: &lt;type&gt; # ... transport specific configuration  Here is an example of using HTTP transport for your client: transport: type: http url: http://localhost:5000  Note: For a full list of supported transports, see transports. "},{"title":"Transports​","type":1,"pageTitle":"Java","url":"/docs/client/java#transports","content":"The Transport abstraction defines an emit() method for OpenLineage.RunEvent. There are three built-in transports: ConsoleTransport, HttpTransport, and KafkaTransport. ConsoleTransport​ in YAML: transport: type: CONSOLE  You can also specify the ConsoleTransport when building a new client instance. OpenLineageClient client = OpenLineageClient.builder() .transport( new ConsoleTransport() .build();  HttpTransport​ in YAML: transport: type: HTTP url: http://localhost:5000 auth: type: api_key api_key: f38d2189-c603-4b46-bdea-e573a3b5a7d5  You can override the default configuration of the HttpTransport by specifying the URL and API key when creating a new client: OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .build();  To configure the client with query params appended on each HTTP request, use: Map&lt;String, String&gt; queryParamsToAppend = Map.of( &quot;param0&quot;,&quot;value0&quot;, &quot;param1&quot;, &quot;value1&quot; ); // Connect to http://localhost:5000 OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;, queryParamsToAppend) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .build(); // Define a simple OpenLineage START or COMPLETE event OpenLineage.RunEvent startOrCompleteRun = ... // Emit OpenLineage event to http://localhost:5000/api/v1/lineage?param0=value0&amp;param1=value1 client.emit(startOrCompleteRun);  Alternatively, use the following environment variables to configure the HttpTransport: OPENLINEAGE_URL: the URL for the HTTP transport (default: http://localhost:8080)OPENLINEAGE_API_KEY: the API key to be set on each HTTP request Not everything will be supported while using this method. KafkaTransport​ in YAML: transport: type: Kafka topicName: openlineage.events # Kafka properties (see: http://kafka.apache.org/0100/documentation.html#producerconfigs) properties: bootstrap.servers: localhost:9092,another.host:9092 acks: all retries: 3 key.serializer: org.apache.kafka.common.serialization.StringSerializer value.serializer: org.apache.kafka.common.serialization.StringSerializer  KafkaTransport depends on you to provide artifact org.apache.kafka:kafka-clients:3.1.0 or compatible on your classpath. "},{"title":"Error Handling via Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#error-handling-via-transport","content":"// Connect to http://localhost:5000 OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .registerErrorHandler(new EmitErrorHandler() { @Override public void handleError(Throwable throwable) { // Handle emit error here } }).build();  "},{"title":"Defining Your Own Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#defining-your-own-transport","content":"OpenLineageClient client = OpenLineageClient.builder() .transport( new MyTransport() { @Override public void emit(OpenLineage.RunEvent runEvent) { // Add emit logic here } }).build();  "},{"title":"Usage​","type":1,"pageTitle":"Java","url":"/docs/client/java#usage","content":""},{"title":"1. Simple OpenLineage Client Test for Console Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#1-simple-openlineage-client-test-for-console-transport","content":"First, let's explore how we can create OpenLineage client instance, but not using any actual transport to emit the data yet, except only to our Console. This would be a good exercise to run tests and check the data payloads.  OpenLineageClient client = OpenLineageClient.builder() .transport(new ConsoleTransport()).build();  Also, we will then get a sample payload to produce a RunEvent:  // create one start event for testing RunEvent event = buildEvent(EventType.START);  Lastly, we will emit this event using the client that we instantiated\\:  // emit the event client.emit(event);  Here is the full source code of the test client application: package ol.test; import io.openlineage.client.OpenLineage; import io.openlineage.client.OpenLineageClient; import io.openlineage.client.OpenLineage.RunEvent; import io.openlineage.client.OpenLineage.InputDataset; import io.openlineage.client.OpenLineage.Job; import io.openlineage.client.OpenLineage.JobFacets; import io.openlineage.client.OpenLineage.OutputDataset; import io.openlineage.client.OpenLineage.Run; import io.openlineage.client.OpenLineage.RunFacets; import io.openlineage.client.OpenLineage.RunEvent.EventType; import io.openlineage.client.transports.ConsoleTransport; import java.net.URI; import java.time.ZoneId; import java.time.ZonedDateTime; import java.util.Arrays; import java.util.List; import java.util.UUID; /** * My first openlinage client code */ public class OpenLineageClientTest { public static void main( String[] args ) { try { OpenLineageClient client = OpenLineageClient.builder() .transport(new ConsoleTransport()).build(); // create one start event for testing RunEvent event = buildEvent(EventType.START); // emit the event client.emit(event); } catch (Exception e) { e.printStackTrace(); } } // sample code to build event public static RunEvent buildEvent(EventType eventType) { ZonedDateTime now = ZonedDateTime.now(ZoneId.of(&quot;UTC&quot;)); URI producer = URI.create(&quot;producer&quot;); OpenLineage ol = new OpenLineage(producer); UUID runId = UUID.randomUUID(); // run facets RunFacets runFacets = ol.newRunFacetsBuilder() .nominalTime( ol.newNominalTimeRunFacetBuilder() .nominalStartTime(now) .nominalEndTime(now) .build()) .build(); // a run is composed of run id, and run facets Run run = ol.newRunBuilder().runId(runId).facets(runFacets).build(); // job facets JobFacets jobFacets = ol.newJobFacetsBuilder().build(); // job String name = &quot;jobName&quot;; String namespace = &quot;namespace&quot;; Job job = ol.newJobBuilder().namespace(namespace).name(name).facets(jobFacets).build(); // input dataset List&lt;InputDataset&gt; inputs = Arrays.asList( ol.newInputDatasetBuilder() .namespace(&quot;ins&quot;) .name(&quot;input&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;input-version&quot;)) .build()) .inputFacets( ol.newInputDatasetInputFacetsBuilder() .dataQualityMetrics( ol.newDataQualityMetricsInputDatasetFacetBuilder() .rowCount(10L) .bytes(20L) .columnMetrics( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsBuilder() .put( &quot;mycol&quot;, ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalBuilder() .count(10D) .distinctCount(10L) .max(30D) .min(5D) .nullCount(1L) .sum(3000D) .quantiles( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalQuantilesBuilder() .put(&quot;25&quot;, 52D) .build()) .build()) .build()) .build()) .build()) .build()); // output dataset List&lt;OutputDataset&gt; outputs = Arrays.asList( ol.newOutputDatasetBuilder() .namespace(&quot;ons&quot;) .name(&quot;output&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;output-version&quot;)) .build()) .outputFacets( ol.newOutputDatasetOutputFacetsBuilder() .outputStatistics(ol.newOutputStatisticsOutputDatasetFacet(10L, 20L)) .build()) .build()); // run state udpate which encapsulates all - with START event in this case RunEvent runStateUpdate = ol.newRunEventBuilder() .eventType(OpenLineage.RunEvent.EventType.START) .eventTime(now) .run(run) .job(job) .inputs(inputs) .outputs(outputs) .build(); return runStateUpdate; } }  The result of running this will result in the following output from your Java application: [main] INFO io.openlineage.client.transports.ConsoleTransport - {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;bb46bbc4-fb1a-495a-ad3b-8d837f566749&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;}  "},{"title":"2. Simple OpenLineage Client Test for Http Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#2-simple-openlineage-client-test-for-http-transport","content":"Now, using the same code base, we will change how the client application works by switching the Console transport into Http Transport as shown below. This code will now be able to send the OpenLineage events into a compatible backends such as Marquez. Before making this change and running it, make sure you have an instance of Marquez running on your local environment. Setting up and running Marquez can be found here. OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .build()) .build();  If we ran the same application, you will now see the event data not emitted in the output console, but rather via the HTTP transport to the marquez backend that was running.  Notice that the Status of this job run will be in RUNNING state, as it will be in that state until it receives an end event that will close off its gaps. That is how the OpenLineage events would work. Now, let's change the previous example to have lineage event doing a complete cycle of START -&gt; COMPLETE: package ol.test; import io.openlineage.client.OpenLineage; import io.openlineage.client.OpenLineageClient; import io.openlineage.client.OpenLineage.RunEvent; import io.openlineage.client.OpenLineage.InputDataset; import io.openlineage.client.OpenLineage.Job; import io.openlineage.client.OpenLineage.JobFacets; import io.openlineage.client.OpenLineage.OutputDataset; import io.openlineage.client.OpenLineage.Run; import io.openlineage.client.OpenLineage.RunFacets; import io.openlineage.client.OpenLineage.RunEvent.EventType; import io.openlineage.client.transports.HttpTransport; import java.net.URI; import java.time.ZoneId; import java.time.ZonedDateTime; import java.util.Arrays; import java.util.List; import java.util.UUID; /** * My first openlinage client code */ public class OpenLineageClientTest { public static void main( String[] args ) { try { OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .build()) .build(); // create one start event for testing RunEvent event = buildEvent(EventType.START, null); // emit the event client.emit(event); // another event to COMPLETE the run event = buildEvent(EventType.COMPLETE, event.getRun().getRunId()); // emit the second COMPLETE event client.emit(event); } catch (Exception e) { e.printStackTrace(); } } // sample code to build event public static RunEvent buildEvent(EventType eventType, UUID runId) { ZonedDateTime now = ZonedDateTime.now(ZoneId.of(&quot;UTC&quot;)); URI producer = URI.create(&quot;producer&quot;); OpenLineage ol = new OpenLineage(producer); if (runId == null) { runId = UUID.randomUUID(); } // run facets RunFacets runFacets = ol.newRunFacetsBuilder() .nominalTime( ol.newNominalTimeRunFacetBuilder() .nominalStartTime(now) .nominalEndTime(now) .build()) .build(); // a run is composed of run id, and run facets Run run = ol.newRunBuilder().runId(runId).facets(runFacets).build(); // job facets JobFacets jobFacets = ol.newJobFacetsBuilder().build(); // job String name = &quot;jobName&quot;; String namespace = &quot;namespace&quot;; Job job = ol.newJobBuilder().namespace(namespace).name(name).facets(jobFacets).build(); // input dataset List&lt;InputDataset&gt; inputs = Arrays.asList( ol.newInputDatasetBuilder() .namespace(&quot;ins&quot;) .name(&quot;input&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;input-version&quot;)) .build()) .inputFacets( ol.newInputDatasetInputFacetsBuilder() .dataQualityMetrics( ol.newDataQualityMetricsInputDatasetFacetBuilder() .rowCount(10L) .bytes(20L) .columnMetrics( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsBuilder() .put( &quot;mycol&quot;, ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalBuilder() .count(10D) .distinctCount(10L) .max(30D) .min(5D) .nullCount(1L) .sum(3000D) .quantiles( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalQuantilesBuilder() .put(&quot;25&quot;, 52D) .build()) .build()) .build()) .build()) .build()) .build()); // output dataset List&lt;OutputDataset&gt; outputs = Arrays.asList( ol.newOutputDatasetBuilder() .namespace(&quot;ons&quot;) .name(&quot;output&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;output-version&quot;)) .build()) .outputFacets( ol.newOutputDatasetOutputFacetsBuilder() .outputStatistics(ol.newOutputStatisticsOutputDatasetFacet(10L, 20L)) .build()) .build()); // run state udpate which encapsulates all - with START event in this case RunEvent runStateUpdate = ol.newRunEventBuilder() .eventType(eventType) .eventTime(now) .run(run) .job(job) .inputs(inputs) .outputs(outputs) .build(); return runStateUpdate; } }  Now, when you run this application, the Marquez would have an output that would looke like this:  "},{"title":"Python","type":0,"sectionRef":"#","url":"/docs/client/python","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Python","url":"/docs/client/python#overview","content":"The Python client is the basis of existing OpenLineage integrations such as Airflow and dbt. The client enables the creation of lineage metadata events with Python code. The core data structures currently offered by the client are the RunEvent, RunState, Run, Job, Dataset, and Transport classes. These either configure or collect data for the emission of lineage events. You can use the client to create your own custom integrations. "},{"title":"Installation​","type":1,"pageTitle":"Python","url":"/docs/client/python#installation","content":"Download the package using pip with pip install openlineage-python  To install the package from source, use python setup.py install  "},{"title":"Configuration​","type":1,"pageTitle":"Python","url":"/docs/client/python#configuration","content":"We recommend configuring the client with an openlineage.yml file that tells the client how to connect to an OpenLineage backend. You can make this file available to the client three ways: Set an environment variable to a file path: OPENLINEAGE_CONFIG=path/to/openlineage.yml.Put the file in the working directory.Put the file in $HOME/.openlineage. In openlineage.yml, use a standard transport interface to specify the transport type (http, console, kafka, or custom) and authorization parameters: transport: type: &quot;http&quot; url: &quot;https://backend:5000&quot; auth: type: &quot;api_key&quot; api_key: &quot;f048521b-dfe8-47cd-9c65-0cb07d57591e&quot;  The type property (required) is a fully qualified class name that can be imported. "},{"title":"Environment Variables​","type":1,"pageTitle":"Python","url":"/docs/client/python#environment-variables","content":"The list of available environment varaibles can be found here. "},{"title":"Built-in Transport Types​","type":1,"pageTitle":"Python","url":"/docs/client/python#built-in-transport-types","content":"HTTP​ type - string (required)url - string (required)timeout - float specifying a timeout value when sending an event. Default: 5 seconds. (optional)verify - boolean specifying whether or not the client should verify TLS certificates from the backend. Default: true. (optional)auth - dictionary specifying authentication options. Requires the type property. (optional)type - string specifying the &quot;api_key&quot; or the fully qualified class name of your TokenProvider. (required if auth is provided)api_key - string setting the Authentication HTTP header as the Bearer. (required if api_key is set) Kafka​ Kafka transport requires confluent-kafka package to be additionally installed. It can be installed also by specifying kafka client extension: pip install openlineage-python[kafka] type - string (required)config - string containing a Kafka producer config (required)topic - string specifying the topic (required)flush - boolean specifying whether or not Kafka should flush after each event. Default: true. (optional) "},{"title":"Custom Transport Type​","type":1,"pageTitle":"Python","url":"/docs/client/python#custom-transport-type","content":"To implement a custom transport, follow the instructions in transport.py. "},{"title":"Getting Started​","type":1,"pageTitle":"Python","url":"/docs/client/python#getting-started","content":"To try out the client, follow the steps below to install and explore OpenLineage, Marquez (the reference implementation of OpenLineage), and the client itself. Then, the instructions will show you how to use these tools to add a run event and datasets to an existing namespace. "},{"title":"Prerequisites​","type":1,"pageTitle":"Python","url":"/docs/client/python#prerequisites","content":"Docker 17.05+Docker Compose 1.29.1+Git (preinstalled on most versions of MacOS; verify your version with git version)4 GB of available memory (the minimum for Docker — more is strongly recommended) "},{"title":"Install OpenLineage and Marquez​","type":1,"pageTitle":"Python","url":"/docs/client/python#install-openlineage-and-marquez","content":"Clone the Marquez Github repository: git clone https://github.com/MarquezProject/marquez.git  "},{"title":"Install the Python client​","type":1,"pageTitle":"Python","url":"/docs/client/python#install-the-python-client","content":"pip install openlineage-python  "},{"title":"Start Docker and Marquez​","type":1,"pageTitle":"Python","url":"/docs/client/python#start-docker-and-marquez","content":"Start Docker Desktop Run Marquez with preloaded data: cd marquez ./docker/up.sh --seed  Marquez should be up and running at http://localhost:3000. Take a moment to explore Marquez to get a sense of how metadata is displayed in the UI. Namespaces – the global contexts for runs and datasets – can be found in the top right corner, and icons for jobs and runs can be found in a tray along the left side. Next, configure OpenLineage and add a script to your project that will generate a new job and new datasets within an existing namespace (here we’re using the food_delivery namespace that got passed to Marquez with the –seed argument we used earlier). Create a directory for your script: .. mkdir python_scripts &amp;&amp; cd python_scripts  In the python_scripts directory, create a Python script (we used the name generate_events.py for ours) and an openlineage.yml file. In openlineage.yml, define a transport type and URL to tell OpenLineage where and how to send metadata: Transport: Type: “http” Url: “http://localhost:5000”  In generate_events.py, import the Python client and the methods needed to create a job and datasets. Also required (to create a run): the datetime and uuid packages: from openlineage.client.run import RunEvent, RunState, Run, Job, Dataset from openlineage.client import OpenLineageClient from datetime import datetime from uuid import uuid4  Then, in the same file, initialize the Python client: client = OpenLineageClient.from_environment()  It is also possible to specify parameters such as URL for client to connect to, without using environment variables or openlineage.yaml file, by directly setting it up when instantiating OpenLineageClient: client = OpenLineageClient(url=&quot;http://localhost:5000&quot;)  For more details about options to setup OpenLineageClient such as API tokens or HTTP transport settings, please refer to the following example Specify the producer of the new lineage metadata with a string: producer = “OpenLineage.io/website/blog”  Now you can create some basic dataset objects. These require a namespace and name: inventory = Dataset(namespace=“food_delivery”, name=“public.inventory”) menus = Dataset(namespace=“food_delivery”, name=“public.menus_1”) orders = Dataset(namespace=“food_delivery”, name=“public.orders_1”)  You can also create a job object (we’ve borrowed this one from the existing food_delivery namespace): job = Job(namespace=“food_delivery”, name=“example.order_data”)  To create a run object you’ll need to specify a unique ID: run = Run(str(uuid4()))  a START run event: client.emit( RunEvent( RunState.START, datetime.now().isoformat(), run, job, producer ) )  and, finally, a COMPLETE run event: client.emit( RunEvent( RunState.COMPLETE, datetime.now().isoformat(), run, job, producer, inputs=[inventory], outputs=[menus, orders], ) )  Now you have a complete script for creating datasets and a run event! Execute it in the terminal to send the metadata to Marquez: python3 generate_scripts.py  Marquez will update itself automatically, so the new job and datasets should now be visible in the UI. Clicking on the jobs icon (the icon with the three interlocking gears), will make the example.order_data job appear in the list of jobs:  When you click on the job, you will see a new map displaying the job, input and outputs we created with our script:  "},{"title":"Full Example Source Code​","type":1,"pageTitle":"Python","url":"/docs/client/python#full-example-source-code","content":"#!/usr/bin/env python3 from openlineage.client.run import ( RunEvent, RunState, Run, Job, Dataset, OutputDataset, InputDataset, ) from openlineage.client.client import OpenLineageClient, OpenLineageClientOptions from openlineage.client.facet import ( SqlJobFacet, SchemaDatasetFacet, SchemaField, OutputStatisticsOutputDatasetFacet, SourceCodeLocationJobFacet, NominalTimeRunFacet, DataQualityMetricsInputDatasetFacet, ColumnMetric, ) import uuid from datetime import datetime, timezone, timedelta import time from random import random PRODUCER = f&quot;https://github.com/openlineage-user&quot; namespace = &quot;python_client&quot; dag_name = &quot;user_trends&quot; url = &quot;http://mymarquez.host:5000&quot; api_key = &quot;1234567890ckcu028rzu5l&quot; client = OpenLineageClient( url=url, # optional api key in case marquez requires it. When running marquez in # your local environment, you usually do not need this. options=OpenLineageClientOptions(api_key=api_key), ) # generates job facet def job(job_name, sql, location): facets = {&quot;sql&quot;: SqlJobFacet(sql)} if location != None: facets.update( {&quot;sourceCodeLocation&quot;: SourceCodeLocationJobFacet(&quot;git&quot;, location)} ) return Job(namespace=namespace, name=job_name, facets=facets) # geneartes run racet def run(run_id, hour): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ) }, ) # generates dataset def dataset(name, schema=None, ns=namespace): if schema == None: facets = {} else: facets = {&quot;schema&quot;: schema} return Dataset(namespace, name, facets) # generates output dataset def outputDataset(dataset, stats): output_facets = {&quot;stats&quot;: stats, &quot;outputStatistics&quot;: stats} return OutputDataset(dataset.namespace, dataset.name, dataset.facets, output_facets) # generates input dataset def inputDataset(dataset, dq): input_facets = { &quot;dataQuality&quot;: dq, } return InputDataset(dataset.namespace, dataset.name, dataset.facets, input_facets) def twoDigits(n): if n &lt; 10: result = f&quot;0{n}&quot; elif n &lt; 100: result = f&quot;{n}&quot; else: raise f&quot;error: {n}&quot; return result now = datetime.now(timezone.utc) # generates run Event def runEvents(job_name, sql, inputs, outputs, hour, min, location, duration): run_id = str(uuid.uuid4()) myjob = job(job_name, sql, location) myrun = run(run_id, hour) st = now + timedelta(hours=hour, minutes=min, seconds=20 + round(random() * 10)) end = st + timedelta(minutes=duration, seconds=20 + round(random() * 10)) started_at = st.isoformat() ended_at = end.isoformat() return ( RunEvent( eventType=RunState.START, eventTime=started_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), RunEvent( eventType=RunState.COMPLETE, eventTime=ended_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), ) # add run event to the events list def addRunEvents( events, job_name, sql, inputs, outputs, hour, minutes, location=None, duration=2 ): (start, complete) = runEvents( job_name, sql, inputs, outputs, hour, minutes, location, duration ) events.append(start) events.append(complete) events = [] # create dataset data for i in range(0, 5): user_counts = dataset(&quot;tmp_demo.user_counts&quot;) user_history = dataset( &quot;temp_demo.user_history&quot;, SchemaDatasetFacet( fields=[ SchemaField(name=&quot;id&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;email_domain&quot;, type=&quot;VARCHAR&quot;, description=&quot;the user id&quot; ), SchemaField(name=&quot;status&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;created_at&quot;, type=&quot;DATETIME&quot;, description=&quot;date and time of creation of the user&quot;, ), SchemaField( name=&quot;updated_at&quot;, type=&quot;DATETIME&quot;, description=&quot;the last time this row was updated&quot;, ), SchemaField( name=&quot;fetch_time_utc&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was fetched&quot;, ), SchemaField( name=&quot;load_filename&quot;, type=&quot;VARCHAR&quot;, description=&quot;the original file this data was ingested from&quot;, ), SchemaField( name=&quot;load_filerow&quot;, type=&quot;INT&quot;, description=&quot;the row number in the original file&quot;, ), SchemaField( name=&quot;load_timestamp&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was ingested&quot;, ), ] ), &quot;snowflake://&quot;, ) create_user_counts_sql = &quot;&quot;&quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS ( SELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count FROM TMP_DEMO.USER_HISTORY GROUP BY date )&quot;&quot;&quot; # location of the source code location = &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; # run simulating Airflow DAG with snowflake operator addRunEvents( events, dag_name + &quot;.create_user_counts&quot;, create_user_counts_sql, [user_history], [user_counts], i, 11, location, ) for event in events: from openlineage.client.serde import Serde print(event) print(Serde.to_json(event)) # time.sleep(1) client.emit(event)  The resulting lineage events received by Marquez would look like this.  "},{"title":"Developing With OpenLineage","type":0,"sectionRef":"#","url":"/docs/development/developing/","content":"","keywords":""},{"title":"Clients​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#clients","content":"For Python and Java, we've created clients that you can use to properly create and emit OpenLineage events to HTTP, Kafka, and other consumers. "},{"title":"API Documentation​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#api-documentation","content":"OpenAPI documentationJava Doc "},{"title":"Common Library (Python)​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#common-library-python","content":"Getting lineage from systems like BigQuery or Redshift isn't necessarily tied to orchestrator or processing engine you're using. For this reason, we've extracted that functionality from our Airflow library and packaged it for separate use. "},{"title":"Environment Variables​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#environment-variables","content":"The following environment variables are available commonly for both Java and Python languages. Name\tDescription\tSinceOPENLINEAGE_API_KEY\tThe optional API key to be set on each lineage request. This will be set as a Bearer token in case authentication is required. OPENLINEAGE_CONFIG\tThe optional path to locate the configuration file. The configuration file is in YAML format. Example: openlineage.yml OPENLINEAGE_DISABLED\tWhen set to true, will prevent OpenLineage from emitting events to the receiving backend\t0.9.0 OPENLINEAGE_URL\tThe URL for the HTTP transport of where to emit lineage events to. If not yet, no lineage data will be emitted, and event data (JSON) will be written to standard output. Example: http://localhost:8080\t "},{"title":"SQL parser​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#sql-parser","content":"We've created SQL parser that allows you to extract lineage from SQL statements. The parser is implemented in Rust, however, it's also available as a Python library. You can take a look at it's documentation here or code on GitHub. "},{"title":"Setup a development environment","type":0,"sectionRef":"#","url":"/docs/development/developing/java/setup","content":"Setup a development environment info This page needs your contribution! Please contribute new examples using the edit link at the bottom.","keywords":""},{"title":"Setup a development environment","type":0,"sectionRef":"#","url":"/docs/development/developing/python/setup","content":"","keywords":""},{"title":"Docker Compose development environment​","type":1,"pageTitle":"Setup a development environment","url":"/docs/development/developing/python/setup#docker-compose-development-environment","content":"There is also possibility to create local Docker-based development environment that has OpenLineage libraries setup along with Airflow and some helpful services. To do that you should run run-dev-airflow.sh script located here. The script uses the same Docker Compose files as integration tests. Two main differences are: it runs in non-blocking wayit mounts OpenLineage Python packages as editable and mounted to Airflow containers. This allows to change code and test it live without need to rebuild whole environment. When using above script, you can add the -i flag or --attach-integration flag. This can be helpful when you need to run arbitrary integration tests during development. For example, the following command run in the integration container... python -m pytest test_integration.py::test_integration[great_expectations_validation-requests/great_expectations.json]  ...runs a single test which you can repeat after changes in code. "},{"title":"Airflow","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/airflow","content":"","keywords":""},{"title":"Unit tests​","type":1,"pageTitle":"Airflow","url":"/docs/development/developing/python/tests/airflow#unit-tests","content":"In order to make running unit tests against multiple Airflow versions easier there is possibility to use tox. To run unit tests against all configured Airflow versions just run: tox  You can also list existing environments with: tox -l  that should list: py3-airflow-2.1.4 py3-airflow-2.2.4 py3-airflow-2.3.4 py3-airflow-2.4.3 py3-airflow.2.5.0  Then you can run tests in chosen environment, e.g.: tox -e py3-airflow-2.3.4  setup.cfg contains tox-related configuration. By default tox command runs: flake8 lintingpytest command Additionally, outside of tox you should run mypy static code analysis. You can do that with: python -m mypy openlineage  "},{"title":"Integration tests​","type":1,"pageTitle":"Airflow","url":"/docs/development/developing/python/tests/airflow#integration-tests","content":"Integration tests are located in tests/integration/tests directory. They require running Docker containers to provision local test environment: Airflow components (worker, scheduler), databases (PostgreSQL, MySQL) and OpenLineage events consumer. How to run​ Integration tests require usage of docker compose. There are scripts prepared to make build images and run tests easier. AIRFLOW_IMAGE=&lt;name-of-airflow-image&gt; ./tests/integration/docker/up.sh  e.g. AIRFLOW_IMAGE=apache/airflow:2.3.4-python3.7 ./tests/integration/docker/up.sh  What tests are ran​ The actual setup is to run all defined Airflow DAGs, collect OpenLineage events and check if they meet requirements. The test you should pay most attention to is test_integration. It compares produced events to expected JSON structures recursively, with a respect if fields are not missing. Some of the tests are skipped if database connection specific environment variables are not set. The example is set of SNOWFLAKE_PASSWORD and SNOWFLAKE_ACCOUNT_ID variables. View stored OpenLineage events​ OpenLineage events produced from Airflow runs are stored locally in ./tests/integration/tests/events directory. The files are not overwritten, rather new events are appended to existing files. Example how to add new integration test​ Let's take following CustomOperator for which we should add CustomExtractor and test it. First we create DAG in integration tests DAGs folder: airflow/tests/integration/tests/airflow/dags. from airflow.models import BaseOperator from airflow.utils.dates import days_ago from airflow import DAG default_args = { 'depends_on_past': False, 'start_date': days_ago(7) } dag = DAG( 'custom_extractor', schedule_interval='@once', default_args=default_args ) class CustomOperator(BaseOperator): def execute(self, context: Any): for i in range(10): print(i) t1 = CustomOperator( task_id='custom_extractor', dag=dag )  In the same folder we create custom_extractor.py: from typing import Union, Optional, List from openlineage.client.run import Dataset from openlineage.airflow.extractors import TaskMetadata from openlineage.airflow.extractors.base import BaseExtractor class CustomExtractor(BaseExtractor): @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['CustomOperator'] def extract(self) -&gt; Union[Optional[TaskMetadata], List[TaskMetadata]]: return TaskMetadata( &quot;test&quot;, inputs=[ Dataset( namespace=&quot;test&quot;, name=&quot;dataset&quot;, facets={} ) ] )  Typically we want to compare produced metadata against expected. In order to do that we create JSON file custom_extractor.json in airflow/tests/integration/requests:  [{ &quot;eventType&quot;: &quot;START&quot;, &quot;inputs&quot;: [{ &quot;facets&quot;: {}, &quot;name&quot;: &quot;dataset&quot;, &quot;namespace&quot;: &quot;test&quot; }], &quot;job&quot;: { &quot;facets&quot;: { &quot;documentation&quot;: { &quot;description&quot;: &quot;Test dag.&quot; } }, &quot;name&quot;: &quot;custom_extractor.custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; }, &quot;run&quot;: { &quot;facets&quot;: { &quot;airflow_runArgs&quot;: { &quot;externalTrigger&quot;: false }, &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; } } } } }, { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;inputs&quot;: [{ &quot;facets&quot;: {}, &quot;name&quot;: &quot;dataset&quot;, &quot;namespace&quot;: &quot;test&quot; }], &quot;job&quot;: { &quot;facets&quot;: {}, &quot;name&quot;: &quot;custom_extractor.custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; } } ]  and add parameter for test_integration in airflow/tests/integration/test_integration.py: (&quot;source_code_dag&quot;, &quot;requests/source_code.json&quot;), + (&quot;custom_extractor&quot;, &quot;requests/custom_extractor.json&quot;), (&quot;unknown_operator_dag&quot;, &quot;requests/unknown_operator.json&quot;),  That should setup a check for existence of both START and COMPLETE events, custom input facet and correct job facet. Full example can be found in source code available in integration tests directory. "},{"title":"Client","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/client","content":"Client info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for OpenLineage Python client. You can run them with a simple pytest command with directory set to client base path.","keywords":""},{"title":"Common","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/common","content":"Common info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for OpenLineage common package. You can run them with a simple pytest command with directory set to package base path.","keywords":""},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/development/developing/java/troubleshooting/logging","content":"","keywords":""},{"title":"Maven​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#maven","content":"pom.xml  &lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt;  "},{"title":"Gradle​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#gradle","content":"build.gradle dependencies { ... implementation &quot;org.apache.logging.log4j:log4j-api:2.7&quot; implementation &quot;org.apache.logging.log4j:log4j-core:2.7&quot; implementation &quot;org.apache.logging.log4j:log4j-slf4j-impl:2.7&quot; ... }  You also need to create a log4j configuration file, log4j2.properties on the classpath. Here is the sample log configuration. # Set to debug or trace if log4j initialization is failing status = warn # Name of the configuration name = ConsoleLogConfigDemo # Console appender configuration appender.console.type = Console appender.console.name = consoleLogger appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # Root logger level rootLogger.level = debug # Root logger referring to console appender rootLogger.appenderRef.stdout.ref = consoleLogger  Re-compiling and running the ol.test.OpenLineageClientTest again will produce the following outputs: 2022-12-07 08:57:24 INFO OpenLineageClientTest:33 - Running OpenLineage Client Test... 2022-12-07 08:57:25 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;0142c998-3416-49e7-92aa-d025c4c93697&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;} 2022-12-07 08:57:25 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;COMPLETE&quot;,&quot;eventTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;0142c998-3416-49e7-92aa-d025c4c93697&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;}  Logs will also produce meaningful error messages when something does not work correctly. For example, if the backend server does not exist, you would get the following messages in your console output: 2022-12-07 09:15:16 INFO OpenLineageClientTest:33 - Running OpenLineage Client Test... 2022-12-07 09:15:16 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;69861937-55ba-43f5-ab5e-fe78ef6a283d&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;} io.openlineage.client.OpenLineageClientException: org.apache.http.conn.HttpHostConnectException: Connect to localhost:5000 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused at io.openlineage.client.transports.HttpTransport.emit(HttpTransport.java:113) at io.openlineage.client.OpenLineageClient.emit(OpenLineageClient.java:42) at ol.test.OpenLineageClientTest.main(OpenLineageClientTest.java:48) Caused by: org.apache.http.conn.HttpHostConnectException: Connect to localhost:5000 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108) at io.openlineage.client.transports.HttpTransport.emit(HttpTransport.java:108) ... 2 more Caused by: java.net.ConnectException: Connection refused at java.base/sun.nio.ch.Net.pollConnect(Native Method) at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672) at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:542) at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:585) at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) at java.base/java.net.Socket.connect(Socket.java:666) at org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75) at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) ... 12 more  If you wish to output loggigng message to a file, you can modify the basic configuration by adding a file appender configuration as follows: # Set to debug or trace if log4j initialization is failing status = warn # Name of the configuration name = ConsoleLogConfigDemo # Console appender configuration appender.console.type = Console appender.console.name = consoleLogger appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # File appender configuration appender.file.type = File appender.file.name = fileLogger appender.file.fileName = app.log appender.file.layout.type = PatternLayout appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # Root logger level rootLogger.level = debug # Root logger referring to console appender rootLogger.appenderRef.stdout.ref = consoleLogger rootLogger.appenderRef.file.ref = fileLogger  And the logs will be saved to a file app.log. Outputting logs using log4j2 is just one way of doing it, so below are some additional resources of undersatnding how Java logging works, and other ways to output the logs. "},{"title":"Further readings​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#further-readings","content":"https://www.baeldung.com/java-logging-introhttps://www.baeldung.com/slf4j-with-log4j2-logback#Log4j2https://mkyong.com/logging/log4j2-properties-example/ "},{"title":"Dagster","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/dagster","content":"Dagster info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for Dagster integration. You can run them with a simple pytest command with directory set to integration base path.","keywords":""},{"title":"Example Lineage Events","type":0,"sectionRef":"#","url":"/docs/development/examples","content":"","keywords":""},{"title":"Simple Examples​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#simple-examples","content":""},{"title":"START event with single input​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#start-event-with-single-input","content":"This is a START event with a single PostgreSQL input dataset. { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"COMPLETE event with single output​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complete-event-with-single-output","content":"This is a COMPLETE event with a single PostgreSQL output dataset. { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.unpaid_taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"Complex Examples​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complex-examples","content":""},{"title":"START event with Facets (run and job)​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#start-event-with-facets-run-and-job","content":"This is a START event with run and job facets of Apache Airflow. { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; &quot;facets&quot;: { &quot;airflow_runArgs&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;externalTrigger&quot;: true }, &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot;: &quot;2022-07-29T14:14:31.458067Z&quot; }, &quot;parentRun&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/ParentRunFacet&quot;, &quot;job&quot;: { &quot;name&quot;: &quot;etl_orders&quot;, &quot;namespace&quot;: &quot;cosmic_energy&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;1ba6fdaa-fb80-36ce-9c5b-295f544ec462&quot; } } } }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot;, &quot;facets&quot;: { &quot;documentation&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/DocumentationJobFacet&quot;, &quot;description&quot;: &quot;Process taxes.&quot; }, &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SqlJobFacet&quot;, &quot;query&quot;: &quot;INSERT into taxes values(1, 100, 1000, 4000);&quot; } }, }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"COMPLETE event with Facets (dataset)​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complete-event-with-facets-dataset","content":"This is a COMPLETE event with dataset facet of Database table. { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.unpaid_taxes&quot;, &quot;facets&quot;: { &quot;dataSource&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/DataSourceDatasetFacet&quot;, &quot;name&quot;: &quot;postgres://workshop-db:None&quot;, &quot;uri&quot;: &quot;workshop-db&quot; }, &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;SERIAL PRIMARY KEY&quot; }, { &quot;name&quot;: &quot;tax_dt&quot;, &quot;type&quot;: &quot;TIMESTAMP NOT NULL&quot; }, { &quot;name&quot;: &quot;tax_item_id&quot;, &quot;type&quot;: &quot;INTEGER REFERENCES tax_itemsid&quot; }, { &quot;name&quot;: &quot;amount&quot;, &quot;type&quot;: &quot;INTEGER NOT NULL&quot; }, { &quot;name&quot;: &quot;ref_id&quot;, &quot;type&quot;: &quot;INTEGER REFERENCES refid&quot; }, { &quot;name&quot;: &quot;comment&quot;, &quot;type&quot;: &quot;TEXT&quot; } ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"OpenLineage Proxy","type":0,"sectionRef":"#","url":"/docs/development/ol-proxy","content":"","keywords":""},{"title":"Accessing the proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#accessing-the-proxy","content":"OpenLineage proxy can be obtained via github: git clone https://github.com/OpenLineage/OpenLineage.git cd OpenLineage/proxy  "},{"title":"Building the proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#building-the-proxy","content":"To build the proxy jar, run $ ./gradlew build  The packaged jar file can be found under ./build/libs/ "},{"title":"Running the proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#running-the-proxy","content":"OpenLineage Proxy requires configuration file named proxy.yml. There is an example that you can copy and name it as proxy.yml. cp proxy.example.yml proxy.yml  By default, the OpenLineage proxy uses the following ports: TCP port 8080 is available for the HTTP API server.TCP port 8081 is available for the admin interface. You can then run the proxy using gradlew: $ ./gradlew runShadow  "},{"title":"Monitoring OpenLineage events via Proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#monitoring-openlineage-events-via-proxy","content":"When proxy is running, you can start sending your OpenLineage events just as the same way as you would be sending to any OpenLineage backend server. For example, in your URL for the OpenLineage backend, you can specify it as http://localhost:8080/api/v1/lineage. Once the message is sent to the proxy, you will see the OpenLineage message content (JSON) to the console output of the proxy. You can also specify in the configuration to store the messages into the log file. You might have noticed that OpenLineage client (python, java) simply requires http://localhost:8080 as the URL endpoint. This is possible because the client code adds the /api/v1/lineage internally before it makes the request. If you are not using OpenLineage client library to emit OpenLineage events, you must use the full URL in order for the proxy to receive the data correctly. "},{"title":"Forwarding the data​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#forwarding-the-data","content":"Not only the OpenLineage proxy is useful in receiving the monitoring the OpenLineage events, it can also be used to relay the events to other endpoints. Please see the example of how to set the proxy to relay the events via Kafka topic or HTTP endpoint. "},{"title":"Other ways to run OpenLineage Proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#other-ways-to-run-openlineage-proxy","content":"You do not have to clone the git repo and build all the time. OpenLineage proxy is published and available in Maven Repository.You can also run OpenLineage Proxy as a docker container.There is also a helm chart for Kubernetes available. "},{"title":"SQL parser","type":0,"sectionRef":"#","url":"/docs/development/sql","content":"","keywords":""},{"title":"Interface​","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#interface","content":"SQL parser interface expressed in pseudo-python. class DbTableMeta: database: Optional[str] schema: Optional[str] name: str class SqlMeta: in_tables: List[DbTableMeta] out_tables: List[DbTableMeta] def parse( sql: Union[List[str], str], # Setting dialect allows you to enable some dialect-specific processing # like using backticks &quot;`&quot; as delimiters in BigQuery tables. dialect: Optional[str] = None, # Setting this will make parser use this schema for every table that # does not specify schema. default_schema: Optional[str] = None ) -&gt; Optional[SqlMeta]  "},{"title":"SQL dialects​","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#sql-dialects","content":"Optional dialect can be specified when using the parser to specify a specific flavor of SQL statement that is required to be parsed. The following dialects are currently available: ansibigqueryhivemssqlmysqlpostgrespostgresqlredshiftsnowflakesqlite If no dialect is specified, the dialect defaults to generic which parses generic SQL statements. "},{"title":"Default databases and schemas​","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#default-databases-and-schemas","content":"SQL processing engines and databases sometimes rely on some implicit information. For example, they often allow you to set current database or schema, instead of forcing you to specify fully-qualified table name every time you're refering to it. For this reason, bare SQL parser might be insufficient to fully understand which tables the query refers to. We recommend to process the data that you acquired from SQL parser to take that into account. "},{"title":"Usage​","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#usage","content":"OpenLineage SQL parser is available as part of the integrations that contains extractors that need to parse SQL statements. An example would be Apache Airflow integrations which houses multiple operators that executes SQL statements. SQL parser is included as part of the library for such integrations (e.g. integration-airflow). However, you can explicitly install and use the SQL parser via pip: pip install openlineage-sql  For details about using the SQL parser, please refer to its git README There is also a python tester script here which you can use to run parsing tests against any arbitrary SQL statements to verify whether the SQL parser can properly parse them. "},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/development/developing/python/troubleshooting/logging","content":"","keywords":""},{"title":"Further readings​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/python/troubleshooting/logging#further-readings","content":"https://docs.python.org/3/library/logging.htmlhttps://realpython.com/python-logging/ "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/docs/getting-started","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#prerequisites","content":"Before you begin, make sure you have installed: Docker 17.05+Docker Compose info In this guide, we'll be using Marquez as the OpenLineage HTTP backend and running the HTTP server via Docker. "},{"title":"Run Marquez with Docker​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#run-marquez-with-docker","content":"The easiest way to get up and running with Marquez is Docker. Check out the Marquez source code and run the ./docker/up.sh script: $ git clone git@github.com:MarquezProject/marquez.git &amp;&amp; cd marquez $ ./docker/up.sh  info Pass the --build flag to the script to build images from source, or --tag X.Y.Z to use a tagged image. To view the Marquez UI and verify it's running, open http://localhost:3000. The UI enables you to discover dependencies between jobs and the datasets they produce and consume via the lineage graph, view run-level metadata of current and previous job runs, and much more. "},{"title":"Collect Run-Level Metadata​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#collect-run-level-metadata","content":"Marquez is an LF AI &amp; DATA incubation project to collect, aggregate, and visualize a data ecosystem’s metadata. Marquez is the reference implementation of the OpenLineage standard. In this example, we show how you can collect dataset and job metadata using Marquez, Using the LineageAPI. When you submit a lineage event, you first need to define an unique run ID that would look something like d46e465b-d358-4d32-83d4-df660ff614dd. This is usually in UUID format, and should be unique. This run ID will enable the tracking of run-level metadata over time for a job which may have a name, like my-job. So, let's get started! info The example shows how to collect metadata via direct HTTP API calls using curl. But, you can also get started using our client library for Java or Python. "},{"title":"Step 1: Start a Run​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#step-1-start-a-run","content":"Use the run ID d46e465b-d358-4d32-83d4-df660ff614dd to start the run for my-job with my-input as the input dataset: REQUEST​ $ curl -X POST http://localhost:5000/api/v1/lineage \\ -H 'Content-Type: application/json' \\ -d '{ &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-job&quot; }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-input&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }'  RESPONSE​ 201 CREATED "},{"title":"Step 2: Complete a Run​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#step-2-complete-a-run","content":"Use d46e465b-d358-4d32-83d4-df660ff614dd to complete the run for my-job with my-output as the output dataset. We also specify the schema facet to collect the schema for my-output before marking the run as completed. Note, you don't have to specify the input dataset my-input again for the run since it already has been associated with the run ID: REQUEST​ $ curl -X POST http://localhost:5000/api/v1/lineage \\ -H 'Content-Type: application/json' \\ -d '{ &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-job&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-output&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;a&quot;, &quot;type&quot;: &quot;VARCHAR&quot;}, { &quot;name&quot;: &quot;b&quot;, &quot;type&quot;: &quot;VARCHAR&quot;} ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }'  RESPONSE​ 201 CREATED "},{"title":"View Collected Lineage Metadata​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#view-collected-lineage-metadata","content":""},{"title":"Search Job Metadata​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#search-job-metadata","content":"To view lineage metadata collected by Marquez, browse to the UI by visiting http://localhost:3000. Then, use the search bar in the upper right-side of the page and search for the job my-job. To view lineage metadata for my-job, click on the job from the drop-down list:  "},{"title":"View Job Metadata​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#view-job-metadata","content":"You should see the job namespace, name, my-input as an input dataset and my-output as an output dataset in the lineage graph and the job run marked as COMPLETED :  "},{"title":"View Input Dataset Metadata​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#view-input-dataset-metadata","content":"Finally, click on the output dataset my-output for my-job. You should see the dataset name, schema, and description:  "},{"title":"Summary​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#summary","content":"In this simple example, we showed you how to use Marquez to collect dataset and job metadata with Openlineage. We also walked you through the set of HTTP API calls to successfully mark a run as complete and view the lineage metadata collected with Marquez. "},{"title":"Next Steps​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#next-steps","content":"Take a look at Marquez's Airflow example to learn how to enable OpenLineage metadata collection for Airflow DAGs and troubleshoot failing DAGs using Marquez.Listen to Solving Data Lineage Tracking And Data Discovery At WeWork.Listen to Unlocking The Power of Data Lineage In Your Platform with OpenLineage. "},{"title":"Feedback​","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#feedback","content":"What did you think of this guide? We would love to hear feedback, and we can be found on the OpenLineage Slack. "},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/faq","content":"","keywords":""},{"title":"Is OpenLineage a metadata server?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-openlineage-a-metadata-server","content":"No. OpenLineage is, at its core, a specification for lineage metadata. But it also contains a collection of integrations, examples, and tools. If you are looking for a metadata server that can receive and analyze OpenLineage events, check out Marquez. "},{"title":"Is there room for another question on this page?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-there-room-for-another-question-on-this-page","content":"You bet! There's always room. Submit an issue or pull request using the edit button at the bottom. "},{"title":"About These Guides","type":0,"sectionRef":"#","url":"/docs/guides/about","content":"About These Guides The following tutorials take you through the process of exploiting the lineage metadata provided by Marquez and OpenLineage to solve common data engineering problems and make new analytical and historical insights into your pipelines. The first tutorial, &quot;Using OpenLineage with Spark,&quot; provides an introduction to OpenLineage's integration with Apache Spark. You will learn how to use Marquez and the OpenLineage standard to produce lineage metadata about jobs and datasets created using Spark and BigQuery in a Jupyter notebook environment. The second tutorial, &quot;Using OpenLineage with Airflow,&quot; shows you how to use OpenLineage on Apache Airflow to produce data lineage on supported operators to emit lineage events to Marquez backend. The tutorial also introduces you to the OpenLineage proxy to monitor the event data being emitted. The third tutorial, &quot;Backfilling Airflow DAGs Using Marquez,&quot; shows you how to use Marquez's Airflow integration and the Marquez CLI to backfill failing runs with the help of lineage metadata. You will learn how data lineage can be used to automate the backfilling process. The fourth tutorial, &quot;Using Marquez with dbt,&quot; takes you through the process of setting up Marquez's dbt integration to harvest metadata produced by dbt. You will learn how to create a Marquez instance, install the integration, configure your dbt installation, and test the configuration using dbt.","keywords":""},{"title":"Backfilling Airflow DAGs Using Marquez","type":0,"sectionRef":"#","url":"/docs/guides/airflow-backfill-dags","content":"","keywords":""},{"title":"Exploring Lineage Metadata using Marquez​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#exploring-lineage-metadata-using-marquez","content":""},{"title":"Prerequisites​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#prerequisites","content":"Sample data (for the dataset used here, follow the instructions in the Write Sample Lineage Metadata to Marquez section of Marquez's quickstart guide)Docker 17.05+Docker DesktopDocker Composejq info If you are using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. Also, port 3000 will need to be free if access to the Marquez web UI is desired. "},{"title":"Query the Lineage Graph​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#query-the-lineage-graph","content":"After running the seed command in the quickstart guide, check to make sure Marquez is up by visiting http://localhost:3000. The page should display an empty Marquez instance and a message saying there is no data. Also, it should be possible to see the server output from requests in the terminal window where Marquez is running. This window should remain open. As you progress through the tutorial, feel free to experiment with the web UI. Use truncated strings (e.g., &quot;example.etl_orders_7_days&quot; instead of &quot;job:food_delivery:example.etl_orders_7_days&quot;) to find the datasets referenced below. In Marquez, each dataset and job has its own globally unique node ID that can be used to query the lineage graph. The LineageAPI returns a set of nodes consisting of edges. An edge is directed and has a defined origin and destination. A lineage graph may contain the following node types: dataset:&lt;namespace&gt;:&lt;dataset&gt;, job:&lt;namespace&gt;:&lt;job&gt;. Start by querying the lineage graph of the seed data via the CLI. The etl_orders_7_days DAG has the node ID job:food_delivery:example.etl_orders_7_days. To see the graph, run the following in a new terminal window: $ curl -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_orders_7_days&quot;  Notice in the returned lineage graph that the DAG input datasets are public.categories, public.orders, and public.menus, while public.orders_7_days is the output dataset. The response should look something like this: { &quot;graph&quot;: [{ &quot;id&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;type&quot;: &quot;JOB&quot;, &quot;data&quot;: { &quot;type&quot;: &quot;BATCH&quot;, &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.etl_orders_7_days&quot; }, &quot;name&quot;: &quot;example.etl_orders_7_days&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:13.931946Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.categories&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menu_items&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menus&quot;} ], &quot;outputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders_7_days&quot;} ], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;description&quot;: &quot;Loads newly placed orders weekly.&quot;, &quot;latestRun&quot;: { &quot;id&quot;: &quot;5c7f0dc4-d3c1-4f16-9ac3-dc86c5da37cc&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:36.853459Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-06T14:54:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-06T14:57:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-06T14:54:14.037399Z&quot;, &quot;endedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;durationMs&quot;: 220000, &quot;args&quot;: {}, &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;facets&quot;: {} } }, &quot;inEdges&quot;: [ {&quot;origin&quot;: &quot;dataset:food_delivery:public.categories&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.orders&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.menus&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;} ], &quot;outEdges&quot;: [ {&quot;origin&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;destination&quot;: &quot;dataset:food_delivery:public.orders_7_days&quot;} ] } }, ...] }  To see a visualization of the graph, search the web UI with public.delivery_7_days. "},{"title":"Backfill a DAG Run​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#backfill-a-dag-run","content":" Figure 1: Backfilled daily table partitions To run a backfill for example.etl_orders_7_days using the DAG lineage metadata stored in Marquez, query the lineage graph for the upstream DAG where an error originated. In this case, the example.etl_orders DAG upstream of example.etl_orders_7_days failed to write some of the daily table partitions needed for the weekly food order trends report. To fix the weekly trends report, backfill the missing daily table partitions public.orders_2021_06_04, public.orders_2021_06_05, and public.orders_2021_06_06 using the Airflow CLI: # Backfill daily food orders $ airflow dags backfill \\ --start-date 2021-06-04 \\ --end-date 2021-06-06 \\ example.etl_orders   Figure 2: Airflow inter-DAG dependencies Then, using the script backfill.sh defined below, we can easily backfill all DAGs downstream of example.etl_orders: (Note: Make sure you have jq installed before running backfill.sh.) #!/bin/bash # # Backfill DAGs automatically using lineage metadata stored in Marquez. # # Usage: $ ./backfill.sh &lt;start-date&gt; &lt;end-date&gt; &lt;dag-id&gt; ​ set -e ​ # Backfills DAGs downstream of the given node ID, recursively. backfill_downstream_of() { node_id=&quot;${1}&quot; # Get out edges for node ID out_edges=($(echo $lineage_graph \\ | jq -r --arg NODE_ID &quot;${node_id}&quot; '.graph[] | select(.id==$NODE_ID) | .outEdges[].destination')) for out_edge in &quot;${out_edges[@]}&quot;; do # Run backfill if out edge is a job node (i.e. &lt;dataset&gt; =&gt; &lt;job&gt;) if [[ &quot;${out_edge}&quot; = job:* ]]; then dag_id=&quot;${out_edge##*:}&quot; echo &quot;backfilling ${dag_id}...&quot; airflow backfill --start_date &quot;${start_date}&quot; --end_date &quot;${start_date}&quot; &quot;${dag_id}&quot; fi # Follow out edges downstream, recursively backfill_downstream_of &quot;${out_edge}&quot; done } ​ start_date=&quot;${1}&quot; end_date=&quot;${2}&quot; dag_id=&quot;${3}&quot; ​ # (1) Build job node ID (format: 'job:&lt;namespace&gt;:&lt;job&gt;') node_id=&quot;job:food_delivery:${dag_id}&quot; ​ # (2) Get lineage graph lineage_graph=$(curl -s -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=${node_id}&quot;) ​ # (3) Run backfill backfill_downstream_of &quot;${node_id}&quot;  When run, the script should output all backfilled DAGs to the console: $ ./backfill.sh 2021-06-06 2021-06-06 example.etl_orders backfilling example.etl_orders_7_days... backfilling example.etl_delivery_7_days... backfilling example.delivery_times_7_days...  "},{"title":"Conclusion​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#conclusion","content":"The lineage metadata provided by Marquez can make the task of backfilling much easier. But lineage metadata can also help avoid the need to backfill altogether. Since Marquez collects DAG run metadata that can be viewed using the Runs API, building automated processes to check DAG run states and notify teams of upstream data quality issues is just one possible preventive measure. Explore Marquez's opinionated Metadata API and define your own automated process(es) for analyzing lineage metadata! Also, join our Slack channel or reach out to us on Twitter if you have questions. "},{"title":"Using OpenLineage with Airflow","type":0,"sectionRef":"#","url":"/docs/guides/airflow","content":"","keywords":""},{"title":"Setting up Local Airflow Environment using Docker Compose​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#setting-up-local-airflow-environment-using-docker-compose","content":"Airflow has a convenient way to setup and run a fully functional environment using docker compose. The followings are therefore required to be installed before we start this tutorial. "},{"title":"Prerequisites​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#prerequisites","content":"Docker 20.10.0+Docker DesktopDocker Compose info If you are using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. Also, port 3000 will need to be free if access to the Marquez web UI is desired. Use the following instructions to setup and run Airflow using docker-compose. First, let's start out by create a new directory which will contain all of our works. mkdir ~/airflow-ol cd ~/airflow-ol  And then, let's download the docker compose file that we'll be running in it. curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.3.3/docker-compose.yaml'  Open the file docker-compose.yaml which got downloaded, and edit the file to add an entry OPENLINEAGE_URL environment variable in line 61: ... --- version: '3' x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the &quot;build&quot; line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.3.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # For backward compatibility, with Airflow &lt;2.3 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0 AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} OPENLINEAGE_URL: ${OPENLINEAGE_URL:-} ...  This will allow a new environment variable OPENLINEAGE_URL to be passed to the docker containers, which is needed for the OpenLineage to work. Then, let's create the following directories which will be mounted and used by the docker compose that will start the Airflow. mkdir dags mkdir logs mkdir plugins  Also, create a file .env that will contain environment variable that is going to be used by Airflow to install additional python packages that is needed. In our tutorial, we are going to have openlineage-airflow be installed. echo &quot;_PIP_ADDITIONAL_REQUIREMENTS=openlineage-airflow&quot; &gt; .env  You also need to let OpenLineage know which backend to emit those lineage data into. echo &quot;OPENLINEAGE_URL=http://host.docker.internal:4433&quot; &gt;&gt; .env  The reason why we are setting the backend to host.docker.internal is because we are going to be running OpenLineage Proxy outside of airflow's docker environment, and on the host machine itself. The port 4433 is the port which the proxy will be listening for lineage data. "},{"title":"Setting up OpenLineage Proxy as receiving end​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#setting-up-openlineage-proxy-as-receiving-end","content":"OpenLineage Proxy is a simple tool that you can easily setup and run to receive for OpenLineage data. The proxy does not do anything other than display what it received into the standard output. Optionally, it can also forward the data into any OpenLineage backend via HTTP. Let's download the proxy code from git, and build it. cd ~ git clone https://github.com/OpenLineage/OpenLineage.git cd OpenLineage/proxy ./gradlew build  Now, copy the proxy.dev.yml and edit its content as the following, and save it as proxy.yml. # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. server: applicationConnectors: - type: http port: ${OPENLINEAGE_PROXY_PORT:-4433} adminConnectors: - type: http port: ${OPENLINEAGE_PROXY_ADMIN_PORT:-4434} logging: level: ${LOG_LEVEL:-INFO} appenders: - type: console proxy: source: openLineageProxyBackend streams: - type: Console - type: Http url: http://localhost:5000/api/v1/lineage  "},{"title":"Setting up Marquez​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#setting-up-marquez","content":"The last piece of the setup is the Marquez backend. Using Marquez's quickstart document, setup the Marquez environment. cd ~ git clone https://github.com/MarquezProject/marquez.git  "},{"title":"Running Everything​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-everything","content":""},{"title":"Running Marquez​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-marquez","content":"cd ~/marquez ./docker/up.sh  "},{"title":"Running OpenLineage proxy​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-openlineage-proxy","content":"cd ~/OpenLineage ./gradlew runShadow  "},{"title":"Running Airflow​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-airflow","content":"cd ~/airflow-ol docker-compose up   Running everything would be the Apache Airflow setup and emitting lineage data into OpenLineage Proxy, and OpenLineage Proxy forwarding those into Marquez, so we can both inspect the data payload entering, as well as see the lineage data in graph form. "},{"title":"Accessing Airflow UI​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#accessing-airflow-ui","content":"After everything is up and running, we can now login to Airflow's UI by opening up the browser, and accessing http://localhost:8080. Initial ID and password to login would be airflow/airflow. "},{"title":"Running an example DAG​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-an-example-dag","content":"When you log into Airflow UI, you will notice that there are several example DAGs already populated when it started up. We can start running some of them to see what kind of OpenLineage event they generate. "},{"title":"Running Bash Operator​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-bash-operator","content":"In the DAGs page, locate the example_bash_operator.  Clicke the ► button at the right, which will show up a popup. Select Trigger DAG to trigger and run the DAG manually. You should see DAG running, and eventually completing. "},{"title":"Check the OpenLineage events​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#check-the-openlineage-events","content":"Once everything is finished, you should be able to see a number of JSON data payloads output in OpenLineage proxy's console. INFO [2022-08-16 21:39:41,411] io.openlineage.proxy.api.models.ConsoleLineageStream: { &quot;eventTime&quot; : &quot;2022-08-16T21:39:40.854926Z&quot;, &quot;eventType&quot; : &quot;START&quot;, &quot;inputs&quot; : [ ], &quot;job&quot; : { &quot;facets&quot; : { }, &quot;name&quot; : &quot;example_bash_operator.runme_2&quot;, &quot;namespace&quot; : &quot;default&quot; }, &quot;outputs&quot; : [ ], &quot;producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;run&quot; : { &quot;facets&quot; : { &quot;airflow_runArgs&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;externalTrigger&quot; : true }, &quot;airflow_version&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;airflowVersion&quot; : &quot;2.3.3&quot;, &quot;openlineageAirflowVersion&quot; : &quot;0.12.0&quot;, &quot;operator&quot; : &quot;airflow.operators.bash.BashOperator&quot;, &quot;taskInfo&quot; : &quot;{'_BaseOperator__init_kwargs': {'task_id': 'runme_2', 'params': &lt;***.models.param.ParamsDict object at 0xffff7467b610&gt;, 'bash_command': 'echo \\&quot;example_bash_operator__runme_2__20220816\\&quot; &amp;&amp; sleep 1'}, '_BaseOperator__from_mapped': False, 'task_id': 'runme_2', 'task_group': &lt;weakproxy at 0xffff74676ef0 to TaskGroup at 0xffff7467ba50&gt;, 'owner': '***', 'email': None, 'email_on_retry': True, 'email_on_failure': True, 'execution_timeout': None, 'on_execute_callback': None, 'on_failure_callback': None, 'on_success_callback': None, 'on_retry_callback': None, '_pre_execute_hook': None, '_post_execute_hook': None, 'executor_config': {}, 'run_as_user': None, 'retries': 0, 'queue': 'default', 'pool': 'default_pool', 'pool_slots': 1, 'sla': None, 'trigger_rule': &lt;TriggerRule.ALL_SUCCESS: 'all_success'&gt;, 'depends_on_past': False, 'ignore_first_depends_on_past': True, 'wait_for_downstream': False, 'retry_delay': datetime.timedelta(seconds=300), 'retry_exponential_backoff': False, 'max_retry_delay': None, 'params': &lt;***.models.param.ParamsDict object at 0xffff7467b4d0&gt;, 'priority_weight': 1, 'weight_rule': &lt;WeightRule.DOWNSTREAM: 'downstream'&gt;, 'resources': None, 'max_active_tis_per_dag': None, 'do_xcom_push': True, 'doc_md': None, 'doc_json': None, 'doc_yaml': None, 'doc_rst': None, 'doc': None, 'upstream_task_ids': set(), 'downstream_task_ids': {'run_after_loop'}, 'start_date': DateTime(2021, 1, 1, 0, 0, 0, tzinfo=Timezone('UTC')), 'end_date': None, '_dag': &lt;DAG: example_bash_operator&gt;, '_log': &lt;Logger ***.task.operators (INFO)&gt;, 'inlets': [], 'outlets': [], '_inlets': [], '_outlets': [], '_BaseOperator__instantiated': True, 'bash_command': 'echo \\&quot;example_bash_operator__runme_2__20220816\\&quot; &amp;&amp; sleep 1', 'env': None, 'output_encoding': 'utf-8', 'skip_exit_code': 99, 'cwd': None, 'append_env': False}&quot; }, &quot;nominalTime&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot; : &quot;2022-08-16T21:39:38.005668Z&quot; }, &quot;parentRun&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/ParentRunFacet&quot;, &quot;job&quot; : { &quot;name&quot; : &quot;example_bash_operator&quot;, &quot;namespace&quot; : &quot;default&quot; }, &quot;run&quot; : { &quot;runId&quot; : &quot;39ad10d1-72d9-3fe9-b2a4-860c651b98b7&quot; } } }, &quot;runId&quot; : &quot;313b4e71-9cde-4c83-b641-dd6773bf114b&quot; } }  "},{"title":"Check Marquez​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#check-marquez","content":"You can also open up the browser and visit http://localhost:3000 to access Marquez UI, and take a look at the OpenLineage events originating from Airflow.  "},{"title":"Running other DAGs​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-other-dags","content":"Due to the length of this tutorial, we are not going to be running additional example DAGs, but you can try running them and it would be interesting to see how each of them are going to be emitting OpenLineage events. Please try running other examples like example_python_operator which will also emit OpenLineage events. Normally, DataLineage will be much more complete and useful if a DAG run involves certain datasets that either get used or created during the runtime of it. When you run those DAGs, you will be able to see the connection between different DAGs and Tasks touching the same dataset that will eventually turn into Data Lineage graph that may look something like this:  Currently, these are the Airflow operators that have extractors that can extract and emit OpenLineage events. PostgresOperatorMySqlOperatorBigQueryOperatorSnowflakeOperatorGreatExpectationsOperatorPythonOperator See additional Apache Examples for DAGs that you can run in Airflow for OpenLineage. "},{"title":"Troubleshooting​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#troubleshooting","content":"You might not see any data going through the proxy or via Marquez. In that case, please check the task log of Airflow and see if you see the following message: [2022-08-16, 21:23:19 UTC] {factory.py:122} ERROR - Did not find openlineage.yml and OPENLINEAGE_URL is not set. In that case, it means that the environment variable OPENLINEAGE_URL was not set properly, thus OpenLineage was not able to emit any events. Please make sure to follow instructions in setting up the proper environment variable when setting up the Airflow via docker compose.Sometimes, Marquez would not respond and fail to receive any data via its API port 5000. You should be able to notice that if you start receiving response code 500 from Marquez or the Marquez UI hangs. In that case, simply stop and restart Marquez. "},{"title":"Conclusion​","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#conclusion","content":"In this short tutorial, we have learned how to setup and run a simple Apache Airflow environment that can emit OpenLineage events during its DAG run. We have also monitored and received the lineage events using combination of OpenLineage proxy and Marquez. We hope this tutorial was helpful in understanding how Airflow could be setup with OpenLineage and how you can easily monitor its data and end result using proxy and Marquez. "},{"title":"Using Marquez with dbt","type":0,"sectionRef":"#","url":"/docs/guides/dbt","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#prerequisites","content":"dbtDocker DesktopgitGoogle Cloud Service account Google Cloud Service account JSON key file Note: your Google Cloud account should have access to BigQuery and read/write access to your GCS bucket. Giving your key file an easy-to-remember name (bq-dbt-demo.json) is recommended. Finally, if using macOS Monterey (macOS 12), you will need to release port 5000 by disabling the AirPlay Receiver. "},{"title":"Instructions​","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#instructions","content":"First, run through this excellent dbt tutorial. It explains how to create a BigQuery project, provision a service account, download a JSON key, and set up a local dbt environment. The rest of this example assumes the existence of a BigQuery project where models can be run, as well as proper configuration of dbt to connect to the project. Next, start a local Marquez instance to store lineage metadata. Make sure Docker is running, and then clone the Marquez repository: git clone https://github.com/MarquezProject/marquez.git &amp;&amp; cd marquez ./docker/up.sh  Check to make sure Marquez is up by visiting http://localhost:3000. The page should display an empty Marquez instance and a message saying there is no data. Also, it should be possible to see the server output from requests in the terminal window where Marquez is running. This window should remain open. Now, in a new terminal window/pane, clone the following GitHub project, which contains some database models: git clone https://github.com/rossturk/stackostudy.git &amp;&amp; cd stackostudy  Now it is time to install dbt and its integration with OpenLineage. Doing this in a Python virtual environment is recommended. To create one and install necessary packages, run the following commands: python -m venv virtualenv source virtualenv/bin/activate pip install dbt dbt-openlineage  Keep in mind that dbt learns how to connect to a BigQuery project by looking for a matching profile in ~/.dbt/profiles.yml. Create or edit this file so it contains a section with the project's BigQuery connection details. Also, point to the location of the JSON key for the service account. Consult this section in the dbt documentation for more help with dbt profiles. At this point, profiles.yml should look something like this: stackostudy: target: dev outputs: dev: type: bigquery method: service-account keyfile: /Users/rturk/.dbt/dbt-example.json project: dbt-example dataset: stackostudy threads: 1 timeout_seconds: 300 location: US priority: interactive  The dbt debug command checks to see that everything has been configured correctly. Running it now should produce output like the following: % dbt debug Running with dbt=0.20.1 dbt version: 0.20.1 python version: 3.8.12 python path: /opt/homebrew/Cellar/dbt/0.20.1_1/libexec/bin/python3 os info: macOS-11.5.2-arm64-arm-64bit Using profiles.yml file at /Users/rturk/.dbt/profiles.yml Using dbt_project.yml file at /Users/rturk/projects/stackostudy/dbt_project.yml ​ Configuration: profiles.yml file [OK found and valid] dbt_project.yml file [OK found and valid] ​ Required dependencies: - git [OK found] ​ Connection: method: service-account database: stacko-study schema: stackostudy location: US priority: interactive timeout_seconds: 300 maximum_bytes_billed: None Connection test: OK connection ok  "},{"title":"Important Details​","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#important-details","content":"Some important conventions should be followed when designing dbt models for use with OpenLineage. Following these conventions will help ensure that OpenLineage collects the most complete metadata possible. First, any datasets existing outside the dbt project should be defined in a schema YAML file inside the models/ directory: version: 2 ​ sources: - name: stackoverflow database: bigquery-public-data schema: stackoverflow tables: - name: posts_questions - name: posts_answers - name: users - name: votes  This contains the name of the external dataset - in this case, bigquery-public-datasets - and lists the tables that are used by the models in this project. The name of the file does not matter, as long as it ends with .yml and is inside models/. Hardcoding dataset and table names into queries can result in incomplete data. When writing queries, be sure to use the {{ ref() }} and {{ source() }} jinja functions when referring to data sources. The {{ ref() }} function can be used to refer to tables within the same model, and the {{ source() }} function refers to tables we have defined in schema.yml. That way, dbt will properly keep track of the relationships between datasets. For example, to select from both an external dataset and one in this model: select * from {{ source('stackoverflow', 'posts_answers') }} where parent_id in (select id from {{ ref('filtered_questions') }} )  "},{"title":"Understanding and Using Facets","type":0,"sectionRef":"#","url":"/docs/guides/facets","content":"","keywords":""},{"title":"Standard Facets​","type":1,"pageTitle":"Understanding and Using Facets","url":"/docs/guides/facets#standard-facets","content":"Run Facets​ nominalTime: Captures the time this run is scheduled for. This is a typical usage for time based scheduled job. The job has a nominal schedule time that will be different from the actual time it is running at. parent: Captures the parent job and Run when the run was spawn from a parent run. For example in the case of Airflow, there's a run for the DAG that then spawns runs for individual tasks that would refer to the parent run as the DAG run. Similarly when a SparkOperator starts a Spark job, this creates a separate run that refers to the task run as its parent. errorMessage: Captures potential error message, programming language - and optionally stack trace - with which the run failed. Job Facets​ sourceCodeLocation: Captures the source code location and version (e.g., the git sha) of the job. sourceCode: Captures the language (e.g., Python) and actual source code of the job. sql: Capture the SQL query if this job is a SQL query. ownership: Captures the owners of the job. Dataset Facets​ schema: Captures the schema of the dataset. dataSource: Captures the database instance containing this dataset (e.g., Database schema, Object store bucket, etc.) lifecycleStateChange: Captures the lifecycle states of the dataset (e.g., alter, create, drop, overwrite, rename, truncate). version: Captures the dataset version when versioning is defined by database (e.g., Iceberg snapshot ID). columnLineage: Captures the column-level lineage. ownership: Captures the owners of the dataset. Input Dataset Facets​ dataQualityMetrics: Captures dataset-level and column-level data quality metrics when scanning a dataset whith a DataQuality library (row count, byte size, null count, distinct count, average, min, max, quantiles). dataQualityAssertions: Captures the result of running data tests on a dataset or its columns. Output Dataset Facets​ outputStatistics: Captures the size of the output written to a dataset (row count and byte size). "},{"title":"Using OpenLineage with Spark","type":0,"sectionRef":"#","url":"/docs/guides/spark","content":"","keywords":""},{"title":"Running Spark with OpenLineage​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#running-spark-with-openlineage","content":""},{"title":"Prerequisites​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#prerequisites","content":"Docker DesktopgitGoogle Cloud Service account Google Cloud Service account JSON key file Note: your Google Cloud account should have access to BigQuery and read/write access to your GCS bucket. Giving your key file an easy-to-remember name (bq-spark-demo.json) is recommended. Finally, if using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. "},{"title":"Instructions​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#instructions","content":"Clone the OpenLineage project, navigate to the spark directory, and create a directory for your Google Cloud Service credentials: git clone https://github.com/OpenLineage/OpenLineage cd integration/spark mkdir -p docker/notebooks/gcs  Copy your Google Cloud Service credentials file into that directory, then run: docker-compose up  This launches a Jupyter notebook with Spark as well as a Marquez API endpoint already installed to report lineage. Once the notebook server is up and running, you should see something like the following in the logs: notebook_1 | [I 21:43:39.014 NotebookApp] Jupyter Notebook 6.4.4 is running at: notebook_1 | [I 21:43:39.014 NotebookApp] http://082cb836f1ec:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.014 NotebookApp] or http://127.0.0.1:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.015 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).  Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from this one) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go.  Click on the notebooks directory, then click on the New button to create a new Python 3 notebook.  In the first cell in the window paste the below text. Update the GCP project and bucket names and the service account credentials file, then run the code: from pyspark.sql import SparkSession import urllib.request # Download dependencies for BigQuery and GCS gc_jars = ['https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.1.1/gcs-connector-hadoop3-2.1.1-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/bigquery-connector/hadoop3-1.2.0/bigquery-connector-hadoop3-1.2.0-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.22.2/spark-bigquery-with-dependencies_2.12-0.22.2.jar'] files = [urllib.request.urlretrieve(url)[0] for url in gc_jars] # Set these to your own project and bucket project_id = 'bq-openlineage-spark-demo' gcs_bucket = 'bq-openlineage-spark-demo-bucket' credentials_file = '/home/jovyan/notebooks/gcs/bq-spark-demo.json' spark = (SparkSession.builder.master('local').appName('openlineage_spark_test') .config('spark.jars', &quot;,&quot;.join(files)) # Install and set up the OpenLineage listener .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.3.+') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.openlineage.host', 'http://marquez-api:5000') .config('spark.openlineage.namespace', 'spark_integration') # Configure the Google credentials and project id .config('spark.executorEnv.GCS_PROJECT_ID', project_id) .config('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS', '/home/jovyan/notebooks/gcs/bq-spark-demo.json') .config('spark.hadoop.google.cloud.auth.service.account.enable', 'true') .config('spark.hadoop.google.cloud.auth.service.account.json.keyfile', credentials_file) .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') .config('spark.hadoop.fs.AbstractFileSystem.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS') .config(&quot;spark.hadoop.fs.gs.project.id&quot;, project_id) .getOrCreate())  Most of this is boilerplate for installing the BigQuery and GCS libraries in the notebook environment. This also sets the configuration parameters to tell the libraries what GCP project to use and how to authenticate with Google. The parameters specific to OpenLineage are the four already mentioned: spark.jars.packages, spark.extraListeners, spark.openlineage.host, spark.openlineage.namespace. Here, the host has been configured to be the marquez-api container started by Docker. With OpenLineage configured, it's time to get some data. The below code populates Spark DataFrames with data from two COVID-19 public data sets. Create a new cell in the notebook and paste the following: from pyspark.sql.functions import expr, col mask_use = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data:covid19_nyt.mask_use_by_county') \\ .load() \\ .select(expr(&quot;always + frequently&quot;).alias(&quot;frequent&quot;), expr(&quot;never + rarely&quot;).alias(&quot;rare&quot;), &quot;county_fips_code&quot;) opendata = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data.covid19_open_data.covid19_open_data') \\ .load() \\ .filter(&quot;country_name == 'United States of America'&quot;) \\ .filter(&quot;date == '2021-10-31'&quot;) \\ .select(&quot;location_key&quot;, expr('cumulative_deceased/(population/100000)').alias('deaths_per_100k'), expr('cumulative_persons_fully_vaccinated/(population - population_age_00_09)').alias('vaccination_rate'), col('subregion2_code').alias('county_fips_code')) joined = mask_use.join(opendata, 'county_fips_code') joined.write.mode('overwrite').parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/')  Some background on the above: the covid19_open_data table is being filtered to include only U.S. data and data for Halloween 2021. The deaths_per_100k data point is being calculated using the existing cumulative_deceased and population columns and the vaccination_rate using the total population, subtracting the 0-9 year olds, since they were ineligible for vaccination at the time. For the mask_use_by_county data, &quot;rarely&quot; and &quot;never&quot; data are being combined into a single number, as are &quot;frequently&quot; and &quot;always.&quot; The columns selected from the two datasets are then stored in GCS. Now, add a cell to the notebook and paste this line: spark.read.parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/').count()  The notebook should print a warning and a stacktrace (probably a debug statement), then return a total of 3142 records. Now that the pipeline is operational it is available for lineage collection. The docker-compose.yml file that ships with the OpenLineage repo includes only the Jupyter notebook and the Marquez API. To explore the lineage visually, start up the Marquez web project. Without terminating the existing docker containers, run the following command in a new terminal: docker run --network spark_default -p 3000:3000 -e MARQUEZ_HOST=marquez-api -e MARQUEZ_PORT=5000 --link marquez-api:marquez-api marquezproject/marquez-web:0.19.1  Next, open a new browser tab and navigate to http://localhost:3000, which should look like this:  Note: the spark_integration namespace is automatically chosen because there are no other namespaces available. Three jobs are listed on the jobs page of the UI. They all start with openlineage_spark_test, which is the appName passed to the SparkSession when the first cell of the notebook was built. Each query execution or RDD action is represented as a distinct job and the name of the action is appended to the application name to form the name of the job. Clicking on the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command node calls up the lineage graph for our notebook:  The graph shows that the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command job reads from two input datasets, bigquery-public-data.covid19_nyt.mask_use_by_county and bigquery-public-data.covid19_open_data.covid19_open_data, and writes to a third dataset, /demodata/covid_deaths_and_mask_usage. The namespace is missing from that third dataset, but the fully qualified name is gs://&lt;your_bucket&gt;/demodata/covid_deaths_and_mask_usage. The bottom bar shows some interesting data that was collected from the Spark job. Dragging the bar up expands the view to offer a closer look.  Two facets always collected from Spark jobs are the spark_version and the spark.logicalPlan. The first simply reports what version of Spark was executing, as well as the version of the openlineage-spark library. This is helpful for debugging job runs. The second facet is the serialized optimized LogicalPlan Spark reports when the job runs. Spark’s query optimization can have dramatic effects on the execution time and efficiency of the query job. Tracking how query plans change over time can significantly aid in debugging slow queries or OutOfMemory errors in production. Clicking on the first BigQuery dataset provides information about the data:  One can see the schema of the dataset as well as the datasource. Similar information is available about the dataset written to in GCS:  As in the BigQuery dataset, one can see the output schema and the datasource — in this case, the gs:// scheme and the name of the bucket written to. In addition to the schema, one can also see a stats facet, reporting the number of output records and bytes as -1. The VERSIONS tab on the bottom bar would display multiple versions if there were any (not the case here). Clicking on the version shows the same schema and statistics facets, but they are specific to the version selected.  In production, this dataset would have many versions, as each time a job runs a new version of the dataset is created. This permits the tracking of changes to the statistics and schema over time, aiding in debugging slow jobs or data quality issues and job failures. The final job in the UI is a HashAggregate job. This represents the count() method called at the end to show the number of records in the dataset. Rather than a count(), this could easily be a toPandas() call or some other job that reads and processes that data -- perhaps one that stores output back into GCS or updates a Postgres database, publishes a new model, etc. Regardless of where the output gets stored, the OpenLineage integration allows one to see the entire lineage graph, unifying datasets in object stores, relational databases, and more traditional data warehouses. "},{"title":"Conclusion​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#conclusion","content":"The Spark integration from OpenLineage offers users insights into graphs of datasets stored in object stores like S3, GCS, and Azure Blob Storage, as well as BigQuery and relational databases like Postgres. Now with support for Spark 3.1, OpenLineage offers visibility into more environments, such as Databricks, EMR, and Dataproc clusters. "},{"title":"Apache Airflow","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/","content":"","keywords":""},{"title":"How does Airflow work with OpenLineage?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-does-airflow-work-with-openlineage","content":"Understanding complex inter-DAG dependencies and providing up-to-date runtime visibility into DAG execution can be challenging. OpenLineage integrates with Airflow to collect DAG lineage metadata so that inter-DAG dependencies are easily maintained and viewable via a lineage graph, while also keeping a catalog of historical runs of DAGs.  The DAG metadata collected can answer questions like: Why has a DAG failed?Why has the DAG runtime increased after a code change?What are the upstream dependencies of a DAG? "},{"title":"How can I use this integration?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-can-i-use-this-integration","content":"To instrument your Airflow instance with OpenLineage, follow these instructions. "},{"title":"How to add lineage coverage for more operators?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-to-add-lineage-coverage-for-more-operators","content":"OpenLineage provides a set of extractors that extract lineage from operators. If you want to add lineage coverage for your own custom operators, follow these instructions to add lineage to operators. If you want to add coverage for operators you can not modify, follow instructions to add custom extractors. If you want to expose lineage as a one off in your workflow, you can also manually annotate the tasks in your DAG. "},{"title":"Where can I learn more?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#where-can-i-learn-more","content":"Take a look at Marquez's Airflow example to learn how to enable OpenLineage metadata collection for Airflow DAGs and troubleshoot failing DAGs using Marquez.Watch Data Lineage with OpenLineage and Airflow "},{"title":"Feedback​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#feedback","content":"You can reach out to us on slack and leave us feedback! "},{"title":"OpenLineage Integrations","type":0,"sectionRef":"#","url":"/docs/integrations/about","content":"","keywords":""},{"title":"Capability Matrix​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#capability-matrix","content":"caution This matrix is not yet complete. The matrix below shows the relationship between an input facet and various mechanisms OpenLineage uses to gather metadata. Not all mechanisms collect data to fill in all facets, and some facets are specific to one integration. ✔️: The mechanism does implement this facet. ✖️: The mechanism does not implement this facet. An empty column means it is not yet documented if the mechanism implements this facet. Mechanism\tIntegration\tMetadata Gathered\tInputDatasetFacet\tOutputDatasetFacet\tSqlJobFacet\tSchemaDatasetFacet\tDataSourceDatasetFacet\tDataQualityMetricsInputDatasetFacet\tDataQualityAssertionsDatasetFacet\tSourceCodeJobFacet\tExternalQueryRunFacet\tDocumentationDatasetFacet\tSourceCodeLocationJobFacet\tDocumentationJobFacet\tParentRunFacetSnowflakeOperator*\tAirflow Extractor\tLineage Job duration\t✔️\t✔️\t✔️\t✔️\t✔️\t✖️\t✖️ BigQueryOperator**\tAirflow Extractor\tLineage Schema details Job duration\t✔️\t✔️ ✔️ PostgresOperator*\tAirflow Extractor\tLineage Job duration\t✔️\t✔️\t✔️\t✔️\t✔️ SqlCheckOperators\tAirflow Extractor\tLineage Data quality assertions\t✔️\t✖️\t✔️\t✔️\t✔️\t✔️\t✔️ dbt\tdbt Project Files\tLineage Row count Byte count.\t✔️ Great Expectations\tAction\tData quality assertions\t✔️ ✔️\t✔️ Spark\tSparkListener\tSchema Row count Column lineage\t✔️ Snowflake***\tAccess History\tLineage  Uses the Rest SQL parser Uses the BigQuery API * Uses Snowflake query logs "},{"title":"Compatibility matrix​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#compatibility-matrix","content":"This matrix shows which data sources are known to work with each integration, along with the minimum versions required in the target system or framework. Platform\tVersion\tData SourcesApache Airflow\t1.10+ 2.0+\tPostgreSQL MySQL Snowflake Amazon Athena Amazon Redshift Amazon SageMaker Amazon S3 Copy Google BigQuery Great Expectations SFTP Apache Spark\t2.4+\tJDBC HDFS Google Cloud Storage Google BigQuery Amazon S3 Azure Blob Storage Azure Data Lake Gen2 Azure Synapse dbt\t0.20+\tSnowflake Google BigQuery "},{"title":"Integration strategies​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integration-strategies","content":"info This section could use some more detail! You're welcome to contribute using the Edit link at the bottom. "},{"title":"Integrating with pipelines​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integrating-with-pipelines","content":" "},{"title":"Integrating with data sources​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integrating-with-data-sources","content":" "},{"title":"Custom extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/custom-extractors","content":"","keywords":""},{"title":"Interface​","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#interface","content":"Custom extractors have to derive from BaseExtractor. Extractors have three methods to implement: extract, extract_on_complete and get_operator_classnames. The last one is a classmethod that is used to provide list of operators that your extractor can get lineage from. For example: @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['PostgresOperator']  If the name of the operator matches one of the names on the list, the extractor will be instantiated - with operator provided in the extractor's self.operator property - and both extract and extract_on_complete methods will be called. They are used to provide actual information data. The difference is that extract is called before operator's executemethod, while extract_on_complete is called after. This can be used to extract any additional information that the operator sets on it's own properties. Good example is SnowflakeOperator that sets query_ids after execution. Both methods return TaskMetadata structure: @attr.s class TaskMetadata: name: str = attr.ib() # deprecated inputs: List[Dataset] = attr.ib(factory=list) outputs: List[Dataset] = attr.ib(factory=list) run_facets: Dict[str, BaseFacet] = attr.ib(factory=dict) job_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)  Inputs and outputs are lists of plain OpenLineage datasets run_facets and job_facets are dictionaries of optional JobFacets and RunFacets that would be attached to the job - for example, you might want to attach SqlJobFacet if your operator is executing SQL. To learn more about facets in OpenLineage, please visit this section. "},{"title":"Registering custom extractor​","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#registering-custom-extractor","content":"OpenLineage integration does not know that you've provided an extractor unless you'll register it. The way to do that is to add them to OPENLINEAGE_EXTRACTORS environment variable. OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass  If you have multiple custom extractors, separate the paths with comma (;) OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass  Optionally, you can separate them with whitespace. It's useful if you're providing them as part of some YAML file. OPENLINEAGE_EXTRACTORS: &gt;- full.path.to.FirstExtractor; full.path.to.SecondExtractor  "},{"title":"Adding extractor to OpenLineage Airflow integration package​","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#adding-extractor-to-openlineage-airflow-integration-package","content":"All Openlineage extractors are defined in this path. In order to add new extractor you should put your code in this directory. Additionally, you need to add the class to _extractors list in extractors.py, e.g.: _extractors = list( filter( lambda t: t is not None, [ try_import_from_string( 'openlineage.airflow.extractors.postgres_extractor.PostgresExtractor' ), ... # other extractors are listed here + try_import_from_string( + 'openlineage.airflow.extractors.new_extractor.ExtractorClass' + ), ] ) )  "},{"title":"Debugging issues​","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#debugging-issues","content":"There are two common problems associated with custom extractors. First, is wrong path provided to OPENLINEAGE_EXTRACTORS. The path needs to be exactly the same as one you'd use from your code. If the path is wrong, the extractor won't get imported and OpenLineage events won't be emitted. Second one, and maybe more insidious, are imports from Airflow. Due to the fact that OpenLineage code gets instantiated when Airflow worker itself starts, any import from Airflow can be unnoticeably cyclical. This causes OpenLineage extraction to fail. To avoid this issue, import from Airflow only locally - in extract or extract_on_complete methods. If you need imports for type checking, guard them behind typing.TYPE_CHECKING. You can also check Development section to learn more about how to setup development environment and create tests. "},{"title":"Default extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/default-extractors","content":"","keywords":""},{"title":"Implementing Default Extractors​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#implementing-default-extractors","content":"To implement a default extractor, first you need an operator class. In this example, we’ll use the DfToGcsOperator, a custom operator created by the Astronomer Data team to load arbitrary dataframes to our GCS bucket. We’ll implement both get_openlineage_facets_on_start() and get_openlineage_facets_on_complete() for our custom operator. The specific details of the implementation will vary from operator to operator, but there will always be five basic steps that these functions will share. Both the methods return an OperatorLineage object, which itself is a collection of facets. Four of the five steps mentioned above are creating these facets where necessary, and the fifth is creating the DataSourceDatasetFacet. First, though, we’ll need to import some OpenLineage objects: from openlineage.airflow.extractors.base import OperatorLineage from openlineage.client.facet import ( DataSourceDatasetFacet, SchemaDatasetFacet, SchemaField, ) from openlineage.client.run import Dataset  Now, we’ll start building the facets for the OperatorLineage object in the get_openlineage_facets_on_start() method. "},{"title":"1. DataSourceDatasetFacet​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#1-datasourcedatasetfacet","content":"The DataSourceDatasestFacet is a simple object, containing two fields, name and uri, which should be populated with the unique name of the data source and the URI. We’ll make two of these objects, an input_source to specify where the data came from, and an output_source to specify where the data is going. A quick note about the philosophy behind the name and uri: the uri is built from the namespace and the name, and each is expected to be unique with respect to its environment. This means a namespace should be globally unique in the OpenLineage universe, and the name unique within the namespace. The two are then concatenated to form the uri, so that uri = namespace + name. The full OpenLineage naming spec can be found here. In our case, the input name will be the table we are pulling data from, self.table, and the namespace will be our self.data_source. input_source = DataSourceDatasetFacet( name=self.table, uri=&quot;://&quot;.join([self.data_source, self.table]), )  The output data source object’s name will always be the base path given to the operator, self.base_path. The namespace is always in GCS, so we use the OpenLineage spec’s gs:// as the scheme and our bucket as the authority, giving us gs://{ol_bucket}. The uri is simply the concatenation of the two. if not self.bucket: ol_bucket = get_env_bucket() else: ol_bucket = self.bucket output_namespace = &quot;gs://&quot; + ol_bucket output_name = self.base_path output_uri = &quot;/&quot;.join( [ output_namespace, output_name, ] ) output_source = DataSourceDatasetFacet( name=output_name, uri=output_uri, )  "},{"title":"2. Inputs​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#2-inputs","content":"Next we’ll create the input dataset object. As we are moving data from a dataframe to GCS in this operator, we’ll make sure that we are capturing all the info in the dataframe being extracted in a Dataset. To create the Dataset object, we’ll need namespace, name, and facets objects. The first two are strings, and facets is a dictionary. Our namespace will come from the operator, where we use self.data_source again. The name parameter for this facet will be the table, again coming from the operator’s parameter list. The facets will contain two entries, the first is our DataSourceDatasetFacet with the key &quot;datasource&quot; coming from the previous step and input_source as the value. The second has the key &quot;schema&quot;, with the value being a SchemaDatasetFacet, which itself is a collection of SchemaField objects, one for each column, created via a list comprehension over the operator's self.col_types parameter. The inputs parameter to OperatorLineage is a list of Dataset objects, so we’ll end up adding a single Dataset object to the list later. The creation of the Dataset object looks as follows: input_facet = { &quot;datasource&quot;: input_source, &quot;schema&quot;: SchemaDatasetFacet( fields=[ SchemaField(name=col_name, type=col_type) for col_name, col_type in self.col_types.items() ] ), } input = Dataset(namespace=self.data_source, name=self.table, facets=input_facet)  "},{"title":"3. Outputs​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#3-outputs","content":"Our output facet will look almost identical to the input facet, except it will use the output_source we previously created, and will also have a different namespace. Our output facet object will be built as follows: output_facet = { &quot;datasource&quot;: output_source, &quot;schema&quot;: SchemaDatasetFacet( fields=[ SchemaField(name=col_name, type=col_type) for col_name, col_type in self.col_types.items() ] ), } output = Dataset( namespace=output_namespace, name=output_name, facets=output_facet, )  "},{"title":"4. Job Facets​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#4-job-facets","content":"A Job in OpenLineage is a process definition that consumes and produces datasets. The Job evolves over time, and that change is captured when the job runs. This means the facets we would want to capture in at the Job level are independent of the state the Job is in. Custom facets can be created to capture this job data. For our operator, we stuck with pre-existing job facets, the DocumentationJobFacet and the OwnershipJobFacet: job_facets = { &quot;documentation&quot;: DocumentationJobFacet( description=f&quot;&quot;&quot; Takes data from the data source {input_uri} and puts it in GCS at the path: {output_uri} &quot;&quot;&quot; ), &quot;ownership&quot;: OwnershipJobFacet( owners=[OwnershipJobFacetOwners(name=self.owner, type=self.email)] ) }  "},{"title":"5. Run facets​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#5-run-facets","content":"A Run is and instance of a Job execution. For example, when an Airflow Operator begins execution, the Run state of the OpenLineage Job transitions to Start, then to Running. When writing an Extractor, this means a Run facet should contain information pertinent to the specific instance of the job, something that could change every Run. In this example, we will emit an error message when there is an empty dataframe, using the existing ErrorMessageRunFacet. starting_facets.run_facets = { &quot;errorMessage&quot;: ErrorMessageRunFacet( message=&quot;Empty dataframe, no artifact saved to GCS.&quot;, programmingLanguage=&quot;python&quot; ) }  "},{"title":"6. On Complete​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#6-on-complete","content":"Finally, we’ll implement the get_openlineage_metadata_on_complete() method. Most our work is already done for us, so we will start by calling get_openlineage_metadata_on_start() and then modify the returned object slightly before returning it again. The two main additions here are replacing the original SchemaDatasetFacet fields and adding a potential error message to the run_facets. For the SchemaDatasetFacet update, we replace the old fields facet with updated ones based on the now-filled-out df_meta dict, which is populated during the operator’s execute() method and is therefore unavailable to get_openlineage_metadata_on_start(). Because df_meta is already a list of SchemaField objects, we can set the property directly. Although we use a for-loop here, the operator ensures only one dataframe will ever be extracted per execute, so the for loop will only ever run once and we therefore do not have to worry about multiple input dataframes updating. The run_facets update is done only if there is an error, which is a mutually exclusive event to updating the fields facets. We pass the same message to this facet that is printed in the execute() method when an empty dataframe is found. This error message does not halt operator execution, as it gets added *after* execution, but it does create an alert in the OpenLineage UI. def get_openlineage_facets_on_complete(self, task_instance): &quot;&quot;&quot;Add lineage to DfToGcsOperator on task completion.&quot;&quot;&quot; starting_facets = self.get_openlineage_facets_on_start() if task_instance.task.df_meta is not None: for i in starting_facets.inputs: i.facets[&quot;SchemaDatasetFacet&quot;].fields = task_instance.task.df_meta else: starting_facets.run_facets = { &quot;errorMessage&quot;: ErrorMessageRunFacet( message=&quot;Empty dataframe, no artifact saved to GCS.&quot;, programmingLanguage=&quot;python&quot; ) } return starting_facets  And with that final piece of the puzzle, we have a working extractor for our custom operator! "},{"title":"Custom Facets​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#custom-facets","content":"The OpenLineage spec may not contain all the facets you need to write your extractor, in which case you will have to make your own custom facets. More on creating custom facets can be found here. "},{"title":"Testing​","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#testing","content":"See the doc on testing custom extractors. "},{"title":"Testing custom extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/extractor-testing","content":"","keywords":""},{"title":"Testing set-up​","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-set-up","content":"We’ll use the same extractor that we built in the blog post, the RedshiftDataExtractor. When testing an extractor, we want to verify a few different sets of assumptions. The first set of assumptions are about the TaskMetadata object being created, specifically verifying that the object is being built with the correct input and output datasets and relevant facets. This is done in OpenLineage via pytest, with appropriate mocking and patching for connections and objects. In the OpenLineage repository, extractor unit tests are found in under [integration/airflow/tests](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/tests). For custom extractors, these tests should go under a tests directory at the top level of your project hierarchy.  An Astro project directory structure, with extractors in an extractors/ folder under include/, and tests under a top-level tests/ folder. "},{"title":"Testing the TaskMetadata object​","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-the-taskmetadata-object","content":"For the RedshiftDataExtractor, this core extract test is actually run on extract_on_complete(), as the extract() method is empty. We’ll walk through a test function to see how we can ensure the output dataset is being built as expected (full test code here) # First, we add patching to mock our connection to Redshift. @mock.patch( &quot;airflow.providers.amazon.aws.operators.redshift_data.RedshiftDataOperator.hook&quot;, new_callable=PropertyMock, ) @mock.patch(&quot;botocore.client&quot;) def test_extract_e2e(self, mock_client, mock_hook): # Mock the descriptions we can expect from a real call. mock_client.describe_statement.return_value = self.read_file_json( &quot;tests/extractors/redshift_statement_details.json&quot; ) mock_client.describe_table.return_value = self.read_file_json( &quot;tests/extractors/redshift_table_details.json&quot; ) # Finish setting mock objects' expected values. job_id = &quot;test_id&quot; mock_client.execute_statement.return_value = {&quot;Id&quot;: job_id} mock_hook.return_value.conn = mock_client # Set the extractor and ensure that the extract() method is not returning anything, as expected. extractor = RedshiftDataExtractor(self.task) task_meta_extract = extractor.extract() assert task_meta_extract is None # Run an instance of RedshiftDataOperator with the predefined test values. self.ti.run() # Run extract_on_complete() with the task instance object. task_meta = extractor.extract_on_complete(self.ti) # Assert that the correct job_id was used in the client call. mock_client.describe_statement.assert_called_with(Id=job_id) # Assert there is a list of output datasets. assert task_meta.outputs # Assert there is only dataset in the list. assert len(task_meta.outputs) == 1 # Assert the output dataset name is the same as the table created by the operator query. assert task_meta.outputs[0].name == &quot;dev.public.fruit&quot; # Assert the output dataset has a parsed schema. assert task_meta.outputs[0].facets[&quot;schema&quot;].fields is not None # Assert the datasource is the correct Redshift URI. assert ( task_meta.outputs[0].facets[&quot;dataSource&quot;].name == f&quot;redshift://{CLUSTER_IDENTIFIER}.{REGION_NAME}:5439&quot; ) # Assert the uri is None (as it already exists in dataSource). assert task_meta.outputs[0].facets[&quot;dataSource&quot;].uri is None # Assert the schema fields match the numnber of fields of the table created by the operator query. assert len(task_meta.outputs[0].facets[&quot;schema&quot;].fields) == 3 # Assert the output statistics match the results of the operator query. assert ( OutputStatisticsOutputDatasetFacet( rowCount=1, size=11, ) == task_meta.outputs[0].facets['stats'] )  Most of the assertions above are straightforward, yet all are important in ensuring that no unexpected behavior occurs when building the metadata object. Testing each facet is important, as data or graphs in the UI can render incorrectly if the facets are wrong. For example, if the task_meta.outputs[0].facets[&quot;dataSource&quot;].name is created incorrectly in the extractor, then the operator’s task will not show up in the lineage graph, creating a gap in pipeline observability. "},{"title":"Testing private functions​","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-private-functions","content":"Private functions with any complexity beyond returning a string should be unit tested as well. An example of this is the _get_xcom_redshift_job_id() private function in the RedshiftDataExtractor. The unit test is shown below: @mock.patch(&quot;airflow.models.TaskInstance.xcom_pull&quot;) def test_get_xcom_redshift_job_id(self, mock_xcom_pull): self.extractor._get_xcom_redshift_job_id(self.ti) mock_xcom_pull.assert_called_once_with(task_ids=self.ti.task_id)  Unit tests do not have to be particularly complex, and in this instance the single assertion is enough to cover the expected behavior that the function was called only once. "},{"title":"Troubleshooting​","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#troubleshooting","content":"Even with unit tests, an extractor may still not be operating as expected. The easiest way to tell if data isn’t coming through correctly is if the UI elements are not showing up correctly in the Lineage tab. When testing code locally, Marquez can be used to inspect the data being emitted—or not being emitted. Using Marquez will allow you to figure out if the error is being caused by the extractor or the API. If data is being emitted from the extractor as expected but isn’t making it to the UI, then the extractor is fine and an issue should be opened up in OpenLineage. However, if data is not being emitted properly, it is likely that more unit tests are needed to cover extractor behavior. Marquez can help you pinpoint which facets are not being formed properly so you know where to add test coverage. "},{"title":"Using OpenLineage with older versions of Airflow","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/older","content":"","keywords":""},{"title":"Airflow 2.1 - 2.2​","type":1,"pageTitle":"Using OpenLineage with older versions of Airflow","url":"/docs/integrations/airflow/older#airflow-21---22","content":"Integration for those versions has limitations: it does not support tracking failed jobs, and job starts are registered only when job ends. To make OpenLineage work, in addition to installing openlineage-airflow you need to set your LineageBackendin your airflow.cfg or via environmental variable AIRFLOW__LINEAGE__BACKEND to openlineage.lineage_backend.OpenLineageBackend  "},{"title":"Manually annotated lineage","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/manual","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"Manually annotated lineage","url":"/docs/integrations/airflow/manual#example","content":"An operator insider the Airflow DAG can be annotated with inlets and outlets like - &quot;&quot;&quot;Example DAG demonstrating the usage of the extraction via Inlets and Outlets.&quot;&quot;&quot; import pendulum import datetime from airflow import DAG from airflow.operators.bash import BashOperator from airflow.lineage.entities import Table, File def create_table(cluster, database, name): return Table( database=database, cluster=cluster, name=name, ) t1 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t1&quot;) t2 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t2&quot;) t3 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t3&quot;) t4 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t4&quot;) f1 = File(url = &quot;http://randomfile&quot;) with DAG( dag_id='example_operator', schedule_interval='0 0 * * *', start_date=pendulum.datetime(2021, 1, 1, tz=&quot;UTC&quot;), dagrun_timeout=datetime.timedelta(minutes=60), params={&quot;example_key&quot;: &quot;example_value&quot;}, ) as dag: task1 = BashOperator( task_id='task_with_inlet_outlet', bash_command='echo &quot;{{ task_instance_key_str }}&quot; &amp;&amp; sleep 1', inlets=[t1, t2], outlets=[t3], ) task2 = BashOperator( task_id='task_with_inlet_outlet', bash_command='echo &quot;{{ task_instance_key_str }}&quot; &amp;&amp; sleep 1', inlets=[t3, f1], outlets=[t4], ) task1 &gt;&gt; task2 if __name__ == &quot;__main__&quot;: dag.cli()   The corresponding lineage graph will be -  (The image is shown with the Marquez UI (metadata collector of OpenLineage events). More info can be found here. Also note that the File entity is not captured by the lineage event currently.  "},{"title":"Conversion from Airflow Table entity to Openlineage Dataset​","type":1,"pageTitle":"Manually annotated lineage","url":"/docs/integrations/airflow/manual#conversion-from-airflow-table-entity-to-openlineage-dataset","content":"The naming convention followed here is: CLUSTER of the table entity becomes the namespace of OpenLineage's DatasetThe name of the dataset is formed by {{DATABASE}}.{{NAME}} where DATABASE and NAME are attributes specified by Airflow's Table entity. "},{"title":"Using the Airflow integration","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/usage","content":"","keywords":""},{"title":"Environment Variables​","type":1,"pageTitle":"Using the Airflow integration","url":"/docs/integrations/airflow/usage#environment-variables","content":"The following environment variables are available specifically for the Airflow integration. Name\tDescription\tSinceOPENLINEAGE_AIRFLOW_DISABLE_SOURCE_CODE\tSet to False if you want source code of callables provided in PythonOperator or BashOperator NOT to be included in OpenLineage events. OPENLINEAGE_EXTRACTORS\tThe optional list of extractors class in case you need to use custom extractors. For example: OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass OPENLINEAGE_NAMESPACE\tThe optional namespace that the lineage data belongs to. If not specified, defaults to default.\t USAGE​ When enabled, the integration will: On TaskInstance start, collect metadata for each task.Collect task input / output metadata (source, schema, etc.).Collect task run-level metadata (execution time, state, parameters, etc.)On TaskInstance complete, also mark the task as complete in Marquez. "},{"title":"dbt","type":0,"sectionRef":"#","url":"/docs/integrations/dbt","content":"","keywords":""},{"title":"How does dbt work with OpenLineage?​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#how-does-dbt-work-with-openlineage","content":"Fortunately, dbt already collects a lot of the data required to create and emit OpenLineage events. When it runs, it creates a target/manifest.json file containing information about jobs and the datasets they affect, and a target/run_results.json file containing information about the run-cycle. These files can be used to trace lineage and job performance. In addition, by using the create catalog command, a user can instruct dbt to create a target/catalog.json file containing information about dataset schemas. These files contain everything needed to trace lineage. However, the target/manifest.json and target/run_results.json files are only populated with comprehensive metadata after completion of a run-cycle. This integration is implemented as a wrapper script, dbt-ol, that calls dbt and, after the run has completed, collects information from the three json files and calls the OpenLineage API accordingly. For most users, enabling OpenLineage metadata collection can be accomplished by simply substituting dbt-ol for dbt when performing a run. "},{"title":"Preparing a dbt project for OpenLineage​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#preparing-a-dbt-project-for-openlineage","content":"First, we need to install the integration: pip3 install openlineage-dbt  Next, we specify where we want dbt to send OpenLineage events by setting the OPENLINEAGE_URL environment variable. For example, to send OpenLineage events to a local instance of Marquez, use: OPENLINEAGE_URL=http://localhost:5000  Finally, we can optionally specify a namespace where the lineage events will be stored. For example, to use the namespace &quot;dev&quot;: OPENLINEAGE_NAMESPACE=dev  "},{"title":"Running dbt with OpenLineage​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#running-dbt-with-openlineage","content":"To run your dbt project with OpenLineage collection, simply replace dbt with dbt-ol: dbt-ol run  The dbt-ol wrapper supports all of the standard dbt subcommands, and is safe to use as a substitutuon (i.e., in an alias). Once the run has completed, you will see output containing the number of events sent via the OpenLineage API: Completed successfully Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Emitted 4 openlineage events  "},{"title":"Where can I learn more?​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#where-can-i-learn-more","content":"Watch a short demonstration of the integration in action "},{"title":"Feedback​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#feedback","content":"What did you think of this guide? You can reach out to us on slack and leave us feedback! "},{"title":"Exposing lineage in Airflow operators","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/operator","content":"","keywords":""},{"title":"Scope​","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#scope","content":"Lineage extraction logic should be as close as possible to the operator logic. For Airflow's included operators, this logic would ideally live in the Airflow repository; for external providers, it would live in their own repository. This makes lineage extraction more stable, as it lives with the operators. Previously the OpenLineage library required one Extractor for each supported Operator which is brittle and can break when operator internals change. It's too cumbersome for people who own operators, and want to add the default implementation of OpenLineage for their operators for external users. This is still an option when you can't modify the operator itself: See add custom extractors. Each operator is responsible for describing lineage per the spec below, but the actual lineage events are still being sent by the OpenLineage library in the TaskInstanceListener. "},{"title":"Context​","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#context","content":"OpenLineage collects the following information regarding the Datasets being read and written by a task: Dataset name and namespace [required] - the format for naming is outlined in the naming specification.Dataset schema [optional] - The column names and types, if known. Complex types, like structs and arrays are supportedQuery id [optional] - for systems that expose an identifier, the ID of the query. This is a Run facet, not a Dataset facet, but it is often exposed by the Data Source’s proprietary API. For example, operators for Bigquery, Redshift, and Snowflake should all allow this.Input/output statistics [optional] - The number of records and/or bytes consumed or written. Example in the BigQuery extractor: Creating the relevant facet.BigQuery API. plan info Data quality metrics [optional] - Metrics associated with quality checks performed on the dataset. For example implemented by the Great Expectations integration. Operators that intend to share information about the datasets being read and written should also expose either some of the above-mentioned information or some minimal information necessary to retrieve that information. The absolute minimum information the operators need to share is The type of datasource being accessed (e.g., BigQuery, Snowflake, PostgreSQL)The host or authority - this is often where the data is being hosted, such as the postgres server URL, the Hive metastore URL, the GCS bucket, the Snowflake account identifier...The fully qualified data path - this may be a table name, such as public.MyDataset.MyTable or a path in a bucket, e.g., path/to/my/data as defined in the OpenLineage spec for consistency across operators. This information needs to be shared for each dataset being read and written in a task. The naming spec in the OpenLineage repository uses the above information to construct a Dataset namespace and name; together they uniquely identify the dataset. For metadata about the execution of the task, a queryId or executionId should be exposed for data sources that support them. With that identifier, we can query the data source about the execution and gather statistics, such as &quot;d&quot; number of records read/written. An operator can also includes data quality assertions. The DataQuality facet specification can be found in here. "},{"title":"Implementation​","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#implementation","content":"Each Operator implements the following methods returning the structure defined below: get_openlineage_facets_on_start()get_openlineage_facets_on_complete(ti) Facets are the JSON facets defined in the OpenLineage specification TaskInstanceLineage: runFacets: dict{} jobFacets: dict{} inputs: [InputDataset] outputs: [OutputDataset] InputDataset: namespace: string name: string facets: dict{} inputFacets: dict{} OutputDataset: namespace: string name: string facets: dict{} outputFacets: dict{}  (all facets are optional) When the task starts/completes, the OpenLineage TaskInstanceListener uses the selected method if available to construct lineage events. The order of selection of the method is as follows: if there is no extractor defined (based on getoperator_classnames) it will fall back to DefaultExtractor. DefaultExtractor uses get_openlineage_facets* methods. If the get_openlineage_facets_on_complete(ti) is not available it falls back to get_openlineage_facets_on_start(). Example: { &quot;runFacets&quot;: { &quot;errorMessage&quot;: { &quot;message&quot;: &quot;could not connect to foo&quot;, &quot;language&quot;: &quot;python&quot; } }, &quot;jobFacets&quot;: { &quot;sql&quot;: { &quot;query&quot;: &quot;CREATE TABLE FOO AS SELECT * FROM BAR&quot; } }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://example&quot;, &quot;name&quot;: &quot;workshop.public.wealth&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [{ &quot;name&quot;: &quot;foo&quot; &quot;type&quot;: &quot;char&quot; &quot;description&quot;: &quot;my first field&quot; }, ] } }, &quot;inputFacets&quot;: { &quot;dataQualityMetrics&quot;: { &quot;rowCount&quot; : 1345 } } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://example&quot;, &quot;name&quot;: &quot;workshop.public.death&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [{ &quot;name&quot;: &quot;foo&quot; &quot;type&quot;: &quot;char&quot; &quot;description&quot;: &quot;my first field&quot; }, ] } }, &quot;outputFacets&quot;: { &quot;outputStatistics&quot;: { &quot;rowCount&quot;: 10, &quot;size&quot;: 1000 } } }, { &quot;namespace&quot;: &quot;postgres://example&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [{ &quot;name&quot;: &quot;foo&quot; &quot;type&quot;: &quot;char&quot; &quot;description&quot;: &quot;my first field&quot; }, ] } }, &quot;outputFacets&quot;: { &quot;outputStatistics&quot;: { &quot;rowCount&quot;: 10, &quot;size&quot;: 1000 } } }], }  "},{"title":"Relevant facets​","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#relevant-facets","content":"Here are some relevant examples of facets that can be added. Please consult the spec for the full list. Custom facets can also be added, using a common facet name prefix. Dataset facets​ Schema schema: { fields: [{ Name: ”” Type: ”” Description: ”” }, …] }  Output facets​ OutputStatistics outputStatistics: { rowCount: 10 Size: 1000 }  Run facets​ ErrorMessage errorMessage: { Message: ”” programmingLanguage: ”” stackTrace: ”” }  All facets​ Facets "},{"title":"Apache Flink","type":0,"sectionRef":"#","url":"/docs/integrations/flink","content":"","keywords":""},{"title":"Getting lineage from Flink​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#getting-lineage-from-flink","content":"OpenLineage utilizes Flink's JobListener interface. This interface is used by Flink to notify user of job submission, successful finish of job, or job failure. Implementations of this interface are executed on JobClient. When OpenLineage listener receives information that job was submitted, it extracts Transformations from job'sExecutionEnvironment. The Transformations represent logical operations in the dataflow graph; they are composed of both Flink's build-in operators, but also user-provided Sources, Sinks and functions. To get the lineage, OpenLineage integration processes dataflow graph. Currently, OpenLineage is interested only in information contained in Sources and Sinks, as they are the places where Flink interacts with external systems. After job submission, OpenLineage integration starts actively listening to checkpoints - this gives insight into whether the job runs properly. "},{"title":"Limitations​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#limitations","content":"Currently OpenLineage's Flink integration is limited to getting information from jobs running in Application Mode. OpenLineage integration extracts lineage only from following Sources and Sinks: Sources\tSinks KafkaSource\tKafkaSink FlinkKafkaConsumer\tFlinkKafkaProducer IcebergFlinkSource\t We expect this list to grow as we add support for more connectors. "},{"title":"Usage​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#usage","content":"In your job, you need to set up OpenLineageFlinkJobListener. For example:  JobListener listener = new OpenLineageFlinkJobListener(streamExecutionEnvironment); streamExecutionEnvironment.registerJobListener(listener);  Also, OpenLineage needs certain parameters to be set in flink-conf.yaml: Configuration Key\tDescription\tExpected Value\tDefault execution.attached\tThis setting needs to be true if OpenLineage is to detect job start and failure\ttrue\tfalse OpenLineage jar needs to be present on JobManager. When the JobListener is configured, you need to point the OpenLineage integration where the events should end up. If you're using Marquez, simplest way to do that is to set up OPENLINEAGE_URL environment variable to Marquez URL. More advanced settings are in the client documentation.. "},{"title":"spark","type":0,"sectionRef":"#","url":"/docs/integrations/spark/","content":"","keywords":""},{"title":"Great Expectations","type":0,"sectionRef":"#","url":"/docs/integrations/great-expectations","content":"","keywords":""},{"title":"How does Great Expectations work with OpenLineage?​","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#how-does-great-expectations-work-with-openlineage","content":"Great Expecations integrates with OpenLineage through the action list in a checkpoint. An OpenLineage action can be specified, which is triggered when all expectations are run. Data from the checkpoint is sent to OpenLineage, which can then be viewed in Marquez or Datakin. "},{"title":"Preparing a Great Expectations project for OpenLineage​","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#preparing-a-great-expectations-project-for-openlineage","content":"First, we specify where we want Great Expectations to send OpenLineage events by setting the OPENLINEAGE_URL environment variable. For example, to send OpenLineage events to a local instance of Marquez, use: OPENLINEAGE_URL=http://localhost:5000  If data is being sent to an endpoint with an API key, then that key must be supplied as well: OPENLINEAGE_API_KEY=123456789  We can optionally specify a namespace where the lineage events will be stored. For example, to use the namespace &quot;dev&quot;: OPENLINEAGE_NAMESPACE=dev  With these environment variables set, we can add the OpenLineage action to the action list of the Great Expecations checkpoint. Note: this must be done for each checkpoint. Note: when using the GreatExpectationsOperator&gt;=0.2.0 in Airflow, there is a boolean parameter, defaulting to True, that will automatically create this action list item when it detects the OpenLineage environment specified in the previous step. In a python checkpoint, this looks like: action_list = [ { &quot;name&quot;: &quot;store_validation_result&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;StoreValidationResultAction&quot;}, }, { &quot;name&quot;: &quot;store_evaluation_params&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;StoreEvaluationParametersAction&quot;}, }, { &quot;name&quot;: &quot;update_data_docs&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;UpdateDataDocsAction&quot;, &quot;site_names&quot;: []}, }, { &quot;name&quot;: &quot;open_lineage&quot;, &quot;action&quot;: { &quot;class_name&quot;: &quot;OpenLineageValidationAction&quot;, &quot;module_name&quot;: &quot;openlineage.common.provider.great_expectations&quot;, &quot;openlineage_host&quot;: os.getenv(&quot;OPENLINEAGE_URL&quot;), &quot;openlineage_apiKey&quot;: os.getenv(&quot;OPENLINEAGE_API_KEY&quot;), &quot;openlineage_namespace&quot;: oss.getenv(&quot;OPENLINEAGE_NAMESPACE&quot;), &quot;job_name&quot;: &quot;openlineage_job&quot;, }, } ]  And in yaml: name: openlineage action: class_name: OpenLineageValidationAction module_name: openlineage.common.provider.great_expectations openlineage_host: &lt;HOST&gt; openlineage_apiKey: &lt;API_KEY&gt; openlineage_namespace: &lt;NAMESPACE_NAME&gt; # Replace with your job namespace; we recommend a meaningful namespace like `dev` or `prod`, etc. job_name: validate_my_dataset  Then run your Great Expecations checkpoint with the CLI or your integration of choice. "},{"title":"Feedback​","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#feedback","content":"What did you think of this guide? You can reach out to us on slack and leave us feedback! "},{"title":"Collecting Lineage in Spark​","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#collecting-lineage-in-spark","content":"Collecting lineage requires hooking into Spark's ListenerBus in the driver application and collecting and analyzing execution events as they happen. Both raw RDD and Dataframe jobs post events to the listener bus during execution. These events expose the structure of the job, including the optimized query plan, allowing the Spark integration to analyze the job for datasets consumed and produced, including attributes about the storage, such as location in GCS or S3, table names in a relational database or warehouse, such as Redshift or Bigquery, and schemas. In addition to dataset and job lineage, Spark SQL jobs also report logical plans, which can be compared across job runs to track important changes in query plans, which may affect the correctness or speed of a job. A single Spark application may execute multiple jobs. The Spark OpenLineage integration maps one Spark job to a single OpenLineage Job. The application will be assigned a Run id at startup and each job that executes will report the application's Run id as its parent job run. Thus, an application that reads one or more source datasets, writes an intermediate dataset, then transforms that intermediate dataset and writes a final output dataset will report three jobs- the parent application job, the initial job that reads the sources and creates the intermediate dataset, and the final job that consumes the intermediate dataset and produces the final output. As an image: "},{"title":"How to Use the Integration​","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#how-to-use-the-integration","content":"Adding OpenLineage metadata collection to existing Spark jobs was designed to be straightforward and unobtrusive to the application. "},{"title":"SparkListener​","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#sparklistener","content":"The SparkListener approach is very simple and covers most cases. The listener simply analyzes events, as they are posted by the SparkContext, and extracts job and dataset metadata that are exposed by the RDD and Dataframe dependency graphs. Most data sources, such as filesystem sources (including S3 and GCS), JDBC backends, and warehouses such as Redshift and Bigquery can be analyzed and reported in this way. Installation requires adding a following package: &lt;dependency&gt; &lt;groupId&gt;io.openlineage&lt;/groupId&gt; &lt;artifactId&gt;openlineage-spark&lt;/artifactId&gt; &lt;version&gt;{spark-openlineage-version}&lt;/version&gt; &lt;/dependency&gt;  or gradle: implementation 'io.openlineage:openlineage-spark:{spark-openlineage-version}'  spark-submit​ The listener can be enabled by adding the following configuration to a spark-submit command: spark-submit --conf &quot;spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener&quot; \\ --packages &quot;io.openlineage:openlineage-spark:&lt;spark-openlineage-version&gt;&quot; \\ --conf &quot;spark.openlineage.url=http://{openlineage.client.host}/api/v1/namespaces/spark_integration/&quot; \\ --class com.mycompany.MySparkApp my_application.jar  The SparkListener reads its configuration from SparkConf parameters. These can be specified on the command line (e.g., --conf &quot;spark.openlineage.url=http://{openlineage.client.host}/api/v1/namespaces/my_namespace/job/the_job&quot;) or from the conf/spark-defaults.conf file. The following parameters can be specified Parameter\tDefinition\tExamplespark.openlineage.host\tThe hostname of the OpenLineage API server where events should be reported\thttp://localhost:5000 spark.openlineage.version\tThe API version of the OpenLineage API server\t1 spark.openlineage.namespace\tThe default namespace to be applied for any jobs submitted\tMyNamespace spark.openlineage.parentJobName\tThe job name to be used for the parent job facet\tParentJobName spark.openlineage.parentRunId\tThe RunId of the parent job that initiated this Spark job\txxxx-xxxx-xxxx-xxxx spark.openlineage.apiKey\tAn API key to be used when sending events to the OpenLineage server\tabcdefghijk spark.openlineage.url.param.xyz\tA url parameter (replace xyz) and value to be included in requests to the OpenLineage API server\tabcdefghijk spark.openlineage.consoleTransport\tEvents will be emitted to a console, no additional backend is required\ttrue "},{"title":"Scheduling from Airflow​","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#scheduling-from-airflow","content":"The same parameters passed to spark-submit can be supplied from Airflow and other schedulers. If using the marquez-airflow integration, each task in the DAG has its own Run id which can be connected to the Spark job run via the openlineage.parentRunId parameter. For example, here is an example of a DataProcPySparkOperator that submits a Pyspark application on Dataproc: t1 = DataProcPySparkOperator( task_id=job_name, gcp_conn_id='google_cloud_default', project_id='project_id', cluster_name='cluster-name', region='us-west1', main='gs://bucket/your-prog.py', job_name=job_name, dataproc_pyspark_properties={ &quot;spark.extraListeners&quot;: &quot;marquez.spark.agent.SparkListener&quot;, &quot;spark.jars.packages&quot;: &quot;io.github.marquezproject:marquez-spark:0.15.+&quot;, &quot;openlineage.url&quot;: f&quot;{marquez_url}/api/v1/namespaces/{marquez_namespace}/jobs/dump_orders_to_gcs/runs/{{{{task_run_id(run_id, task)}}}}?api_key={api_key}&quot; }, dag=dag)  "},{"title":"Quickstart with Jupyter","type":0,"sectionRef":"#","url":"/docs/integrations/spark/quickstart_local","content":"Quickstart with Jupyter Trying out the Spark integration is super easy if you already have Docker Desktop and git installed. Note: If you're on macOS Monterey (macOS 12) you'll have to release port 5000 before beginning by disabling the AirPlay Receiver. Check out the OpenLineage project into your workspace with: git clone https://github.com/OpenLineage/OpenLineage From the spark integration directory ($OPENLINEAGE_ROOT/integration/spark) execute docker-compose up This will start Marquez as an Openlineage client and Jupyter Spark notebook on localhost:8888. On startup, the notebook container logs will show a list of URLs including an access token, such as notebook_1 | To access the notebook, open this file in a browser: notebook_1 | file:///home/jovyan/.local/share/jupyter/runtime/nbserver-9-open.html notebook_1 | Or copy and paste one of these URLs: notebook_1 | http://abc12345d6e:8888/?token=XXXXXX notebook_1 | or http://127.0.0.1:8888/?token=XXXXXX Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from mine) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go. Once your notebook environment is ready, click on the notebooks directory, then click on the New button to create a new Python 3 notebook. In the first cell in the window paste the following text: from pyspark.sql import SparkSession spark = (SparkSession.builder.master('local') .appName('sample_spark') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.12.0') .config('spark.openlineage.consoleTransport', 'true') .getOrCreate()) Once the Spark context is started, we adjust logging level to INFO with: spark.sparkContext.setLogLevel(&quot;INFO&quot;) and create some Spark table with: spark.createDataFrame([ {'a': 1, 'b': 2}, {'a': 3, 'b': 4} ]).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;temp&quot;) The command shold output OpenLineage event in a form of log: 22/08/01 06:15:49 INFO ConsoleTransport: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-08-01T06:15:49.671Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;204d9c56-6648-4d46-b6bd-f4623255d324&quot;,&quot;facets&quot;:{&quot;spark_unknown&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;inputs&quot;:[{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;id&quot;:1,&quot;streaming&quot;:false,&quot;traceEnabled&quot;:false,&quot;canonicalizedPlan&quot;:false},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}}]}]},&quot;spark.logicalPlan&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;plan&quot;:[{&quot;class&quot;:&quot;org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand&quot;,&quot;num-children&quot;:1,&quot;table&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogTable&quot;,&quot;identifier&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.TableIdentifier&quot;,&quot;table&quot;:&quot;temp&quot;},&quot;tableType&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogTableType&quot;,&quot;name&quot;:&quot;MANAGED&quot;},&quot;storage&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat&quot;,&quot;compressed&quot;:false,&quot;properties&quot;:null},&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[]},&quot;provider&quot;:&quot;parquet&quot;,&quot;partitionColumnNames&quot;:[],&quot;owner&quot;:&quot;&quot;,&quot;createTime&quot;:1659334549656,&quot;lastAccessTime&quot;:-1,&quot;createVersion&quot;:&quot;&quot;,&quot;properties&quot;:null,&quot;unsupportedFeatures&quot;:[],&quot;tracksPartitionsInCatalog&quot;:false,&quot;schemaPreservesCase&quot;:true,&quot;ignoredProperties&quot;:null},&quot;mode&quot;:null,&quot;query&quot;:0,&quot;outputColumnNames&quot;:&quot;[a, b]&quot;},{&quot;class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;num-children&quot;:0,&quot;output&quot;:[[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;a&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:6,&quot;jvmId&quot;:&quot;6a1324ac-917e-4e22-a0b9-84a5f80694ad&quot;},&quot;qualifier&quot;:[]}],[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;b&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:7,&quot;jvmId&quot;:&quot;6a1324ac-917e-4e22-a0b9-84a5f80694ad&quot;},&quot;qualifier&quot;:[]}]],&quot;rdd&quot;:null,&quot;outputPartitioning&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning&quot;,&quot;numPartitions&quot;:0},&quot;outputOrdering&quot;:[],&quot;isStreaming&quot;:false,&quot;session&quot;:null}]},&quot;spark_version&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;spark-version&quot;:&quot;3.1.2&quot;,&quot;openlineage-spark-version&quot;:&quot;0.12.0-SNAPSHOT&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;sample_spark.execute_create_data_source_table_as_select_command&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;file&quot;,&quot;name&quot;:&quot;/home/jovyan/notebooks/spark-warehouse/temp&quot;,&quot;facets&quot;:{&quot;dataSource&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet&quot;,&quot;name&quot;:&quot;file&quot;,&quot;uri&quot;:&quot;file&quot;},&quot;schema&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}]},&quot;lifecycleStateChange&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet&quot;,&quot;lifecycleStateChange&quot;:&quot;CREATE&quot;}},&quot;outputFacets&quot;:{}}],&quot;producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;} Generated JSON contains output dataset name and location {&quot;namespace&quot;:&quot;file&quot;,&quot;name&quot;:&quot;/home/jovyan/notebooks/spark-warehouse/temp&quot;, schema fields [{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}], etc. More comprehensive demo, that integrates Spark events with Marquez backend can be foundon our blog Tracing Data Lineage with OpenLineage and Apache Spark","keywords":""},{"title":"Quickstart with Databricks","type":0,"sectionRef":"#","url":"/docs/integrations/spark/quickstart_databricks","content":"","keywords":""},{"title":"Enable OpenLineage​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#enable-openlineage","content":"Follow the steps below to enable OpenLineage on Databricks. Build the jar via Gradle or download the latest release.Configure the Databricks CLI with your desired workspace: Azure Databricks CLIGCP Databricks CLIAWS Databricks CLI Run upload-to-databricks.sh or upload-to-databricks.ps1. This will: create a folder in DBFS to store the OpenLineage jar.copy the jar to the DBFS foldercopy the init script to the DBFS folder Create an interactive or job cluster with the relevant Spark configs: spark.openlineage.consoleTransport true spark.extraListeners io.openlineage.spark.agent.OpenLineageSparkListener spark.openlineage.version v1 Set the cluster init script to be: dbfs:/databricks/openlineage/open-lineage-init-script.sh. info Please note that the init script approach is currently obligatory to install OpenLineage on Databricks. The Openlineage integration relies on providing a custom extra listener class io.openlineage.spark.agent.OpenLineageSparkListener that has to be available on the classpath at the driver startup. Providing it with spark.jars.packages does not work on the Databricks platform as of August 2022. "},{"title":"Verify Initialization​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#verify-initialization","content":"A successful initialization will emit logs in the Log4j output that look similar to the following: YY/MM/DD HH:mm:ss INFO SparkContext: Registered listener io.openlineage.spark.agent.OpenLineageSparkListener YY/MM/DD HH:mm:ss INFO OpenLineageContext: Init OpenLineageContext: Args: ArgumentParser(host=https://YOURHOST, version=v1, namespace=YOURNAMESPACE, jobName=default, parentRunId=null, apiKey=Optional.empty) URI: https://YOURHOST/api/v1/lineage YY/MM/DD HH:mm:ss INFO AsyncEventQueue: Process of event SparkListenerApplicationStart(Databricks Shell,Some(app-XXX-0000),YYYY,root,None,None,None) by listener OpenLineageSparkListener took Xs.  "},{"title":"Create a Dataset​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#create-a-dataset","content":"Open a notebook and create an example dataset with: spark.createDataFrame([ {'a': 1, 'b': 2}, {'a': 3, 'b': 4} ]).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;default.temp&quot;)  "},{"title":"Observe OpenLineage Events​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#observe-openlineage-events","content":"To troubleshoot or observe OpenLineage information in Databricks, see the Log4j output in the Cluster definition's Driver Logs. The Log4j output should contain entries starting with a message INFO ConsoleTransport that contain generated OpenLineage events: {&quot;eventType&quot;:&quot;COMPLETE&quot;,&quot;eventTime&quot;:&quot;2022-08-01T08:36:21.633Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;64537bbd-00ac-498d-ad49-1c77e9c2aabd&quot;,&quot;facets&quot;:{&quot;spark_unknown&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;inputs&quot;:[{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.catalyst.analysis.ResolvedTableName&quot;,&quot;id&quot;:1,&quot;traceEnabled&quot;:false,&quot;streaming&quot;:false,&quot;cacheId&quot;:{&quot;id&quot;:2,&quot;empty&quot;:true,&quot;defined&quot;:false},&quot;canonicalizedPlan&quot;:false,&quot;defaultTreePatternBits&quot;:{&quot;id&quot;:3}},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[]},{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;id&quot;:1,&quot;streaming&quot;:false,&quot;traceEnabled&quot;:false,&quot;cacheId&quot;:{&quot;id&quot;:2,&quot;empty&quot;:true,&quot;defined&quot;:false},&quot;canonicalizedPlan&quot;:false,&quot;defaultTreePatternBits&quot;:{&quot;id&quot;:3}},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}}]}]},&quot;spark.logicalPlan&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;plan&quot;:[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect&quot;,&quot;num-children&quot;:2,&quot;name&quot;:0,&quot;partitioning&quot;:[],&quot;query&quot;:1,&quot;tableSpec&quot;:null,&quot;writeOptions&quot;:null,&quot;orCreate&quot;:true},{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.analysis.ResolvedTableName&quot;,&quot;num-children&quot;:0,&quot;catalog&quot;:null,&quot;ident&quot;:null},{&quot;class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;num-children&quot;:0,&quot;output&quot;:[[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;a&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:18,&quot;jvmId&quot;:&quot;481bebf6-f861-400e-bb00-ea105ed8afef&quot;},&quot;qualifier&quot;:[]}],[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;b&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:19,&quot;jvmId&quot;:&quot;481bebf6-f861-400e-bb00-ea105ed8afef&quot;},&quot;qualifier&quot;:[]}]],&quot;rdd&quot;:null,&quot;outputPartitioning&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning&quot;,&quot;numPartitions&quot;:0},&quot;outputOrdering&quot;:[],&quot;isStreaming&quot;:false,&quot;session&quot;:null}]},&quot;spark_version&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;spark-version&quot;:&quot;3.2.1&quot;,&quot;openlineage-spark-version&quot;:&quot;0.12.0-SNAPSHOT&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;databricks_shell.atomic_replace_table_as_select&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;dbfs&quot;,&quot;name&quot;:&quot;/user/hive/warehouse/temp&quot;,&quot;facets&quot;:{&quot;dataSource&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet&quot;,&quot;name&quot;:&quot;dbfs&quot;,&quot;uri&quot;:&quot;dbfs&quot;},&quot;schema&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}]},&quot;storage&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/StorageDatasetFacet.json#/$defs/StorageDatasetFacet&quot;,&quot;storageLayer&quot;:&quot;delta&quot;,&quot;fileFormat&quot;:&quot;parquet&quot;},&quot;lifecycleStateChange&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet&quot;,&quot;lifecycleStateChange&quot;:&quot;OVERWRITE&quot;}},&quot;outputFacets&quot;:{}}],&quot;producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;}  The generated JSON contains the output dataset name and location {&quot;namespace&quot;:&quot;dbfs&quot;,&quot;name&quot;:&quot;/user/hive/warehouse/temp&quot;&quot; metadata, schema fields [{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}], and more. "},{"title":"Column Level Lineage","type":0,"sectionRef":"#","url":"/docs/integrations/spark/spark_column_lineage","content":"","keywords":""},{"title":"Standard specification​","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#standard-specification","content":"Collected information is sent in OpenLineage event within columnLineage dataset facet described here. "},{"title":"Code architecture and its mechanics​","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#code-architecture-and-its-mechanics","content":"Column level lineage has been implemented separately from the rest of builders and visitors extracting lineage information from Spark logical plans. As a result the codebase is stored in io.openlineage.spark3.agent.lifecycle.plan.columnLineage package within classes responsible only for this feature. Class ColumnLevelLineageUtils.java is an entry point to run the mechanism and is used within OpenLineageRunEventBuilder. Classes ColumnLevelLineageUtilsNonV2CatalogTest and ColumnLevelLineageUtilsV2CatalogTest contain real-life test cases which run Spark jobs and get an access to the last query plan executed. They evaluate column level lineage based on the plan and expected output schema. Then, they verify if this meets the requirements. This allows testing column level lineage behavior in real scenarios. The more tests and scenarios put here, the better. Class ColumnLevelLineageBuilder is used when traversing logical plans to store all the information required to produce column lineage. It allows storing input/output columns. It also stores dependencies between the expressions contained in query plan. Once inputs, outputs and dependencies are filled, build method is used to produce output facet (ColumnLineageDatasetFacetFields). The mechanism gets output schema and logical plan as input. Output schemas are tightly coupled with root nodes of execution plans, however we do already have this information extracted within the other visitors and dataset input builders.OutputFieldsCollector class is used to traverse the plan to gather the outputs. Outputs can be extracted from Aggregate or Project and each output field has its ExprId (expression id) attached from the plan. InputFieldsCollector class is used to collect the inputs which can be extracted from DataSourceV2Relation, DataSourceV2ScanRelation, HiveTableRelation or LogicalRelation. Each input field has its ExprId within the plan. Each input is identified by DatasetIdentifier, which means it contains name and namespace, of a dataset and an input field. FieldDependenciesCollector traverses the plan to identify dependencies between different ExprId. Dependencies map parent expressions to children expressions'. This is used to identify inputs used to evaluate certain output. "},{"title":"Writing custom extensions​","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#writing-custom-extensions","content":"Spark framework is known for its great ability to be extended by custom libraries capable of reading or writing to anything. In case of having a custom implementation, we prepared an ability to extend column lineage implementation to be able to retrieve information from other input or output LogicalPlan nodes. Creating such an extension requires implementing a following interface: /** Interface for implementing custom collectors of column level lineage. */ interface CustomColumnLineageVisitor { /** * Collect inputs for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Input information * should be put into builder. * * @param node * @param builder */ void collectInputs(LogicalPlan node, ColumnLevelLineageBuilder builder); /** * Collect outputs for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Output information * should be put into builder. * * @param node * @param builder */ void collectOutputs(LogicalPlan node, ColumnLevelLineageBuilder builder); /** * Collect expressions for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Expression * dependency information should be put into builder. * * @param node * @param builder */ void collectExpressionDependencies(LogicalPlan node, ColumnLevelLineageBuilder builder); }  and making it available for Service Loader (implementation class name has to be put in a resource file META-INF/services/io.openlineage.spark3.agent.lifecycle.plan.column.CustomColumnLineageVisitor). "},{"title":"Future work​","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#future-work","content":"Current version of the mechanism allows finding input fields that were used to produce the output field but does not determine how were they used. This simplification allowed us to built column lineage feature at the minimum amount of code written. Given an incredible amount of Spark's functions, operators and expressions, our implementation needs just to know it was UnaryOperator, BinaryOperator, etc. without a requirement to understand the operation it performs. This approach still has some room for extensions like: Being able to find out if an output field is a simple copy of input one or some modification has been applied.Assume there exist a mechanism within an organisation to blur personal data. Be able to extract information if the output still contains personal data or it got blurred. "},{"title":"0.10.0 - 2022-6-24","type":0,"sectionRef":"#","url":"/docs/releases/0_10_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.10.0 - 2022-6-24","url":"/docs/releases/0_10_0#added","content":"Add static code anlalysis tool mypy to run in CI for against all python modules (#802) @howardyooExtend SaveIntoDataSourceCommandVisitor to extract schema from LocalRelaiton and LogicalRdd in spark integration (#794) @pawel-big-lebowskiAdd InMemoryRelationInputDatasetBuilder for InMemory datasets to Spark integration (#818) @pawel-big-lebowskiAdd copyright to source files #755 @merobi-hubAdd SnowflakeOperatorAsync extractor support to Airflow integration #869 @merobi-hubAdd PMD analysis to proxy project (#889) @howardyoo "},{"title":"Changed​","type":1,"pageTitle":"0.10.0 - 2022-6-24","url":"/docs/releases/0_10_0#changed","content":"Skip FunctionRegistry.class serialization in Spark integration (#828) @mobuchowskiInstall new rust-based SQL parser by default in Airflow integration (#835) @mobuchowskiImprove overall pytest and integration tests for Airflow integration (#851,#858) @denimalpacaReduce OL event payload size by excluding local data and including output node in start events (#881) @collado-mikeSplit spark integration into submodules (#834, #890) @tnazarew @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.10.0 - 2022-6-24","url":"/docs/releases/0_10_0#fixed","content":"Conditionally import sqlalchemy lib for Great Expectations integration (#826) @pawel-big-lebowskiAdd check for missing class org.apache.spark.sql.catalyst.plans.logical.CreateV2Table in Spark integration (#866) @pawel-big-lebowskiFix static code analysis issues (#867,#874) @pawel-big-lebowski "},{"title":"0.1.0 - 2021-8-13","type":0,"sectionRef":"#","url":"/docs/releases/0_1_0","content":"0.1.0 - 2021-8-13 OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution. The initial public release includes: An inital specification. The the inital version 1-0-0 of the OpenLineage specification defines the core model and facets.Integrations that collect lineage metadata as OpenLineage events: Apache Airflow with support for BigQuery, Great Expectations, Postgres, Redshift, SnowflakeApache Sparkdbt Clients that send OpenLineage events to an HTTP backend. Both java and python are initially supported.","keywords":""},{"title":"0.12.0 - 2022-8-1","type":0,"sectionRef":"#","url":"/docs/releases/0_12_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.12.0 - 2022-8-1","url":"/docs/releases/0_12_0#added","content":"Add Spark 3.3.0 support #950 @pawel-big-lebowskiAdd Apache Flink integration #951 @mobuchowskiAdd ability to extend column level lineage mechanism #922 @pawel-big-lebowskiAdd ErrorMessageRunFacet #897 @mobuchowskiAdd SQLCheckExtractors #717 @denimalpacaAdd RedshiftSQLExtractor &amp; RedshiftDataExtractor #930 @JDarDagranAdd dataset builder for AlterTableCommand #927 @tnazarew "},{"title":"Changed​","type":1,"pageTitle":"0.12.0 - 2022-8-1","url":"/docs/releases/0_12_0#changed","content":"Limit Delta events #905 @pawel-big-lebowskiAirflow integration: allow lineage metadata to flow through inlets and outlets #914 @fenil25 "},{"title":"Fixed​","type":1,"pageTitle":"0.12.0 - 2022-8-1","url":"/docs/releases/0_12_0#fixed","content":"Limit size of serialized plan #917 @pawel-big-lebowskiFix noclassdef error #942 @pawel-big-lebowski "},{"title":"0.11.0 - 2022-7-7","type":0,"sectionRef":"#","url":"/docs/releases/0_11_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.11.0 - 2022-7-7","url":"/docs/releases/0_11_0#added","content":"HTTP option to override timeout and properly close connections in openlineage-java lib. #909 @mobuchowskiDynamic mapped tasks support to Airflow integration #906 @JDarDagranSqlExtractor to Airflow integration #907 @JDarDagranPMD to Java and Spark builds in CI #898 @merobi-hub "},{"title":"Changed​","type":1,"pageTitle":"0.11.0 - 2022-7-7","url":"/docs/releases/0_11_0#changed","content":"When testing extractors in the Airflow integration, set the extractor length assertion dynamic #882 @denimalpacaRender templates as start of integration tests for TaskListener in the Airflow integration #870 @mobuchowski  "},{"title":"Fixed​","type":1,"pageTitle":"0.11.0 - 2022-7-7","url":"/docs/releases/0_11_0#fixed","content":"Dependencies bundled with openlineage-java lib. #855 @collado-mikePMD reported issues #891 @pawel-big-lebowskiSpark casting error and session catalog support for iceberg in Spark integration #856 @wslulciuc "},{"title":"0.13.1 - 2022-8-25","type":0,"sectionRef":"#","url":"/docs/releases/0_13_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.13.1 - 2022-8-25","url":"/docs/releases/0_13_1#fixed","content":"Rename all parentRun occurrences to parent in Airflow integration 1037 @fm100 Changes the parentRun property name to parent in the Airflow integration to match the spec.Do not change task instance during on_running event 1028 @JDarDagran Fixes an issue in the Airflow integration with the on_running hook, which was changing the TaskInstance object along with the task attribute. "},{"title":"0.13.0 - 2022-8-22","type":0,"sectionRef":"#","url":"/docs/releases/0_13_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.13.0 - 2022-8-22","url":"/docs/releases/0_13_0#added","content":"Add BigQuery check support #960 @denimalpaca Adds logic and support for proper dynamic class inheritance for BigQuery-style operators. (BigQuery's extractor needed additional logic to support the forthcoming BigQueryColumnCheckOperator and BigQueryTableCheckOperator.)Add RUNNING EventType in spec and Python client #972 @mzareba382 Introduces a RUNNING event state in the OpenLineage spec to indicate a running task and adds a RUNNING event type in the Python API.Use databases &amp; schemas in SQL Extractors #974 @JDarDagran Allows the Airflow integration to differentiate between databases and schemas. (There was no notion of databases and schemas when querying and parsing results from information_schema tables.)Implement Event forwarding feature via HTTP protocol #995 @howardyoo Adds HttpLineageStream to forward a given OpenLineage event to any HTTP endpoint.Introduce SymlinksDatasetFacet to spec #936 @pawel-big-lebowski Creates a new facet, the SymlinksDatasetFacet, to support the storing of alternative dataset names.Add Azure Cosmos Handler to Spark integration #983 @hmoazam Defines a new interface, the RelationHandler, to support Spark data sources that do not have TableCatalog, Identifier, or TableProperties set, as is the case with the Azure Cosmos DB Spark connector.Support OL Datasets in manual lineage inputs/outputs #1015 @conorbev Allows Airflow users to create OpenLineage Dataset classes directly in DAGs with no conversion necessary. (Manual lineage definition required users to create an airflow.lineage.entities.Table, which was then converted to an OpenLineage Dataset.) Create ownership facets #996 @julienledem Adds an ownership facet to both Dataset and Job in the OpenLineage spec to capture ownership of jobs and datasets. "},{"title":"Changed​","type":1,"pageTitle":"0.13.0 - 2022-8-22","url":"/docs/releases/0_13_0#changed","content":"Use RUNNING EventType in Flink integration for currently running jobs #985 @mzareba382 Makes use of the new RUNNING event type in the Flink integration, changing events sent by Flink jobs from OTHER to this new type.Convert task objects to JSON-encodable objects when creating custom Airflow version facets #1018 @fm100 Implements a to_json_encodable function in the Airflow integration to make task objects JSON-encodable. "},{"title":"Fixed​","type":1,"pageTitle":"0.13.0 - 2022-8-22","url":"/docs/releases/0_13_0#fixed","content":"Add support for custom SQL queries in v3 Great Expectations API #1025 @collado-mike Fixes support for custom SQL statements in the Great Expectations provider. (The Great Expectations custom SQL datasource was not applied to the support for the V3 checkpoints API.) "},{"title":"0.14.1 - 2022-9-7","type":0,"sectionRef":"#","url":"/docs/releases/0_14_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.14.1 - 2022-9-7","url":"/docs/releases/0_14_1#fixed","content":"Fix Spark integration issues including error when no openlineage.timeout #1069 @pawel-big-lebowski OpenlineageSparkListener was failing when no openlineage.timeout was provided. "},{"title":"0.14.0 - 2022-9-6","type":0,"sectionRef":"#","url":"/docs/releases/0_14_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.14.0 - 2022-9-6","url":"/docs/releases/0_14_0#added","content":"Support ABFSS and Hadoop Logical Relation in Column-level lineage #1008 @wjohnson Introduces an extractDatasetIdentifier that uses similar logic to InsertIntoHadoopFsRelationVisitor to pull out the path on the HDFS compliant file system; tested on ABFSS and DBFS (Databricks FileSystem) to prove that lineage could be extracted using non-SQL commands.Add Kusto relation visitor #939 @hmoazam Implements a KustoRelationVisitor to support lineage for Azure Kusto's Spark connector.Add ColumnLevelLineage facet doc #1020 @julienledem Adds documentation for the Column-level lineage facet.Include symlinks dataset facet #935 @pawel-big-lebowski Includes the recently introduced SymlinkDatasetFacet in generated OpenLineage events.Add support for dbt 1.3 beta's metadata changes #1051 @mobuchowski Makes projects that are composed of only SQL models work on 1.3 beta (dbt 1.3 renamed the compiled_sql field to compiled_code to support Python models). Does not provide support for dbt's Python models.Support Flink 1.15 #1009 @mzareba382 Adds support for Flink 1.15.Add Redshift dialect to the SQL integration #1066 @mobuchowski Adds support for Redshift's SQL dialect in OpenLineage's SQL parser, including quirks such as the use of square brackets in JSON paths. (Note, this does not add support for all of Redshift's custom syntax.) "},{"title":"Changed​","type":1,"pageTitle":"0.14.0 - 2022-9-6","url":"/docs/releases/0_14_0#changed","content":"Make the timeout configurable in the Spark integration #1050 @tnazarew Makes timeout configurable by the user. (In some cases, the time needed to send events was longer than 5 seconds, which exceeded the timeout value.) "},{"title":"Fixed​","type":1,"pageTitle":"0.14.0 - 2022-9-6","url":"/docs/releases/0_14_0#fixed","content":"Add a dialect parameter to Great Expectations SQL parser calls #1049 @collado-mike Specifies the dialect name from the SQL engine.Fix Delta 2.1.0 with Spark 3.3.0 #1065 @pawel-big-lebowski Allows delta support for Spark 3.3 and fixes potential issues. (The Openlineage integration for Spark 3.3 was turned on without delta support, as delta did not support Spark 3.3 at that time.) "},{"title":"0.15.1 - 2022-10-5","type":0,"sectionRef":"#","url":"/docs/releases/0_15_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.15.1 - 2022-10-5","url":"/docs/releases/0_15_1#added","content":"Airflow: improve development experience #1101 @JDarDagran Adds an interactive development environment to the Airflow integration and improves integration testing.Spark: add description for URL parameters in readme, change overwriteName to appName #1130 @tnazarew Adds more information about passing arguments with spark.openlineage.url and changes overwriteName to appName for clarity.Documentation: update issue templates for proposal &amp; add new integration template #1116 @rossturk Adds a YAML issue template for new integrations and fixes a bug in the proposal template. "},{"title":"Changed​","type":1,"pageTitle":"0.15.1 - 2022-10-5","url":"/docs/releases/0_15_1#changed","content":"Airflow: lazy load BigQuery client #1119 @mobuchowski Moves import of the BigQuery client from top level to local level to decrease DAG import time. "},{"title":"Fixed​","type":1,"pageTitle":"0.15.1 - 2022-10-5","url":"/docs/releases/0_15_1#fixed","content":"Airflow: fix UUID generation conflict for Airflow DAGs with same name #1056 @collado-mike Adds a namespace to the UUID calculation to avoid conflicts caused by DAGs having the same name in different namespaces in Airflow deployments.Spark/BigQuery: fix issue with spark-bigquery-connector &gt;=0.25.0 #1111 @pawel-big-lebowski Makes the Spark integration compatible with the latest connector.Spark: fix column lineage #1069 @pawel-big-lebowski Fixes a null pointer exception error and an error when openlineage.timeout is not provided.Spark: set log level of Init OpenLineageContext to DEBUG #1064 @varuntestaz Prevents sensitive information from being logged unless debug mode is used.Java client: update version of SnakeYAML #1090 @TheSpeedding Bumps the SnakeYAML library version to include a key bug fix. dbt: remove requirement for OPENLINEAGE_URL to be set #1107 @mobuchowski Removes erroneous check for OPENLINEAGE_URL in the dbt integration.Python client: remove potentially cyclic import #1126 @mobuchowski Hides imports to remove potentially cyclic import.CI: build macos release package on medium resource class #1131 @mobuchowski Fixes failing build due to resource class being too large. "},{"title":"0.16.1 - 2022-11-3","type":0,"sectionRef":"#","url":"/docs/releases/0_16_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#added","content":"Airflow: add dag_run information to Airflow version run facet #1133 @fm100 Adds the Airflow DAG run ID to the taskInfo facet, making this additional information available to the integration.Airflow: add LoggingMixin to extractors #1149 @JDarDagran Adds a LoggingMixin class to the custom extractor to make the output consistent with general Airflow and OpenLineage logging settings.Airflow: add default extractor #1162 @mobuchowski Adds a DefaultExtractor to support the default implementation of OpenLineage for external operators without the need for custom extractors.Airflow: add on_complete argument in DefaultExtractor #1188 @JDarDagran Adds support for running another method on extract_on_complete.SQL: reorganize the library into multiple packages #1167 @StarostaGit @mobuchowski Splits the SQL library into a Rust implementation and foreign language bindings, easing the process of adding language interfaces. Also contains CI fix. "},{"title":"Changed​","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#changed","content":"Airflow: move get_connection_uri as extractor's classmethod #1169 @JDarDagran The get_connection_uri method allowed for too many params, resulting in unnecessarily long URIs. This changes the logic to whitelisting per extractor.Airflow: change get_openlineage_facets_on_start/complete behavior #1201 @JDarDagran Splits up the method for greater legibility and easier maintenance. "},{"title":"Fixed​","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#fixed","content":"Airflow: always send SQL in SqlJobFacet as a string #1143 @mobuchowski Changes the data type of query from array to string to an fix error in the RedshiftSQLOperator. Airflow: include __extra__ case when filtering URI query params #1144 @JDarDagran Includes the conn.EXTRA_KEY in the get_connection_uri method to avoid exposing secrets in URIs via the __extra__ key. Airflow: enforce column casing in SQLCheckExtractors #1159 @denimalpaca Uses the parent extractor's _is_uppercase_names property to determine if the column should be upper cased in the SQLColumnCheckExtractor's _get_input_facets() method.Spark: prevent exception when no schema provided #1180 @pawel-big-lebowski Prevents evalution of column lineage when the schemFacet is null.Great Expectations: add V3 API compatibility #1194 @denimalpaca Fixes the Pandas datasource to make it V3 API-compatible. "},{"title":"Removed​","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#removed","content":"Airflow: remove support for Airflow 1.10 #1128 @mobuchowski Removes the code structures and tests enabling support for Airflow 1.10. "},{"title":"0.17.0 - 2022-11-16","type":0,"sectionRef":"#","url":"/docs/releases/0_17_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#added","content":"Spark: support latest Spark 3.3.1 #1183 @pawel-big-lebowski Adds support for the latest version of Spark.Spark: add Kinesis Transport and support config Kinesis in Spark integration #1200 @yogayang Adds support for sending to Kinesis from the Spark integration. Spark: Disable specified facets #1271 @pawel-big-lebowski Adds the ability to disable specified facets from generated OpenLineage events.Python: add facets implementation to Python client #1233 @pawel-big-lebowski Adds missing facets to the Python client.SQL: add Rust parser interface #1172 @StarostaGit @mobuchowski Implements a Java interface in the Rust SQL parser, including a build script, native library loading mechanism, CI support and build fixes.Proxy: add helm chart for the proxy backed #1068 @wslulciuc Adds a helm chart for deploying the proxy backend on Kubernetes.Spec: include possible facets usage in spec #1249 @pawel-big-lebowski Extends the facets definition with a list of available facets.Website: publish YML version of spec to website #1300 @rossturk Adds configuration necessary to make the OpenLineage website auto-generate openAPI docs when the spec is published there.Docs: update language on nominating new committers #1270 @rossturk Updates the governance language to reflect the new policy on nominating committers. "},{"title":"Changed​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#changed","content":"Website: publish spec into new website repo location #1295 @rossturk Creates a new deploy key, adds it to CircleCI &amp; GitHub, and makes the necessary changes to the release.sh script.Airflow: change how pip installs packages in tox environments #1302 @JDarDagran Use deprecated resolver and constraints files provided by Airflow to avoid potential issues caused by pip's new resolver. "},{"title":"Fixed​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#fixed","content":"Airflow: fix README for running integration test #1238 @sekikn Updates the README for consistency with supported Airflow versions.Airflow: add task_instance argument to get_openlineage_facets_on_complete #1269 @JDarDagran Adds the task_instance argument to DefaultExtractor.Java client: fix up all artifactory paths #1290 @harels Not all artifactory paths were changed in the build CI script in a previous PR.Python client: fix Mypy errors and adjust to PEP 484 #1264 @JDarDagran Adds a --no-namespace-packages argument to the Mypy command and adjusts code to PEP 484.Website: release all specs since last_spec_commit_id, not just HEAD~1 #1298 @rossturk The script now ships all specs that have changed since .last_spec_commit_id. "},{"title":"Removed​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#removed","content":"Deprecate HttpTransport.Builder in favor of HttpConfig #1287 @collado-mike Deprecates the Builder in favor of HttpConfig only and replaces the existing Builder implementation by delegating to the HttpConfig. "},{"title":"0.18.0 - 2022-12-8","type":0,"sectionRef":"#","url":"/docs/releases/0_18_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.18.0 - 2022-12-8","url":"/docs/releases/0_18_0#added","content":"Airflow: support SQLExecuteQueryOperator #1379 @JDarDagran Changes the SQLExtractor and adds support for the dynamic assignment of extractors based on conn_type.Airflow: introduce a new extractor for SFTPOperator #1263 @sekikn Adds an extractor for tracing file transfers between local file systems.Airflow: add Sagemaker extractors #1136 @fhoda Creates extractors for SagemakeProcessingOperator and SagemakerTransformOperator.Airflow: add S3 extractor for Airflow operators #1166 @fhoda Creates an extractor for the S3CopyObject in the Airflow integration.Spec: add spec file for ExternalQueryRunFacet #1262 @howardyoo Adds a spec file to make this facet available for the Java client. Includes a README.Docs: add a TSC doc #1303 @merobi-hub Adds a document listing the members of the Technical Steering Committee. "},{"title":"Fixed​","type":1,"pageTitle":"0.18.0 - 2022-12-8","url":"/docs/releases/0_18_0#fixed","content":"Spark: improve Databricks to send better events #1330 @pawel-big-lebowski Filters unwanted events and provides a meaningful job name.Spark-Bigquery: fix a few of the common errors #1377 @mobuchowski Fixes a few of the common issues with the Spark-Bigquery integration and adds an integration test and configures CI.Python: validate eventTime field in Python client #1355 @pawel-big-lebowskiValidates the eventTime of a RunEvent within the client library.Databricks: Handle Databricks Runtime 11.3 changes to DbFsUtils constructor #1351 @wjohnson Recaptures lost mount point information from the DatabricksEnvironmentFacetBuilder and environment-properties facet by looking at the number of parameters in the DbFsUtils constructor to determine the runtime version. "},{"title":"0.2.0 - 2021-8-23","type":0,"sectionRef":"#","url":"/docs/releases/0_2_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.2.0 - 2021-8-23","url":"/docs/releases/0_2_0#added","content":"Parse dbt command line arguments when invoking dbt-ol @mobuchowski. For example: $ dbt-ol run --project-dir path/to/dir Set UnknownFacet for spark (captures metadata about unvisited nodes from spark plan not yet supported) @OleksandrDvornik "},{"title":"Changed​","type":1,"pageTitle":"0.2.0 - 2021-8-23","url":"/docs/releases/0_2_0#changed","content":"Remove model from dbt job name @mobuchowskiDefault dbt job namespace to output dataset namespace @mobuchowskiRename openlineage.spark.* to io.openlineage.spark.* @OleksandrDvornik "},{"title":"Fixed​","type":1,"pageTitle":"0.2.0 - 2021-8-23","url":"/docs/releases/0_2_0#fixed","content":"Remove instance references to extractors from DAG and avoid copying log property for serializability @collado-mike "},{"title":"0.19.2 - 2023-1-4","type":0,"sectionRef":"#","url":"/docs/releases/0_19_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.19.2 - 2023-1-4","url":"/docs/releases/0_19_2#added","content":"Airflow: add Trino extractor #1288 @sekikn Adds a Trino extractor to the Airflow integration.Airflow: add S3FileTransformOperator extractor #1450 @sekikn Adds an S3FileTransformOperator extractor to the Airflow integration.Airflow: add standardized run facet #1413 @JDarDagran Creates one standardized run facet for the Airflow integration.Airflow: add NominalTimeRunFacet and OwnershipJobFacet #1410 @JDarDagran Adds nominalEndTime and OwnershipJobFacet fields to the Airflow integration.dbt: add support for postgres datasources #1417 @julienledem Adds the previously unsupported postgres datasource type.Proxy: add client-side proxy (skeletal version) #1439 #1420 @fm100 Implements a skeletal version of a client-side proxy.Proxy: add CI job to publish Docker image #1086 @wslulciuc Includes a script to build and tag the image plus jobs to verify the build on every CI run and publish to Docker Hub.SQL: add ExtractionErrorRunFacet #1442 @mobuchowski Adds a facet to the spec to reflect internal processing errors, especially failed or incomplete parsing of SQL jobs.SQL: add column-level lineage to SQL parser #1432 #1461 @mobuchowski @StarostaGit Adds support for extracting column-level lineage from SQL statements in the parser, including adjustments to Rust-Python and Rust-Java interfaces and the Airflow integration's SQL extractor to make use of the feature. Also includes more tests, removal of the old parser, and removal of the common-build cache in CI (which was breaking the parser).Spark: pass config parameters to the OL client #1383 @tnazarew Adds a mechanism for making new lineage consumers transparent to the integration, easing the process of setting up new types of consumers. "},{"title":"Fixed​","type":1,"pageTitle":"0.19.2 - 2023-1-4","url":"/docs/releases/0_19_2#fixed","content":"Airflow: fix collect_ignore, add flags to Pytest for cleaner output #1437 @JDarDagran Removes the extractors directory from the ignored list, improving unit testing.Spark &amp; Java client: fix README typos @versaurabh Fixes typos in the SPDX license headers. "},{"title":"0.2.1 - 2021-8-27","type":0,"sectionRef":"#","url":"/docs/releases/0_2_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.2.1 - 2021-8-27","url":"/docs/releases/0_2_1#fixed","content":"dbt: default --project-dir argument to current directory in dbt-ol script @mobuchowski "},{"title":"0.2.2 - 2021-9-8","type":0,"sectionRef":"#","url":"/docs/releases/0_2_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.2.2 - 2021-9-8","url":"/docs/releases/0_2_2#added","content":"Implement OpenLineageValidationAction for Great Expectations @collado-mikefacet: add expectations assertions facet @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.2.2 - 2021-9-8","url":"/docs/releases/0_2_2#fixed","content":"airflow: pendulum formatting fix, add tests @mobuchowskidbt: do not emit events if run_result file was not updated @mobuchowski "},{"title":"0.2.3 - 2021-10-7","type":0,"sectionRef":"#","url":"/docs/releases/0_2_3","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.2.3 - 2021-10-7","url":"/docs/releases/0_2_3#fixed","content":"dbt: add dbt v3 manifest support @mobuchowski "},{"title":"0.20.6 - 2023-2-10","type":0,"sectionRef":"#","url":"/docs/releases/0_20_6","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.20.6 - 2023-2-10","url":"/docs/releases/0_20_6#added","content":"Airflow: add new extractor for FTPFileTransmitOperator #1603 @sekikn Adds a new extractor for this Airflow operator serving legacy systems. "},{"title":"Changed​","type":1,"pageTitle":"0.20.6 - 2023-2-10","url":"/docs/releases/0_20_6#changed","content":"Airflow: make extractors for async operators work #1601 @JDarDagran Sends a deterministic Run UUID for Airflow runs. "},{"title":"Fixed​","type":1,"pageTitle":"0.20.6 - 2023-2-10","url":"/docs/releases/0_20_6#fixed","content":"dbt: render actual profile only in profiles.yml #1599 @mobuchowski Adds an include_section argument for the Jinja render method to include only one profile if needed.dbt: make compiled_code optional #1595 @JDarDagran Makes compiled_code optional for manifest &gt; v7. "},{"title":"0.20.4 - 2023-2-7","type":0,"sectionRef":"#","url":"/docs/releases/0_20_4","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.20.4 - 2023-2-7","url":"/docs/releases/0_20_4#added","content":"Airflow: add new extractor for GCSToGCSOperator #1495 @sekikn Adds a new extractor for this operator.Flink: resolve topic names from regex, support 1.16.0 #1522 @pawel-big-lebowski Adds support for Flink 1.16.0 and makes the integration resolve topic names from Kafka topic patterns.Proxy: implement lineage event validator for client proxy #1469 @fm100 Implements logic in the proxy (which is still in development) for validating and handling lineage events. "},{"title":"Changed​","type":1,"pageTitle":"0.20.4 - 2023-2-7","url":"/docs/releases/0_20_4#changed","content":"CI: use ruff instead of flake8, isort, etc., for linting and formatting #1526 @mobuchowski Adopts the ruff package, which combines several linters and formatters into one fast binary. "},{"title":"Fixed​","type":1,"pageTitle":"0.20.4 - 2023-2-7","url":"/docs/releases/0_20_4#fixed","content":"Airflow: make the Trino catalog non-mandatory #1572 @JDarDagran Makes the Trino catalog optional in the Trino extractor.Common: add explicit SQL dependency #1532 @mobuchowski Addresses a 0.19.2 breaking change to the GE integration by including the SQL dependency explicitly.DBT: adjust tqdm logging in dbt-ol #1549 @JdarDagran Adjusts tqdm to show the correct number of iterations and adds START events for parent runs.DBT: fix typo in log output #1493 @denimalpaca Fixes 'emittled' typo in log output.Great Expectations/Airflow: follow Snowflake dataset naming rules #1527 @mobuchowski Normalizes Snowflake dataset and datasource naming rules among DBT/Airflow/GE; canonizes old Snowflake account paths around making them all full-size with account, region and cloud names.Java and Python Clients: Kafka does not initialize properties if they are empty; check and notify about Confluent-Kafka requirement #1556 @mobuchowski Fixes the failure to initialize KafkaTransport in the Java client and adds an exception if the required confluent-kafka module is missing from the Python client.Spark: add square brackets for list-based Spark configs #1507 @Varunvaruns9 Adds a condition to treat configs with [] as lists. Note: [] will be required for list-based configs starting with 0.21.0.Spark: fix several Spark/BigQuery-related issues #1557 @mobuchowski Fixes the assumption that a version is always a number; adds support for HadoopMapReduceWriteConfigUtil; makes the integration access BigQueryUtil and getTableId using reflection, which supports all BigQuery versions; makes logs provide the full serialized LogicalPlan on debug.SQL: only report partial failures `#1479 @mobuchowski Changes the parser so it reports partial failures instead of failing the whole extraction. "},{"title":"0.3.0 - 2021-12-3","type":0,"sectionRef":"#","url":"/docs/releases/0_3_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.3.0 - 2021-12-3","url":"/docs/releases/0_3_0#added","content":"Spark3 support @OleksandrDvornik / @collado-mikeLineageBackend for Airflow 2 @mobuchowskiAdding custom spark version facet to spark integration @OleksandrDvornikAdding dbt version facet @mobuchowskiAdded support for Redshift profile @AlessandroLollo "},{"title":"Fixed​","type":1,"pageTitle":"0.3.0 - 2021-12-3","url":"/docs/releases/0_3_0#fixed","content":"Sanitize JDBC URLs @OleksandrDvornikstrip openlineage url in python client @OleksandrDvornikdeploy spec if spec file changes @mobuchowski "},{"title":"0.3.1 - 2021-12-3","type":0,"sectionRef":"#","url":"/docs/releases/0_3_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.3.1 - 2021-12-3","url":"/docs/releases/0_3_1#fixed","content":"fix import in spark3 visitor @mobuchowski "},{"title":"0.4.0 - 2021-12-13","type":0,"sectionRef":"#","url":"/docs/releases/0_4_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.4.0 - 2021-12-13","url":"/docs/releases/0_4_0#added","content":"Spark output metrics @OleksandrDvornikSeparated tests between Spark 2 &amp; 3 @pawel-big-lebowskiDatabricks install README and init scripts @wjohnsonIceberg integration with unit tests @pawel-big-lebowskiKafka read and write support @OleksandrDvornik / @collado-mikeArbitrary parameters supported in HTTP URL construction @wjohnsonIncreased visitor coverage for Spark commands @mobuchowski / @pawel-big-lebowski "},{"title":"Fixed​","type":1,"pageTitle":"0.4.0 - 2021-12-13","url":"/docs/releases/0_4_0#fixed","content":"dbt: column descriptions are properly filled from metadata.json @mobuchowskidbt: allow parsing artifacts with version higher than officially supported @mobuchowskidbt: dbt build command is supported @mobuchowskidbt: fix crash when build command is used with seeds in dbt 1.0.0rc3 @mobuchowskispark: increase logical plan visitor coverage @mobuchowski spark: fix logical serialization recursion issue @OleksandrDvornikUse URL#getFile to fix build on Windows @mobuchowski "},{"title":"0.5.1 - 2022-1-18","type":0,"sectionRef":"#","url":"/docs/releases/0_5_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.5.1 - 2022-1-18","url":"/docs/releases/0_5_1#added","content":"Support for dbt-spark adapter @mobuchowskiNew backend to proxy OpenLineage events to one or more event streams 🎉 @mandy-chessell @wslulciucAdd Spark extensibility API with support for custom Dataset and custom facet builders @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.5.1 - 2022-1-18","url":"/docs/releases/0_5_1#fixed","content":"airflow: fix import failures when dependencies for bigquery, dbt, great_expectations extractors are missing @lukaszlaszkoFixed openlineage-spark jar to correctly rename bundled dependencies @collado-mike "},{"title":"0.5.2 - 2022-2-10","type":0,"sectionRef":"#","url":"/docs/releases/0_5_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.5.2 - 2022-2-10","url":"/docs/releases/0_5_2#added","content":"Proxy backend example using Kafka @wslulciucSupport Databricks Delta Catalog naming convention with DatabricksDeltaHandler @wjohnsonAdd javadoc as part of build task @mobuchowskiInclude TableStateChangeFacet in non V2 commands for Spark @mr-yusupovSupport for SqlDWRelation on Databricks' Azure Synapse/SQL DW Connector @wjohnsonImplement input visitors for v2 commands @pawel-big-lebowskiEnabled SparkListenerJobStart events to trigger open lineage events @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.5.2 - 2022-2-10","url":"/docs/releases/0_5_2#fixed","content":"dbt: job namespaces for given dbt run match each other @mobuchowskiFix Breaking SnowflakeOperator Changes from OSS Airflow @denimalpacaMade corrections to account for DeltaDataSource handling @collado-mike "},{"title":"0.6.1 - 2022-3-7","type":0,"sectionRef":"#","url":"/docs/releases/0_6_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.6.1 - 2022-3-7","url":"/docs/releases/0_6_1#fixed","content":"Catch possible failures when emitting events and log them @mobuchowskidbt: jinja2 code using do extensions does not crash @mobuchowski "},{"title":"0.6.0 - 2022-3-4","type":0,"sectionRef":"#","url":"/docs/releases/0_6_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.6.0 - 2022-3-4","url":"/docs/releases/0_6_0#added","content":"Extract source code of PythonOperator code similar to SQL facet @mobuchowskiAdd DatasetLifecycleStateDatasetFacet to spec @pawel-big-lebowskiAirflow: extract source code from BashOperator @mobuchowskiAdd generic facet to collect environmental properties (EnvironmentFacet) @harishsuneOpenLineage sensor for OpenLineage-Dagster integration @dalinkimJava-client: make generator generate enums as well @pawel-big-lebowskiAdded UnknownOperatorAttributeRunFacet to Airflow integration to record operators that don't produce lineage @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.6.0 - 2022-3-4","url":"/docs/releases/0_6_0#fixed","content":"Airflow: increase import timeout in tests, fix exit from integration @mobuchowskiReduce logging level for import errors to info @rossturkRemove AWS secret keys and extraneous Snowflake parameters from connection uri @collado-mikeConvert to LifecycleStateChangeDatasetFacet @pawel-big-lebowski "},{"title":"0.6.2 - 2022-3-16","type":0,"sectionRef":"#","url":"/docs/releases/0_6_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.6.2 - 2022-3-16","url":"/docs/releases/0_6_2#added","content":"CI: add integration tests for Airflow's SnowflakeOperator and dbt-snowflake @mobuchowskiIntroduce DatasetVersion facet in spec @pawel-big-lebowskiAirflow: add external query id facet @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.6.2 - 2022-3-16","url":"/docs/releases/0_6_2#fixed","content":"Complete Fix of Snowflake Extractor get_hook() Bug @denimalpacaUpdate artwork @rossturkAirflow tasks in a DAG now report a common ParentRunFacet @collado-mike "},{"title":"0.8.1 - 2022-4-29","type":0,"sectionRef":"#","url":"/docs/releases/0_8_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.8.1 - 2022-4-29","url":"/docs/releases/0_8_1#added","content":"Airflow integration uses new TaskInstance listener API for Airflow 2.3+ (#508) @mobuchowskiSupport for HiveTableRelation as input source in Spark integration (#683) @collado-mikeAdd HTTP and Kafka Client to openlineage-java lib (#480) @wslulciuc, @mobuchowskiNew SQL parser, used by Postgres, Snowflake, Great Expectations integrations (#644) @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.8.1 - 2022-4-29","url":"/docs/releases/0_8_1#fixed","content":"GreatExpectations: Fixed bug when invoking GreatExpectations using v3 API (#683) @collado-mike "},{"title":"0.7.1 - 2022-4-19","type":0,"sectionRef":"#","url":"/docs/releases/0_7_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.7.1 - 2022-4-19","url":"/docs/releases/0_7_1#added","content":"Python implements Transport interface - HTTP and Kafka transports are available (#530) @mobuchowskiAdd UnknownOperatorAttributeRunFacet and support in lineage backend (#547) @collado-mikeSupport Spark 3.2.1 (#607) @pawel-big-lebowskiAdd StorageDatasetFacet to spec (#620) @pawel-big-lebowskiAirflow: custom extractors lookup uses only get_operator_classnames method (#656) @mobuchowskiREADME.md created at OpenLineage/integrations for compatibility matrix (#663) @howardyoo "},{"title":"Fixed​","type":1,"pageTitle":"0.7.1 - 2022-4-19","url":"/docs/releases/0_7_1#fixed","content":"Dagster: handle updated PipelineRun in OpenLineage sensor unit test (#624) @dominiquetiptonDelta improvements (#626) @collado-mikeFix SqlDwDatabricksVisitor for Spark2 (#630) @wjohnsonAirflow: remove redundant logging from GE import (#657) @mobuchowskiFix Shebang issue in Spark's wait-for-it.sh (#658) @mobuchowskiUpdate parent_run_id to be a uuid from the dag name and run_id (#664) @collado-mikeSpark: fix time zone inconsistency in testSerializeRunEvent (#681) @sekikn "},{"title":"0.8.2 - 2022-5-19","type":0,"sectionRef":"#","url":"/docs/releases/0_8_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.8.2 - 2022-5-19","url":"/docs/releases/0_8_2#added","content":"openlineage-airflow now supports getting credentials from Airflows secrets backend (#723) @mobuchowskiopenlineage-spark now supports Azure Databricks Credential Passthrough (#595) @wjohnsonopenlineage-spark detects datasets wrapped by ExternalRDDs (#746) @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.8.2 - 2022-5-19","url":"/docs/releases/0_8_2#fixed","content":"PostgresOperator fails to retrieve host and conn during extraction (#705) @sekiknSQL parser accepts lists of sql statements (#734) @mobuchowskiMissing schema when writing to Delta tables in Databricks (#748) @collado-mike "},{"title":"Facets & Extensibility","type":0,"sectionRef":"#","url":"/docs/spec/facets/","content":"Facets &amp; Extensibility Facets provide context to the OpenLineage events. Generally, an OpenLineage event contains the type of the event, who created it, and when the event happened. In addition to the basic information related to the event, it provides facets for more details in four general categories: job: What kind of activity ranrun: How it raninputs: What was used during its runoutputs: What was the outcome of the run Here is an example of the four facets in action. Notice the element facets under each of the four categories of the OpenLineage event: { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot;, &quot;facets&quot;: { &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;dbt-execution-parent-job&quot;, &quot;namespace&quot;: &quot;dbt-namespace&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;f99310b4-3c3c-1a1a-2b2b-c1b95c24ff11&quot; } } } }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot;, &quot;facets&quot;: { &quot;sql&quot;: { &quot;query&quot;: &quot;insert into taxes_out select id, name, is_active from taxes_in&quot; } } }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-in&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-out&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; } For more information of what kind of facets are available as part of OpenLineage spec, please refer to the sub sections Run Facets, Job Facets, and Dataset Facets of this document.","keywords":""},{"title":"0.9.0 - 2022-6-3","type":0,"sectionRef":"#","url":"/docs/releases/0_9_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.9.0 - 2022-6-3","url":"/docs/releases/0_9_0#added","content":"Add static code anlalysis tool mypy to run in CI for against all python modules (#802) @howardyooExtend SaveIntoDataSourceCommandVisitor to extract schema from LocalRelaiton and LogicalRdd in spark integration (#794) @pawel-big-lebowskiAdd InMemoryRelationInputDatasetBuilder for InMemory datasets to Spark integration (#818) @pawel-big-lebowskiAdd copyright to source files #755 @merobi-hubAdd SnowflakeOperatorAsync extractor support to Airflow integration #869 @merobi-hubAdd PMD analysis to proxy project (#889) @howardyoo "},{"title":"Changed​","type":1,"pageTitle":"0.9.0 - 2022-6-3","url":"/docs/releases/0_9_0#changed","content":"Skip FunctionRegistry.class serialization in Spark integration (#828) @mobuchowskiInstall new rust-based SQL parser by default in Airflow integration (#835) @mobuchowskiImprove overall pytest and integration tests for Airflow integration (#851,#858) @denimalpacaReduce OL event payload size by excluding local data and including output node in start events (#881) @collado-mikeSplit spark integration into submodules (#834, #890) @tnazarew @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.9.0 - 2022-6-3","url":"/docs/releases/0_9_0#fixed","content":"Conditionally import sqlalchemy lib for Great Expectations integration (#826) @pawel-big-lebowskiAdd check for missing class org.apache.spark.sql.catalyst.plans.logical.CreateV2Table in Spark integration (#866) @pawel-big-lebowskiFix static code analysis issues (#867,#874) @pawel-big-lebowski "},{"title":"Dataset Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/","content":"Dataset Facets Dataset Facets are generally consisted of common facet that is used both in inputs and outputs of the OpenLineage event. There are facets that exist specifically for input or output datasets. { ... &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-in&quot;, &quot;facets&quot;: { # This is where the common dataset facets are located }, &quot;inputFacets&quot;: { # This is where the input dataset facets are located } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-out&quot;, &quot;facets&quot;: { # This is where the common dataset facets are located }, &quot;outputFacets&quot;: { # This is where the output dataset facets are located } }], ... } In the above Example, Notice that there is a distinction of facets that are common for both input and output dataset, and input or output specific datasets. As for the common datasets, they all reside under the facets property. However, input or output specific facets are located either in inputFacets or outputFacets property.","keywords":""},{"title":"Column Level Lineage Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/column_lineage_facet","content":"Column Level Lineage Dataset Facet Column level lineage provides fine grained information on datasets' dependencies. Not only we know the dependency exist, but we are also able to understand which input columns are used to produce output columns. This allows answering questions like Which root input columns are used to construct column x? For example, a Job might executes the following query: INSERT INTO top_delivery_times ( order_id, order_placed_on, order_delivered_on, order_delivery_time ) SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time, FROM delivery_7_days ORDER BY order_delivery_time DESC LIMIT 1; This would establish the following relationships between the delivery_7_days and top_delivery_times tables: An OpenLinage run state update that represent this query using column-level lineage facets might look like: { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-02-22T22:42:42.000Z&quot;, &quot;run&quot;: ..., &quot;job&quot;: ..., &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot; } ], &quot;outputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.top_delivery_times&quot;, &quot;facets&quot;: { &quot;columnLineage&quot;: { &quot;_producer&quot;: &quot;https://github.com/MarquezProject/marquez/blob/main/docker/metadata.json&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-1/ColumnLineageDatasetFacet.json&quot;, &quot;fields&quot;: { &quot;order_id&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_id&quot; } ] }, &quot;order_placed_on&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_placed_on&quot; } ] }, &quot;order_delivered_on&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_delivered_on&quot; } ] }, &quot;order_delivery_time&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_placed_on&quot; }, { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_delivered_on&quot; } ] } } } } } ], ... } The facet specification can be found here.","keywords":""},{"title":"Data Quality Assertions Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/data_quality_assertions","content":"Data Quality Assertions Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;dataQualityAssertions&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DataQualityAssertionsDatasetFacet.json&quot;, &quot;assertions&quot;: [ { &quot;assertion&quot;: &quot;not_null&quot;, &quot;success&quot;: true, &quot;column&quot;: &quot;user_name&quot; }, { &quot;assertion&quot;: &quot;is_string&quot;, &quot;success&quot;: true, &quot;column&quot;: &quot;user_name&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Datasource Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/data_source","content":"Datasource Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;dataSource&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json&quot;, &quot;name&quot;: &quot;datasource_one&quot;, &quot;url&quot;: &quot;https://some.location.com/datsource/one&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Custom Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/custom-facets","content":"","keywords":""},{"title":"Example of creating your first custom facet​","type":1,"pageTitle":"Custom Facets","url":"/docs/spec/facets/custom-facets#example-of-creating-your-first-custom-facet","content":"Let's look at this sample OpenLineage client code written in python, that defines and uses a custom facet called my-facet. #!/usr/bin/env python3 from openlineage.client.run import ( RunEvent, RunState, Run, Job, Dataset, OutputDataset, InputDataset, ) from openlineage.client.client import OpenLineageClient, OpenLineageClientOptions from openlineage.client.facet import ( BaseFacet, SqlJobFacet, SchemaDatasetFacet, SchemaField, SourceCodeLocationJobFacet, NominalTimeRunFacet, ) import uuid from datetime import datetime, timezone, timedelta from typing import List import attr from random import random import logging, os logging.basicConfig(level=logging.DEBUG) PRODUCER = f&quot;https://github.com/openlineage-user&quot; namespace = &quot;python_client&quot; url = &quot;http://localhost:5000&quot; api_key = &quot;1234567890ckcu028rzu5l&quot; client = OpenLineageClient( url=url, # optional api key in case the backend requires it options=OpenLineageClientOptions(api_key=api_key), ) # generates job facet def job(job_name, sql, location): facets = { &quot;sql&quot;: SqlJobFacet(sql) } if location != None: facets.update( {&quot;sourceCodeLocation&quot;: SourceCodeLocationJobFacet(&quot;git&quot;, location)} ) return Job(namespace=namespace, name=job_name, facets=facets) @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email # geneartes run racet def run(run_id, hour, name, age, email): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ), &quot;my_facet&quot;: MyFacet(name, age, email) }, ) # generates dataset def dataset(name, schema=None, ns=namespace): if schema == None: facets = {} else: facets = {&quot;schema&quot;: schema} return Dataset(namespace, name, facets) # generates output dataset def outputDataset(dataset, stats): output_facets = {&quot;stats&quot;: stats, &quot;outputStatistics&quot;: stats} return OutputDataset(dataset.namespace, dataset.name, dataset.facets, output_facets) # generates input dataset def inputDataset(dataset, dq): input_facets = { &quot;dataQuality&quot;: dq, } return InputDataset(dataset.namespace, dataset.name, dataset.facets, input_facets) def twoDigits(n): if n &lt; 10: result = f&quot;0{n}&quot; elif n &lt; 100: result = f&quot;{n}&quot; else: raise f&quot;error: {n}&quot; return result now = datetime.now(timezone.utc) # generates run Event def runEvents(job_name, sql, inputs, outputs, hour, min, location, duration): run_id = str(uuid.uuid4()) myjob = job(job_name, sql, location) myrun = run(run_id, hour, 'user_1', 25, 'user_1@email.com') st = now + timedelta(hours=hour, minutes=min, seconds=20 + round(random() * 10)) end = st + timedelta(minutes=duration, seconds=20 + round(random() * 10)) started_at = st.isoformat() ended_at = end.isoformat() return ( RunEvent( eventType=RunState.START, eventTime=started_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), RunEvent( eventType=RunState.COMPLETE, eventTime=ended_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), ) # add run event to the events list def addRunEvents( events, job_name, sql, inputs, outputs, hour, minutes, location=None, duration=2 ): (start, complete) = runEvents( job_name, sql, inputs, outputs, hour, minutes, location, duration ) events.append(start) events.append(complete) events = [] # create dataset data for i in range(0, 5): user_counts = dataset(&quot;tmp_demo.user_counts&quot;) user_history = dataset( &quot;temp_demo.user_history&quot;, SchemaDatasetFacet( fields=[ SchemaField(name=&quot;id&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;email_domain&quot;, type=&quot;VARCHAR&quot;, description=&quot;the user id&quot; ), SchemaField(name=&quot;status&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;created_at&quot;, type=&quot;DATETIME&quot;, description=&quot;date and time of creation of the user&quot;, ), SchemaField( name=&quot;updated_at&quot;, type=&quot;DATETIME&quot;, description=&quot;the last time this row was updated&quot;, ), SchemaField( name=&quot;fetch_time_utc&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was fetched&quot;, ), SchemaField( name=&quot;load_filename&quot;, type=&quot;VARCHAR&quot;, description=&quot;the original file this data was ingested from&quot;, ), SchemaField( name=&quot;load_filerow&quot;, type=&quot;INT&quot;, description=&quot;the row number in the original file&quot;, ), SchemaField( name=&quot;load_timestamp&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was ingested&quot;, ), ] ), &quot;snowflake://&quot;, ) create_user_counts_sql = &quot;&quot;&quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS ( SELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count FROM TMP_DEMO.USER_HISTORY GROUP BY date )&quot;&quot;&quot; # location of the source code location = &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; # run simulating Airflow DAG with snowflake operator addRunEvents( events, &quot;create_user_counts&quot;, create_user_counts_sql, [user_history], [user_counts], i, 11, location, ) for event in events: from openlineage.client.serde import Serde # print(Serde.to_json(event)) # time.sleep(1) client.emit(event)  As you can see in the source code, there is a class called MyFacet which extends from the BaseFacet of OpenLineage, having three attributes of name, age, and email. @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email  And, when the application is generating a Run data, you can see the instantiation of MyFacet, having the name my_facet. def run(run_id, hour, name, age, email): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ), &quot;my_facet&quot;: MyFacet(name, age, email) }, )  When you run this application with python (and please make sure you have installed openlineage-python using pip before running it), you will see a series of JSON output that represents the OpenLineage events being submitted. Here is one example. { &quot;eventTime&quot;: &quot;2022-12-09T09:17:28.239394+00:00&quot;, &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;inputs&quot;: [ { &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;BIGINT&quot; }, { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;email_domain&quot;, &quot;type&quot;: &quot;VARCHAR&quot; }, { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;status&quot;, &quot;type&quot;: &quot;BIGINT&quot; }, { &quot;description&quot;: &quot;date and time of creation of the user&quot;, &quot;name&quot;: &quot;created_at&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the last time this row was updated&quot;, &quot;name&quot;: &quot;updated_at&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the time the data was fetched&quot;, &quot;name&quot;: &quot;fetch_time_utc&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the original file this data was ingested from&quot;, &quot;name&quot;: &quot;load_filename&quot;, &quot;type&quot;: &quot;VARCHAR&quot; }, { &quot;description&quot;: &quot;the row number in the original file&quot;, &quot;name&quot;: &quot;load_filerow&quot;, &quot;type&quot;: &quot;INT&quot; }, { &quot;description&quot;: &quot;the time the data was ingested&quot;, &quot;name&quot;: &quot;load_timestamp&quot;, &quot;type&quot;: &quot;DATETIME&quot; } ] } }, &quot;name&quot;: &quot;temp_demo.user_history&quot;, &quot;namespace&quot;: &quot;python_client&quot; } ], &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCodeLocation&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SourceCodeLocationJobFacet&quot;, &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; }, &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SqlJobFacet&quot;, &quot;query&quot;: &quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS (\\n\\t\\t\\tSELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count\\n\\t\\t\\tFROM TMP_DEMO.USER_HISTORY\\n\\t\\t\\tGROUP BY date\\n\\t\\t\\t)&quot; } }, &quot;name&quot;: &quot;create_user_counts&quot;, &quot;namespace&quot;: &quot;python_client&quot; }, &quot;outputs&quot;: [ { &quot;facets&quot;: {}, &quot;name&quot;: &quot;tmp_demo.user_counts&quot;, &quot;namespace&quot;: &quot;python_client&quot; } ], &quot;producer&quot;: &quot;https://github.com/openlineage-user&quot;, &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; }, &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot;: &quot;2022-04-14T04:12:00Z&quot; } }, &quot;runId&quot;: &quot;7886a902-8fec-422f-9ee4-818489e59f5f&quot; } }  Notice the facet information my_facet that has is now part of the OpenLineage event.  ... &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; }, ...  OpenLineage backend should be able to store this information when submitted, and later, when you access the Lineage, you should be able to view the facet information that you submitted, along with your custom facet that you made. Below is the screen shot of one of the OpenLineage backend called Marquez, that shows th custom facet that the application has submitted.  You might have noticed the schema URL is actually that of BaseFacet. By default, if the facet class did not specify its own schema URL, that value would be that of BaseFacet. From the view of OpenLineage specification, this is legal. However, if you have your own JSON spec defined, and has it publically accessible, you can specify it by overriding the _get_schema function as such: @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email @staticmethod def _get_schema() -&gt; str: return &quot;https://somewhere/schemas/myfacet.json#/definitions/MyFacet&quot;  And the _schemaURL of the OpenLineage event would now reflect the change as such:  &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://somewhere/schemas/myfacet.json#/definitions/MyFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; },  "},{"title":"Data Quality Metrics Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics","content":"Data Quality Metrics Facet Example: { ... &quot;inputs&quot;: { &quot;inputFacets&quot;: { &quot;dataQualityMetrics&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json&quot;, &quot;rowCount&quot;: 123, &quot;bytes&quot;: 35602, &quot;columnMetrics&quot;: { &quot;column_one&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } }, &quot;column_two&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } }, &quot;column_three&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } } } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Lifecycle State Change Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/lifecycle_state_change","content":"Lifecycle State Change Facet Example: { ... &quot;outputs&quot;: { &quot;facets&quot;: { &quot;lifecycleStateChange&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json&quot;, &quot;lifecycleStateChange&quot;: &quot;CREATE&quot; } } } ... } { ... &quot;outputs&quot;: { &quot;facets&quot;: { &quot;lifecycleStateChange&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json&quot;, &quot;lifecycleStateChange&quot;: &quot;RENAME&quot;, &quot;previousIdentifier&quot;: { &quot;namespace&quot;: &quot;example_namespace&quot;, &quot;name&quot;: &quot;example_table_1&quot; } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Output Statistics Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/output-dataset-facets/output_statistics","content":"Output Statistics Facet Example: { ... &quot;inputs&quot;: { &quot;outputFacets&quot;: { &quot;outputStatistics&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json&quot;, &quot;rowCount&quot;: 123, &quot;size&quot;: 35602 } } } ... } The facet specification can be found here.","keywords":""},{"title":"Ownership Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/ownership","content":"Ownership Dataset Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;ownership&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OwnershipDatasetFacet.json&quot;, &quot;owners&quot;: [ { &quot;name&quot;: &quot;maintainer_one&quot;, &quot;type&quot;: &quot;MAINTAINER&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Storage Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/storage","content":"Storage Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;storage&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/StorageDatasetFacet.json&quot;, &quot;storageLayer&quot;: &quot;iceberg&quot;, &quot;fileFormat&quot;: &quot;csv&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Schema Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/schema","content":"Schema Dataset Facet The schema dataset facet contains the schema of a particular dataset. Besides a name, it provides an optional type and description of each field. Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Symlinks Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/symlinks","content":"Symlinks Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;symlinks&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/SymlinksDatasetFacet.json&quot;, &quot;identifiers&quot;: [ &quot;namespace&quot;: &quot;example_namespace&quot;, &quot;name&quot;: &quot;example_dataset_1&quot;, &quot;type&quot;: &quot;table&quot; ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Version Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/version_facet","content":"Version Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;version&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json&quot;, &quot;datasetVersion&quot;: &quot;1&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Job Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/","content":"Job Facets Job Facets apply to a distinct instance of a job: an abstract process that consumes, executes, and produces datasets (defined as its inputs and outputs). It is identified by a unique name within a namespace. The Job evolves over time and this change is captured during the job runs.","keywords":""},{"title":"Documentation Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/documentation","content":"Documentation Facet Contains the documentation or description of the job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;documentation&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/DocumentationJobFacet.json&quot;, &quot;description&quot;: &quot;This is the documentation of something.&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Ownership Job Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/ownership","content":"Ownership Job Facet The facet that contains the information regarding users or group who owns this particular job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;ownership&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OwnershipJobFacet.json&quot;, &quot;owners&quot;: [ { &quot;name&quot;: &quot;maintainer_one&quot;, &quot;type&quot;: &quot;MAINTAINER&quot; } ] } } } ... } The facet specification can be found here","keywords":""},{"title":"Source Code Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/source-code","content":"Source Code Facet The source code of a particular job (e.g. Python script) Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCode&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SourceCodeJobFacet.json&quot;, &quot;language&quot;: &quot;python&quot;, &quot;sourceCode&quot;: &quot;print('hello, OpenLineage!')&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Source Code Location Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/source-code-location","content":"Source Code Location Facet The facet that indicates where the source code is located. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCodeLocation&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SourceCodeLocationJobFacet.json&quot;, &quot;type&quot;: &quot;git|svn&quot;, &quot;url&quot;: &quot;https://github.com/MarquezProject/marquez-airflow-quickstart/blob/693e35482bc2e526ced2b5f9f76ef83dec6ec691/dags/hello.py&quot;, &quot;repoUrl&quot;: &quot;git@github.com:{org}/{repo}.git or https://github.com/{org}/{repo}.git|svn://&lt;your_ip&gt;/&lt;repository_name&gt;&quot;, &quot;path&quot;: &quot;path/to/my/dags&quot;, &quot;version&quot;: &quot;git: the git sha | Svn: the revision number&quot;, &quot;tag&quot;: &quot;example&quot;, &quot;branch&quot; &quot;main&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"SQL Job Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/sql","content":"SQL Job Facet The SQL Job Facet contains a SQL query that was used in a particular job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SQLJobFacet.json&quot;, &quot;query&quot;: &quot;select id, name from schema.table where id = 1&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Error Message Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/error_message","content":"Error Message Facet The facet to contain information about the failures during the run of the job. A typical payload would be the message, stack trace, etc. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;errorMessage&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/ErrorMessageRunFacet.json&quot;, &quot;message&quot;: &quot;org.apache.spark.sql.AnalysisException: Table or view not found: wrong_table_name; line 1 pos 14&quot;, &quot;programmingLanguage&quot;: &quot;JAVA&quot;, &quot;stackTrace&quot;: &quot;Exception in thread \\&quot;main\\&quot; java.lang.RuntimeException: A test exception\\nat io.openlineage.SomeClass.method(SomeClass.java:13)\\nat io.openlineage.SomeClass.anotherMethod(SomeClass.java:9)&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Run Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/","content":"Run Facets Run Facets apply to a specific instance of a particular running job. Every run will have a uniquely identifiable run ID that is usually in UUID format, that can later be tracked.","keywords":""},{"title":"External Query Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/external_query","content":"External Query Facet The facet that describes the identification of the query that the run is related to which was executed by external systems. Even though the query itself is not contained, using this facet, the user should be able to access the query and its details. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;externalQuery&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/ExternalQueryRunFacet.json&quot;, &quot;externalQueryId&quot;: &quot;my-project-1234:US.bquijob_123x456_123y123z123c&quot;, &quot;source&quot;: &quot;bigquery&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Nominal Time Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/nominal_time","content":"Nominal Time Facet The facet to describe the nominal start and end time of the run. The nominal usually means the time the job run was expected to run (like a scheduled time), and the actual time can be different. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SQLJobFacet.json&quot;, &quot;nominalStartTime&quot;: &quot;2020-12-17T03:00:00.000Z&quot;, &quot;nominalEndTime&quot;: &quot;2020-12-17T03:05:00.000Z&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Parent Run Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/parent_run","content":"Parent Run Facet Commonly, scheduler systems like Apache Airflow will trigger processes on remote systems, such as on Apache Spark or Apache Beam jobs. Those systems might have their own OpenLineage integration and report their own job runs and dataset inputs/outputs. The ParentRunFacet allows those downstream jobs to report which jobs spawned them to preserve job hierarchy. To do that, the scheduler system should have a way to pass its own job and run id to the child job. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;the-execution-parent-job&quot;, &quot;namespace&quot;: &quot;the-namespace&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;f99310b4-3c3c-1a1a-2b2b-c1b95c24ff11&quot; } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Naming Conventions","type":0,"sectionRef":"#","url":"/docs/spec/naming","content":"","keywords":""},{"title":"Dataset Naming​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#dataset-naming","content":"A dataset, or table, is organized according to a producer, namespace, database and (optionally) schema. Producer\tFormula\tExample URIPostgres\tproducer + host + port + database + table\tpostgres://db.foo.com:6543/metrics.sales.orders MySQL\tproducer + host + port + database + table\tmysql://db.foo.com:6543/metrics.orders S3\tproducer + bucket + path\ts3://sales-metrics/orders.csv GCS\tproducer + bucket + path\tgcs://sales-metrics/orders.csv HDFS\tproducer + host + port + path\thdfs://stg.foo.com:3000/salesorders.csv BigQuery\tproducer + project + dataset + table\tbigquery:metrics.sales.orders Redshift\tproducer + host + port + database + schema + table\tredshift://examplecluster.XXXXXXXXXXXX.us-west-2.redshift.amazonaws.com:5439/metrics.sales.orders Athena\tproducer + host + catalog + database + table\tawsathena://athena.us-west-2.amazonaws.com/metrics.sales.orders Azure Synapse\tproducer + host + port + database + schema + table\tsqlserver://XXXXXXXXXXXX.sql.azuresynapse.net:1433;database=SQLPool1/sales.orders Azure Cosmos DB\tproducer + host + database + 'colls' + table\tazurecosmos://XXXXXXXXXXXX.documents.azure.com/dbs/metrics/colls/orders "},{"title":"Job Naming​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#job-naming","content":"A Job is a recurring data transformation with inputs and outputs. Each execution is captured as a Run with corresponding metadata. A Run event identifies the Job it instances by providing the job’s unique identifier. The Job identifier is composed of a Namespace and Name. The job name is unique within its namespace. Producer\tFormula\tExampleAirflow\tnamespace + DAG + task\tairflow-staging.orders_etl.count_orders SQL\tnamespace + name\tgx.validate_datasets "},{"title":"Run Naming​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#run-naming","content":"Runs are named using client-generated UUIDs. The OpenLineage client is responsible for generating them and maintaining them throughout the duration of the runcycle. from openlineage.client.run import Run run = Run(str(uuid4()))  "},{"title":"Why Naming Matters​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#why-naming-matters","content":"Naming enables focused insight into data flows, even when datasets and workflows are distributed across an organization. This focus enabled by naming is key to the production of useful lineage.  "},{"title":"Additional Resources​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#additional-resources","content":"The OpenLineage Naming SpecWhat's in a Namespace Blog Post "},{"title":"Producers","type":0,"sectionRef":"#","url":"/docs/spec/producers","content":"Producers info This page could use some extra detail! You're welcome to contribute using the Edit link at the bottom. The _producer value is included in an OpenLineage request as a way to know how the metadata was generated. It is a URI that links to a source code SHA or the location where a package can be found. For example, this field is populated by many of the common integrations. For example, the dbt integration will set this value to https://github.com/OpenLineage/OpenLineage/tree/{__version__}/integration/dbt and the Python client will set it to https://github.com/OpenLineage/OpenLineage/tree/{__version__}/client/python.","keywords":""},{"title":"Object Model","type":0,"sectionRef":"#","url":"/docs/spec/object-model","content":"","keywords":""},{"title":"Run State Update​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run-state-update","content":"A Run State Update is prepared and sent when something important occurs within your pipeline, and each one can be thought of as a distinct observation. This commonly happens when a Job starts or finishes. The run state itself refers to a stage within the run cycle of the current run. Usually, the first Run State for a Job would be START and the last would be COMPLETE. A run cycle is likely to have at least two Run State Updates, and perhaps more. Each one will also have timestamp of when this particular state change happened.  Each Run State Update can include detail about the Job, the Run, and the input and output Datasets involved in the run. Subsequent updates are additive: input Datasets, for example, can be specified along with START, along with COMPLETE, or both. This accommodates situations where information is only available at certain times. Each of these three core entities can also be extended through the use of facets, some of which are documented in the relevant sections below. "},{"title":"Job​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#job","content":"A Job is a process that consumes or produces Datasets. This is abstract, and can map to different things in different operational contexts. For example, a job could be a task in a workflow orchestration system. It could also be a model, a query, or a checkpoint. Depending on the system under observation, a Job can represent a small or large amount of work. A Job is the part of the object model that represents a discrete bit of defined work. If, for example, you have cron running a Python script that executes a CREATE TABLE x AS SELECT * FROM y query every day, the Python script is the Job. Jobs are identified by a unique name within a namespace. They are expected to evolve over time and their changes can be captured through Run State Updates. "},{"title":"Job Facets​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#job-facets","content":"Facets that can be used to augment the metadata of a Job include: sourceCodeLocation: Captures the source code location and version (e.g., the git SHA) of the job. sourceCode: Captures the language (e.g. python) and complete source code of the job. Using this source code, users can gain useful information about what the job does. For more details, please refer to the Job Facets. "},{"title":"Run​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run","content":"A Run is an instance of a Job that represents one of its occurrences in time. Each run will have a uniquely identifiable runId that is generated by the client in UUID format. The client is responsible for maintaining the runId between different Run State Updates in the same Run. Runs can be used to observe changes in Jobs between their instances. If, for example, you have cron running a Python script that repeats a query every day, this should resuilt in a separate Run for each day. "},{"title":"Run Facets​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run-facets","content":"Facets that can be used to augment the metadata of a Run include: nominalTime: Captures the time this run is scheduled for. This is typically used for scheduled jobs. The job has a nominally scheduled time that will be different from the actual time it ran. parent: Captures the parent Job and Run, for instances where this Run was spawned from a parent Run. For example in the case of Airflow, there's a Run that represents the DAG itself that is the parent of the individual Runs that represent the tasks it spawns. Similarly when a SparkOperator starts a Spark job, this creates a separate run that refers to the task run as its parent. errorMessage: Captures potential error messages - and optionally stack traces - with which the run failed. sql: Captures the SQL query, if this job runs one. For more details, please refer to the Run Facets. "},{"title":"Dataset​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#dataset","content":"A Dataset is an abstract representation of data. This can refer to a small amount or large amount of data, as long as it's discrete. For databases, this should be a table. For cloud storage, this is often an object in a bucket. This can represent a directory of a filesystem. It has a unique name within a namespace derived from its physical location (i.e., db.host.database.schema.table). The combined namespace and name for a Dataset should be enough to uniquely identify it within a data ecosystem. Typically, a Dataset changes when a job writing to it completes. Similarly to the Job and Run distinction, metadata that is more static from Run to Run is captured in a DatasetFacet - for example, the schema that does not change every run). What changes every Run is captured as an InputFacet or an OutputFacet - for example, a time partition indicating the subset of the data set that was read or written). A Dataset is the part of the object model that represents a discrete collection of data. If, for example, you have cron running a Python script that executes a CREATE TABLE x AS SELECT * FROM y query every day, the x and y tables are Datasets. "},{"title":"Dataset Facets​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#dataset-facets","content":"Facets that can be used to augment the metadata of a Dataset include: schema: Captures the schema of the dataset dataSource: Captures the database instance containing this Dataset (e.g., database schema, object store bucket) lifecycleStateChange: Captures the lifecycle states of the Dataset (e.g., alter, create, drop, overwrite, rename, truncate) version: Captures the dataset version when versioning is defined by the data store (e.g.. Iceberg snapshot ID) Input Datasets have the following facets: dataQualityMetrics: Captures dataset-level and column-level data quality metrics (row count, byte size, null count, distinct count, average, min, max, quantiles) dataQualityAssertions: Captures the result of running data tests on dataset or its columns Output Datasets have the following facets: outputStatistics: Captures the size of the output written to a dataset (e.g., row count and byte size) For more details, please refer to the Dataset Facets. "},{"title":"The Run Cycle","type":0,"sectionRef":"#","url":"/docs/spec/run-cycle","content":"","keywords":""},{"title":"Run States​","type":1,"pageTitle":"The Run Cycle","url":"/docs/spec/run-cycle#run-states","content":"There are five run states currently defined in the OpenLineage spec: START to indicate the beginning of a Job RUNNING to provide additional information about a running Job COMPLETE to signify that execution of the Job has concluded ABORT to signify that the Job has been stopped abnormally FAIL to signify that the Job has failed OTHER to provide additional metadata outside of the standard run cycle - e.g., on a run that has already completed "},{"title":"Typical Scenarios​","type":1,"pageTitle":"The Run Cycle","url":"/docs/spec/run-cycle#typical-scenarios","content":"A batch Job - e.g., an Airflow task or a dbt model - will typically be represented as a START event followed by a COMPLETE event. Occasionally, an ABORT or FAIL event will be sent when a job does not complete successfully.  A long-running Job - e.g., a microservice or a stream - will typically be represented by a START event followed by a series of RUNNING events that report changes in the run or emit performace metrics. Occasionally, a COMPLETE, ABORT, or FAIL event will occur, often followed by a START event as the job is reinitiated.  "},{"title":"Working with Schemas","type":0,"sectionRef":"#","url":"/docs/spec/schemas","content":"","keywords":""},{"title":"Create a new issue with label spec​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#create-a-new-issue-with-label-spec","content":"Before you decide to make any changes, it is best advised that you first label your issue with spec. This will indicate the the issue is related to any changes in the current OpenLineage spec. "},{"title":"Make changes to the spec's version​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#make-changes-to-the-specs-version","content":"Whenever there is a change to existing spec file (JSON), you need to bump up the version of the existing current spec, so that the changes can go through the code generation and gradle build. Consider the following spec file, where you will see the URL in $id that shows what is the current spec version the file currently is. { &quot;$schema&quot;: &quot;https://json-schema.org/draft/2020-12/schema&quot;, &quot;$id&quot;: &quot;https://openlineage.io/spec/facets/1-0-1/ColumnLineageDatasetFacet.json&quot;, &quot;$defs&quot;: {  In this example, bumping up the version to the new value, should be changed from 1-0-1 to 1-0-2. { &quot;$schema&quot;: &quot;https://json-schema.org/draft/2020-12/schema&quot;, &quot;$id&quot;: &quot;https://openlineage.io/spec/facets/1-0-2/ColumnLineageDatasetFacet.json&quot;, &quot;$defs&quot;: {  If you do not bump the version to higher number, the code generation of Java client will fail. "},{"title":"Python client's codes need to be manually updated​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#python-clients-codes-need-to-be-manually-updated","content":"Java client's build process does involve code generation that automatically produces OpenLineage classes derived from the spec files, so you do not need to do anything in terms of coding the client. However, python client libraries does not depend on the spec files to be generated, so you have to make sure to add changes to the python code in order for it to know and use the changes. As for the facets, they are implemented here, so generally, you need to apply necessary changes to it. As for the general structure of OpenLineage's run events, it can be found here. "},{"title":"Add test cases​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#add-test-cases","content":"Make sure to add changes to the unit tests for python and java to make sure the unit test can be performed against your new SPEC changes. Refer to existing test codes to add yours in. "},{"title":"Test the SPEC change using code generation and integration tests​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#test-the-spec-change-using-code-generation-and-integration-tests","content":"When you have modified the SPEC file(s), always make sure to perform code generation and unit tests by going into client/java and running ./gradlew generateCode and ./gradlew test. As for python, cd into client/python and run pytest. Note: Some of the tests may fail due to the fact that they require external systems like kafka. You can ignore those errors. "}]